{"url": "https://habrahabr.ru/post/322700/", "title": ["Охота на мифический MVC. Пользовательский интерфейс: как делить и куда помещать логику"], "text": ["Детектив по материалам IT. Часть вторая Чтобы не было разочарований, \"считаю долгом предупредить, что кот древнее и неприкосновенное животное\" что в этой части будет продолжено исследование “исторического хлама” и написанное здесь практически бесполезно при работе с конкретными фреймворками. Ну а тем, кто не испугался, я попробую показать, как же изначально выглядело деление Вид/Контроллер и какие интересные решения можно найти в этой области на сегодняшний день. Ссылки на первоисточники приведены в начале первой части. Итак, переходим к Пользовательскому Интерфейсу. Не смотря на то что Вид определяется как модуль, отображающий Модель – \"а view is a (visual) representation of its model\", на практике к Виду, как правило, просто относят все графические элементы GUI, то есть Видом считается все то, что мы видим на экране ЭВМ. Понятно, что тут содержится некое противоречие, поскольку такие графические компоненты как меню, кнопки, тулбары служат не для отображения информации о системе, а прежде всего для управления системой. Клавиатура и мышь всегда были средством управления программой и находились в «ведомости» Контроллера (как бы его не трактовали). Поэтому кажется нелогичным и странным, что кнопки, сделанные из пластмассы, считаются элементами управления и относятся к Контроллеру, а кнопки, нарисованные на экране, и по сути выполняющие те же самые функции (производить входящие события), почему то относят к Виду. Кроме того, поскольку разумно считается что в графических элементах не хорошо «прописывать логику», то отсюда почему-то делается вывод, что Вид должен быть тонким и тупым (dumb View) и соответственно логику работы интерфейса можно обнаружить в самых неожиданных местах, вплоть до доменной модели (даже Фаулер пишет о «загрязнении» Модели настройками интерфейса GUI Architectures). Интерфейс. Деление на Вид и Контроллер И вновь для того, чтобы прояснить ситуацию предлагаю обратиться к «архитектурным принципам» и первоисточникам. Когда речь идет о декомпозиции то одно из основных «архитектурных» правил заключается в том, что делить на модули нужно прежде всего исходя из тех задач, которые решает система. Каждый модуль должен отвечать за решение какой-то определенной задачи (желательно одной) и выполнять соответствующую ей функцию. Соответственно, для того, чтобы понять как следует делить пользовательский интерфейс на модули, в первую очередь надо проанализировать что он делает и какие задачи решает. Ведь интерфейс это функция, а не графические элементы. И функция эта заключается в том, чтобы обеспечивать взаимо-действие пользователя с системой. Что означает: выводить и удобно отображать пользователю информацию о системе вводить данные и команды пользователя в систему (передавать их системе) То есть, в общем случае пользовательский интерфейс является двунаправленным и решает две задачи, одна из которых связана с выводом и представлением информации, а вторая – с вводом команд и данных. Вот эти-то задачи и определяют то, как нужно делить интерфейс на модули. Вновь смотрим картинку из первоисточника (Реенскауг): Как видно пользовательский интерфейс довольно естественно и логично делится на два относительно независимых функциональных модуля. Причем такое функциональное деление, основанное на решаемых задачах, универсально. Не важно звуковой это интерфейс, графический или сенсорный в нем должен быть модуль, отвечающий за Ввод управляющих команд и данных (отсюда и название Controller — управление), и модуль отвечающий Вывод и представление информации о системе и о том что в ней происходит (View — Вид, представление). Не смотря на простоту и очевидность этого принципа он часто нарушается. Сравните: «Вид это то, что мы видим на экране ЭВМ, графические элементы» Нет решаемой задачи, за определение взят вторичный признак. Функциональное определение: «Вид отвечает за вывод информации о системе и ее представление для пользователя»Четко указана решаемая задача, при этом абсолютно не важны детали ее реализации, то каким именно образом будет выводиться информация – графически ли, с помощью аудио или еще как-то. За счет чего и достигается универсальность. «Контроллер обрабатывает действия пользователя, \"interprets the mouse and keyboard inputs\"» Указано действие, но не определена цель этого действия, то есть решаемая задача. Это равносильно тому, как сказать, что работа программиста заключается в том, чтобы нажимать клавиши на компьютере. В результате под «обработкой действий пользователя» может пониматься все что угодно, вплоть до бизнес логики. Плохо также то, что в определение вынесены детали реализации – мышь и клавиатура. Как только технология поменялась – мышь и клавиатура сменились сенсорными экранами, определение устарело) Функциональное определение: «Контроллер обеспечивает удобный ввод команд и данных и их передачу от пользователя системе»Но это теория, а было обещано, что вы увидите как деление Вид/Контроллер выглядело на практике. И для того чтобы это сделать нам потребуется небольшой экскурс в историю. Дело в том, что вначале термина CONTROLLER вообще не существовало. Вместо него Реенскауг использовал термин EDITOR (MODEL-VIEW-EDITOR) и писал о разделении пользовательского интерфейса на View и Editor. \"The hardest part was to hit upon good names for the different architectural components. Model-View-Editor was the first set.\"(Trygve Reenskaug). Попробую объяснить почему. Не смотря на наличие технических возможностей (растровые экраны) во времена создания MVC и SmallTalk пользовательские интерфейсы преимущественно все еще оставались командными (Command-driven Interface) и представляли собой по сути обычные текстовые редакторы. В SmallTalk интерфейс так и назывался – Editor. И вот как он выглядел: Можно сказать что Editor объединял в себе функции Контроллера и Вида. Он давал возможность относительно удобно вводить команды и данные (отображая нажатые клавиши и обрабатывая ввод с клавиатуры), и одновременно выводил информацию о выполнении команд и вообще о том что происходит в системе. Лишь постепенно этот Editor преобразовывался в то, что мы сейчас привыкли понимать под GUI. Вначале возникла простая но весьма плодотворная идея – разделить единое окно на множество панелей. Парадигма многопанельности появились существенно раньше MVC (многопанельные браузеры точно присутствовали уже в Smalltalk-76) и была значительным продвижением сама по себе. Она до сих пор активно используется практически во всех текстовых интерфейсах. Реенскауг ее, естественно, тоже использовал. Тут нужно иметь в виду, что Dynabook, вокруг которого в Xerox Parc создавались и smallTalk и графические интерфейсы и, в частности, MVC, задумывался как «детский компьютер». Поэтому стояла задача сделать работу с компьютерными программами доступной любому неподготовленному пользователю, в частности ребенку. Реенскауг исходил из того, что пользователь может вообще ничего не знать о программе и о том как она устроена. И для того чтобы он мог с программой взаимодействовать нужно каким-то образом отобразить ему основную информацию о системе и доменной модели, лежащей в ее основе, чтобы пользователь понимал с чем имеет дело. Выводилась такая информация на отдельных панелях, которые собственно и стали называться View. Как правило доменная модель отображалась при помощи нескольких различных Видов: \"MVC задумывался как общее решение, дающее возможность пользователям контролировать большие и сложные наборы данных… Он особенно полезен тогда, когда пользователю нужно видеть Модель одновременно в разных контекстах и/или с разных точек зрения.\" И поскольку Реенскауг считал, что графическое представление информации нагляднее чем текстовое, то в основном виды у него представляли собой разного рода графики и диаграммы. Далее. Так как пользователь ничего (или почти ничего) не знает о системе, а видит лишь всевозможные View, удобно и наглядно отображающие нужную ему информацию, то соответственно и управление системой должно было выглядеть так, как если бы пользователь управлял непосредственно самими View и тем что на них отображено. Поэтому для ввода команд стал использоваться не один «general Editor», а целое множество специализированных редакторов, каждый из которых был связан со своим Видом и был заточен на ввод команд (взаимодействие с доменной моделью) лишь в контексте этого Вида – permits the user to modify the information that is presented by the view. Таким образом, если для специалиста интерфейсом обычно служил некий общий редактор, дающий возможность вводить в систему любые команды (но для этого нужно было штудировать мануалы, знать команды и понимать, как система работает), то для неподготовленного пользователя минимальным интерфейсом становится пара – View и связанный с ней специализированный Editor (который собственно и будет впоследствии переименован в Контроллер). Такой интерфейс предоставлял весьма редуцированный набор команд и возможностей, зато им можно было пользоваться без предварительной подготовки. Первый доклад Реенскауга так и назывался: \"THING-MODEL-VIEW-EDITOR. An Example from a planning system\". В нем была представлена первая реализация MVC на примере системы планирования и управления неким большим проектом (своего рода task-manager). Доменной моделью являлась сеть (network) «активностей», которая описывала что должно быть сделано (активность), в каком порядке, за какое время, кто в каких активностях участвует и какие ресурсы для каждой активности требуются. Пример призван был показать что “одна Модель может быть отображена с помощью многих различных Видов” и вот как там выглядел пользовательский интерфейс: Доменная модель отображалась с помощью трех различных диаграмм (Видов). Нужно отметить, что виды и у Реенскауга и в SmallTalk вовсе не были \"пассивными\", они самостоятельно обрабатывали относящиеся к ним действия пользователя, в частности позволяли делать скроллинг и выделение элементов: “A ListView has fields where it remembers its frame, a list of textual items and a possible selection within the list. It is able to display its list on the screen within its frame, and reacts to messages asking it to scroll itself.” Это важный момент и я еще к нему вернусь. Первая диаграмма отображает множество активностей, относящихся к некоторому проекту/сети и связи между ними. Подобно тому, как список позволяет выбрать/выделить некий свой элемент, данная диаграмма позволяла выбрать некую активность. С диаграммой связан редактор, который дает возможность запрашивать у системы и редактировать информацию, относящуюся к выбранной активности. Например: длительность активности, кто в ней участвует, какие активности предшествуют и тп Вторая диаграмма – GanttView (календарный или временной график Гантта) показывает расположение проекта и его активностей во времени. Эта диаграмма также позволяет выбрать некую активность. Редактор, связанный с GanttView дает возможность \"to pass on operations on the network and its activities that are related to this particular View\". В частности он позволяет изменить запланированную дату начала и окончания выбранной активности, а также осуществлять планирование и управление проектом/сетью как целым. Третья диаграмма — это диаграмма ресурсов, требуемых для осуществления активностей, в зависимости от времени. Любопытно то, что с этой диаграммой связан не редактор, а список и в зависимости от того какая активность в этом списке выбрана, диаграмма ресурсов отображает только те ресурсы, которые относятся к выбранной активности. Такое сочетание двух видов, один из которых \"управляет\" отображаемой информацией другого характерно для многих интерфейсов. А идея использовать для «управления» не текстовый редактор а список ляжет в основу большинства контроллеров в SmallTalk-80. Термин Controller возник практически перед самым уходом Реенскауга из Xerox PARC \"After long discussions, particularly with Adele Goldberg\". И из-за этого оригинальные работы Реенскауга сложно читать, поскольку одним и тем же словом «Editor» он называет: Панели-редакторы которые связывались с соответствующими View для ввода команд и «управления View» и которые в Smalltalk-80 стали Контроллерами (\"Smalltalk-80 Controller у меня назывался Editor\"). Пару – Вид и связанный с ним Контроллер, которая стала базовым блоком для построения любых интерфейсов. В этих случаях Реенскауг пишет о разделении Editor на View и Controller Весь интерфейс целиком, который мог включать в себя несколько блоков Вид-Контроллер. И в этом случае Реенскауг пишет о первичном разделении приложения на Model (domain model) и Editor. Позже для обозначения таких сложно составных интерфейсов Реенскауг будет использовать термин Tool. Также к обязанностям Контроллера Реенскауг относил управление самим интерфейсом и в частности множеством входящих в него Видов: \"Controller was responsible for creating and coordinating its subordinate views\". Соответственно иногда он пишет, что Контроллер это связь между пользователем и системой, а иногда что это связь между пользователем и Видами и что Контроллер передает команды Видам. Тем не менее, как видно из примера, Реенскауговский «Editor-Controller» был вполне себе видим и имел графическое представление. Посмотрим как дело обстояло в SmallTalk-80. Первое: в SmallTalk-80 текстовые редакторы, которые Реенскауг использовал для ввода команд, «официально» стали Контроллерами. «ParagraphEditor» и «TextEditor», обладающие стандартными функциями ввода и редактирования текста, в SmallTalk-80 являлись потомками класса «Controller»: Поскольку, как уже упоминалось, редакторы объединяли в себе функции Ввода и Вывода, то они также использовались для отображения текстовой информации во многих Видах – TextView, TextEditorView. Стив Барбек дает по этому поводу довольно подробное разьяснение: \"Все контроллеры, которые принимают ввод с клавиатуры, являются наследниками «ParagraphEditor» в иерархии Контроллеров. «ParagraphEditor» предшествовал созданию парадигмы MVC. Он одновременно выполняет две функции – обрабатывает ввод текста с клавиатуры и отображает его на экране. Поэтому в некотором смысле он представляет собой нечто среднее между Видом и Контроллером. Виды, которые используют подклассы «ParagraphEditor» в качестве контроллера, полностью переворачивают стандартные роли – для того чтобы отобразить текст они посылают его своему контроллеру\" [All controllers that accept keyboard text input are under the ParagraphEditor in the Controller hierarchy. ParagraphEditor predates the full development of the MVC paradigm. It handles both text input and text display functions, hence it is in some ways a cross between a view and a controller. The views which use it or its subcasses for a controller reverse the usual roles for display; they implement the display of text by sending controller display]. О причине подобных «парадоксов» я постараюсь написать дальше, пока же просто обратите на это внимание Второе: в SmallTalk-80 с каждым Видом (подвидом) ОБЯЗАТЕЛЬНО был связан свой Контроллер, который давал возможность производить некие операции с той информацией, которую Вид отображает Соответственно любое множество Видов (подвидов) входящих в состав пользовательского интерфейса на самом деле всегда сопровождалось точно таким же множеством связанных с ним Контроллеров. Опять таки у Стива Барбека этой теме посвящен целый раздел который так и называется — \"Communication Between Controllers\". Третье: главное усовершенствование состояло в том, что для ввода команд преимущественно стала использоваться мышь, а не клавиатура. В SmallTalk-80 помимо контроллеров-редакторов (ParagraphEditor и TextEditor) появляется MouseMenuController, который становится основным средством ввода команд. Пользовательские интерфейсы из Command-driven становятся Menu-driven (User Interfaces) Доступные команды для каждого Вида формировались явно в виде списка. А связанный с Видом MouseMenuController предоставлял для их отображения и удобного ввода специальное графическое средство – всплывающие pop-up menu, которые появлялись при нажатии соответствующей кнопки мыши. Вот так они выглядели: Повторю: pop-up menu относились к MouseMenuController. Меню являлись специальным графическим инструментом/средством для удобного отображения и ввода команд (определенных в контексте некоторого вида), который Контроллер, связанный с этим Видом, предоставлял пользователю. Вот что пишет Краснер: \"Хотя меню можно рассматривать как пару вид-контроллер, но чаще всего они считаются входными устройствами и следовательно относятся к сфере контроллера… За создание всплывающих меню при нажатии какой-нибудь кнопки мыши отвечает класс MouseMenuController… По умолчанию PopUpMenus возвращают числовое значение которое вызвавший их контроллер использует для того чтобы определить какое действие ему нужно совершить… Из-за широкого использования всплывающих меню большинство контроллеров пользовательского интерфейса являются подклассами MouseMenuController\". Так что основной контроллер SmallTalk-80 (MouseMenuController) тоже имел свою графическую часть – pop-up menu, и разделение пользовательского интерфейса на Виды и Контроллеры стало выглядеть следующим образом (иллюстрация взята из статьи Гленна Краснера): Меню относящиеся к MouseMenuController-ам можно видеть абсолютно во всех SmallTalk приложениях и примерах. Вот так с развернутыми меню выглядят уже упоминавшиеся Workspace и Inspector: А вот так выглядит Browser, включающий в себя 5 Видов (подВидов) и соответствующих им Контроллеров Предполагаю, что именно из-за широкого использования всплывающих меню Реенскауг и писал, что контроллер в smalltalk \"это эфемерный компонент, который View создает при необходимости в качестве связывающего звена между View и входными устройствами такими как мышь и клавиатура\". Так что, можем развеять очередной миф: Мифы: Все графические элементы пользовательского интерфейса относятся к Виду. Контроллер это исключительно логика обработки движений мыши, нажатий клавиш на клавиатуре и других входящих событий производимых пользователем.На самом деле и в реализации Реенскауга и затем в Smalltalk-80 большинство Контроллеров имели «графическую составляющую, помогающую пользователю вводить команды и данные». И именно такие Контроллеры в основном использовались в пользовательских приложениях. Хотя, конечно же, были Контроллеры и без графической составляющей, но они в основном применялись для более низкоуровневых системных задач (об этом чуть позже). Если просуммировать, то можно сказать что Контроллер это часть пользовательского интерфейса, которая отвечает за то чтобы 1) предоставить пользователю удобные средства для ввода команд и данных, а затем 2) действия пользователя перевести в вызовы соответствующих методов Модели и передать их ей. Вот как определял Контроллер сам Реенскауг: \"Контроллер это связь между пользователем и системой. Он предоставляет пользователю меню и другие средства для ввода команд и данных. Контроллер получает результат таких действий пользователя, транслирует их в соответствующие сообщения и передает эти сообщения\" [ A controller is the link between a user and the system. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on]. Современные графические интерфейсы (GUI) для ввода команд используют весь спектр доступных средств: текстовые и графические меню, кнопки, всплывающие pop-up меню, всевозможные переключатели (как в реальных приборах), текстовые поля для ввода данных (TextEditor свелся к TextField). Все эти элементы служат в основном для управления, а не для отображения информации. По английски они так и называются — controls (элементы управления). И если продолжить аналогию, то разделение Вид/Контроллер в современных системах выглядело бы примерно следующим образом: Можно сказать, что Контроллер это «панель управления» (Control panel). А Вид это «обзорная панель» или «панель наблюдения за системой» включающая в себя текстовые описания, списки, таблицы, графики, шкалы, световые табло, и всевозможные индикаторы состояния. Красота MVC заключается в том, что его идеи универсальны и применимы не только к информационным системам. Не важно прибор это или программа, в простейшем случае интерфейс, как правило, содержит блок/панель управления, позволяющий вводить команды – Контроллер, и блок отображения информации – Вид. Реализация Видов и Контроллеров Что нам это дает? Ну во первых, становится понятно что логика работы Вида никак не может быть помещена в Контроллер: Если всю логику работы GUI вынести в Контроллер, то это нарушало бы сразу несколько принципов: главный принцип определяющий качество декомпозиции – High Cohesion + Low Coupling, который говорит что «резать» на модули нужно так, чтобы связи, особенно сильные, оставались преимущественно внутри модулей, а не между ними Принцип единственной ответственности (Single responsibility principle). Второе. Тонкий Вид, рассматриваемый исключительно как набор графических элементов, не выполняет никакой функции и поэтому в плане пере-использования мало полезен. Если же мы рассматриваем Вид как полноценный функциональный модуль, решающий довольно общую и востребованную задачу – визуализация и удобное представление данных, то при правильном подходе он становится идеальным кандидатом на пере-использование. В этом смысле разделение пользовательского интерфейса на Вид и Контроллер очень красивый шаг – Контроллер вобрал в себя большую часть зависимостей. Виду от Модели нужны лишь данные для отображения в определенном формате. Соответственно потенциально один и тот же Вид может быть использован для визуализации информации в разных приложениях. Последнее время активно разрабатывается и используется концепция Dashboard-ов (информационных панелей), для которых создаются наборы универсальных виджетов, позволяющих наглядно и удобно визуализировать «все что угодно». В отличие от «тупого вида» такие «полноценные блоки визуализации» (инкапсулирующие свою логику, настройки и способные работать самостоятельно) очень востребованы и ценны сами по себе. Когда Вид и Контроллер, трактуются как функциональные модули, отвечающие за решение определенных задач, то становится понятно, что для того чтобы инкапсулировать свою логику работы им вовсе нет нужды смешивать ее с графикой. Ведь любой модуль, при необходимости, может быть разделен на подмодули и обладать своей внутренней структурой. Как мы выяснили основной Контроллер SmallTalk-80 — MouseMenuController вовсе не являлся «всего лишь обработчиком действий пользователя с мышью и клавиатурой», на самом деле он делал довольно много вещей: Задавал набор и названия команд, доступных пользователю в контексте некоего Вида, Определял логику того, как эти команды транслировать в вызовы соответствующих методов Модели Отображал доступные команды Обрабатывал низкоуровневые движения мыши и создавал высокоуровневые события, к которым удобно привязывать выполнение команд (Event Driven подход). Для таких Контроллеров просто «просилась» MVC архитектура. И в SmallTalk-80 она была использована: \"pop-up menu are implemented as a special kind of MVC class\" (Краснер). Тут важно понимать, что Модель в этом «внутреннем» MVC не имеет никакого отношения к доменной модели, это именно внутренняя вспомогательная модель, описывающая «состояние» самого контроллера (в частности то, какая команда выбрана) и логику изменения этого состояния. Аналогично дело обстоит и с Видом. Мифы: То, что Вид это всего лишь «графика», является такой же идеализацией как и то, что Контроллер это «исключительно логика».Визуализация информации является непростой задачей, для решения которой, как правило, требуются дополнительные данные, характеризующие именно сам процесс отображения. Большинство Видов, используемых в реальных приложениях, это довольно сложные объекты со своим «состоянием» и логикой его изменения. Например, Вид редко когда может отобразить всю Модель целиком, обычно он отражает лишь какую-то ее часть и ему необходимо «знать» какая именно «часть Модели» должна быть отражена в данный момент. Многие виды позволяют «выделять» какие-то элементы, а значить им где-то нужно хранить информацию о выделениях. Где хранились такого рода дополнительные данные и логика их изменения? В принципе ответ очевиден и, если вы помните, Реенскауг ответил на этот вопрос – конечно же в самом Виде. Но! Не в перемешку с графикой, а в отдельном под-модуле/классе/скрипте, то есть в некоторой внутренней модели. И раз есть вспомогательные внутренние модели, то должны быть и специальные Контроллеры, которые этими внутренними моделями управляют. Такие Контроллеры изменяют исключительно состояние самого Вида и не не имеют никакого отношения к Контроллеру приложения, изменяющему состояние доменной модели. Положением фрейма, например управлял ScrollController. То есть в общем случае Виды тоже имеют структуру MVC. Но пока отложим вопрос с Контроллерами и сосредоточится на главном — на внутренних моделях. Фаулер такие внутренние модели, являющиеся частью представления называет — Presentation Model: Presentation Model pulls the state and behavior of the view out into a model class that is part of the presentation. И вот тут внимание! В первой части статьи подробно рассказывается о том, как в MVC был «потерян» Фасад и что из-за этого его роль на себе вынуждены брать другие компоненты. Так вот хотя Фаулер и пишет что \"Presentation Model is not a GUI friendly facade to a specific domain object\" на практике PresentationModel, ViewModel, ApplicationModel не только описывают состояние и поведение представления, но одновременно являются еще и Фасадами к доменной модели. В примере, который Фаулер подробно разбирает, хорошо видно, что его PresentationModel является именно смесью Фасада и модели представления. Ну а Microsoft прямо пишет: \"Presentation model class acts as a façade on the model with UI-specific state and behavior, by encapsulating the access to the model and providing a public interface that is easy to consume from the view\" (MSDN: Presentation Model) Безусловно такой подход противоречит Принципу единой ответственности, но он существует, используется во фреймворках и в принципе работает. Поэтому о нем стоит знать. В отличие от него в Java подобного смешения стараются не допускать. В Java Swing «application-data models» и «GUI-state models» разделены гораздо более четко. Все, наверное, читали или слышали что Java Swing компоненты реализованы в виде MVC. И большинство наверняка уверены в том, что M в этой триаде это тот самый интерфейс к доменным данным. И по сути это правильно, почти… Давайте внимательно посмотрим на список JList. У него действительно есть модель, обеспечивающая доступ к доменным данным – ListModel. Но кроме нее у списка имеется еще одна модель — ListSelectionModel, которая отвечает исключительно за внутреннюю логику выделения элементов списка (item selection). И вот эта модель как раз и является в чистом виде внутренней моделью представления – «GUI-state model»: \"The models provided by Swing fall into two general categories: GUI-state models and application-data models. GUI state models are interfaces that define the visual status of a GUI control, such as whether a button is pressed or armed, or which items are selected in a list. GUI-state models typically are relevant only in the context of a graphical user interface (GUI). An application-data model is an interface that represents some quantifiable data that has meaning primarily in the context of the application, such as the value of a cell in a table or the items displayed in a list. These data models provide a very powerful programming paradigm for Swing programs that need a clean separation between their application data/logic and their GUI\" (см статью от создателей A Swing Architecture Overview). У таблицы JTable помимо application-data модели, обеспечивающей доступ к доменным данным – TableModel, имеются целых две внутренних GUI-stateмодели — ListSelectionModel и TableColumnModel. А вот у кнопки JButton имеется лишь GUI-state модель – ButtonModel. Что в принципе и логично. Кнопка не отображает доменных данных, это в чистом виде Контроллер с внутренней моделью, которая определяет состояние кнопки (нажата/не нажата). При использовании графических компонент нам, в основном, приходится иметь дело с application-data моделями, через которые собственно и осуществляется взаимодействие домена и интерфейса. С GUI-state моделями мы сталкиваемся лишь тогда, когда возникает необходимость изменить дефолтное поведение компонента. Поэтому это нормально и правильно что многие даже не знают о наличие GUI-state моделей. Видно, что в таблице некоторые модели отмечены одновременно и как GUI и как data. Особых пояснений не дается, написано что это зависит от контекста использования модели. Я могу высказать по этому поводу лишь предположение. Все такого рода промежуточные модели относятся к компонентам, которые являются либо разновидностью скроллбара либо разновидностью переключателя (кнопка с состоянием). Такие компоненты предназначены прежде всего для управления и по сути представляют собой контроллер (в SmallTalk скроллбар и был контроллером — ScrollController) но с некоторым внутренним состоянием. Соответственно у таких компонент имеется лишь GUI-state модель и в дефолтной реализации состояние этих компонент никак не связано с состоянием доменной модели, а зависит лишь от действий пользователя, то есть от того, в какое состояние пользователь этот контрол/кнопку перевел. Но при этом выглядит состояние таких контроллеров так, как если бы оно было согласовано с состоянием доменной модели: мы перевели переключатель в состояние «On», в доменную модель передалась соответствующая команда, там что-то включилось… и доменная модель тоже перешла в некое состояние «On». Достигается такая псевдо-согласованность как правило \"сама собой\", автоматически. Тем не менее возможны ситуации когда может понадобиться настоящее реальное согласование доменной модели и состояния переключателя. В этом случае будет написана реализация ButtonModel, в которой метод isSelected() перестанет зависеть от действий пользователя с кнопкой, а вместо этого будет напрямую отображать состояние домена (некий его флаг). Кнопка при этом становится отчасти видом, а ButtonModel перестает быть внутренней GUI-state и становится application-data В заключение темы привожу схемы обоих описанных тут подходов чтобы каждый мог выбрать то, что ему больше подходит: Я стараюсь первого варианта избегать. Мое ИМХО что подобное смешение хоть и соблазняет своей кажущейся простотой но на практике приводит к тому что PresentationModel превращается в большую \"свалку\". Представьте что есть реальный сложный интерфейс. Если использовать подход (2), то мы этот интерфейс разобьем на ui-модули, и каждый модуль будет инкапсулировать свою логику в виде небольшой внутренней ui-state модели. А вот если использовать подход (1), то логика работы всего этого большого интерфейса будет свалена в кучу вперемешку с логикой фасада в PresentationModel. Пока такая PresentationModel или ApplicationModel создается и управляется автоматически неким фреймворком все хорошо. Но если подобное писать ручками… то мозг начинает ломаться и часто это приводит к тому, что логика представления рано или поздно просачивается через фасад в доменную модель. Даже у такого гуру как Фаулер, в его детском примере это таки произошло (интересно, кто-нибудь еще заметил это место?). В архитектуре, где фасад и GUI-state модели разделены, вероятность такого рода ошибок значительно ниже. Кто хочет развлечься, то в том же примере у Фаулера можно найти ненужное копирование данных, о котором писалось в первой части. Объединенный ВидКонтроллер. «Упрощенный MVC» Раз уж мы коснулись Java Swing, то нужно сказать еще об одной его важной особенности – в отличие от SmallTalk-80 где и скроллбар и pop-up меню были реализованы в виде полноценного MVC (с внутренней моделью, внутренним низкоуровневым контроллером и видом) Swing в реализации базовых gui компонент использует «упрощенный MVC», в котором Вид и Контроллер объединены в единый компонент, который одновременно отображает данные и обрабатывает действия пользователя. Обычно он так и называется: объединенный ViewController или UI-object, IU-delegate. Вот еще одна статья, где это подробно описывается: MVC meets Swing. Самое интересное и неожиданное заключается в том, что такой «упрощенный MVC» де-факто используется в большинстве GUI библиотек и фреймворках пришедших на смену SmallTalk-80: VisualAge Smalltalk от IBM, Visual SmallTalk, VisualWorks SmallTalk, MacApp… (Smalltalk, Objects, and Design стр 124-125). Фактически классический вариант MVC, с обязательным разделением Видов и Контроллеров, особенно на низком уровне, только в SmallTalk-80 и был реализован. Почему? Опять таки я могу высказать лишь предположение. Основное отличие SmallTalk-80 от всех последующих систем заключалось в том что он работал без операционной системы. Поэтому весь объем низкоуровневой работы по отслеживанию движений мыши а также нажатий клавиш на клавиатуре приходилось выполнять Контроллеру. Контроллер в тех условиях был необходим, фактически он выполнял роль драйвера входящих устройств и кроме ссылок на Модель и Вид обязательно содержал ссылку на «сенсор». Соответсвенно именно эта сторона его деятельности акцентировалась и выходила на первый план. После того, как эту работу взяли на себя операционные системы, низкоуровневая «обработка действий пользователя» в большинстве случаев становится достаточно простой для того чтобы с ней мог справится сам Вид. И для простых компонент это оказывается плюсом, потому как разделение функций ввода и вывода хорошо работает для интерфейса в целом или для сложных компонент. А в случае создания базовых ui-компонент, как пишут создатели Swing: \"this split didn't work well in practical terms because the view and controller parts of a component required a tight coupling (for example, it was very difficult to write a generic controller that didn't know specifics about the view)\". На самом деле и в SmallTalk-80 Виды и Контроллеры тоже были очень тесно связанными: Контроллер всегда содержал ссылку на Вид, а Вид на Контроллер. Кроме того, соответствующие классы Вида и Контроллера обычно еще и разрабатывались совместно: \"Поскольку классы вида и контроллера часто разрабатывались совместно, для многих подклассов вида был определен класс контроллера, используемого по умолчанию, и метод для получения его экземпляра – defaultControllerClass. И конкретный контроллер связанный с видом часто создавался автоматически просто как экземпляр такого класса\" [Because view and controller classes are often designed in consort, a view's controller is often simply initialized to an instance of the corresponding controller class. To support this, the message defaultControllerClass, that returns the class of the appropriate controller, is defined in many of the subclasses of View – Glenn Krasner ]. Что еще делал Контроллер? Помимо обработки низкоуровневых действий пользователя и создания высокоуровневых событий, Контроллер также содержал \"логику перевода этих высокоуровневых событий в соответствующие методы Модели\". Но как мы выяснили в первой части статьи, Моделями в SmallTalk-80 являлись не сами доменные объекты а интерфейсы и фасады к ним, причем клиент ориентированные. Такие Модели-фасады изначально формировались (были заточена) под требования клиента, так что команды, которые Контроллер отображал в popup menu, практически всегда однозначно соответствовали методам Модели. Вот что пишет Краснер: \"В конце концов сообщения контроллера почти всегда напрямую передавались модели; это означает что в ответ на выбор пункта меню «aMessage» контроллеру посылалось сообщение aMessage и в результате почти всегда вызывался метод модели, который так и назывался «aMessage».\" [Finally, the controller messages were almost always passed directly on to the model; that is, the method for message aMessage, which was sent to the controller when the menu item aMessage was selected, was almost always implemented as ↑model aMessage]. В конце своей статьи Краснер приводит несколько примеров из которых видно, что Контроллеры приложений были «пустыми» и не содержали никакой логики. Их создание сводилось к тому что нужно было указать названия команд, отображаемых в pop-up меню, а затем для каждого названия указать метод модели, который должен был вызываться. По сути это означает, что вся логика находилась в моделях: бизнес-логика – в доменной модели, логика перевода команд пользователя в команды системы – в моделях-фасадах, логика работы самого GUI – в ui-state моделях. От Контроллера требовалось лишь связать вызовы методов этих моделей с соответствующими высокоуровневыми событиями (нажатие кнопки, выбор пункта меню) а это настолько тривиально что и эту функцию тоже легко может выполнить сам Вид. Следующий важный шаг заключается в том, что в современных GUI библиотеках исчезла и существовавшая в smallTalk-80 классификация самих базовых ui-компонент по типу Вид или Контроллер. Как выяснилось далеко не все графические элементы были Видами: pop-up меню относились к Контроллеру, скроллбар и TextEdit просто были Контроллерами. Сейчас такое разделение не делается и мы имеем «ui-компоненты» или «виджеты», которые изначально проектируются так, чтобы быть универсальными – они одновременно могут отображать информацию и создавать высокоуровневые события, к которым удобно привязывать выполнение команд. И таким образом могут играть роль Вида или Контроллера в зависимости от контекста. Дело в том, что многие ui-компоненты оказались похожи на ParagraphEditor, о котором писал Стив Барбек – они одновременно могут отображать информацию и позволяют ее изменять, объединяя в себе функции Вида и Контроллера. Такими интерактивными элементами являются текстовые поля, формы, всевозможные переключатели (toggle button, radio button, check box) и аналоги скроллбара... Для таких компонент грань Вид или Контроллер оказывается размытой. Один и тот же элемент (объект) может быть Видом или Контроллером в зависимости от контекста в котором он используется и от того какую функцию он в данный момент выполняет. Список, используемый для отображения и ввода команд, являлся частью Контроллера (popUpMenu), а тот же список используемый для отображения данных – частью Вида (ListView). Аналогично — текстовое поле. Как напишет Реенскауг в своих более поздних работах: \"Модель, Вид и Контроллер это на самом деле роли, которые могут исполняться объектами\" (Model, View and Controller are actually roles that can be played by the objects… – The DCI Architecture: A New Vision of Object-Oriented Programming). То, что в SmallTalk-80 сами объекты пытались поделить на Модели, Виды и Контроллеры, вызывало лишь ненужную путаницу, которая в частности проявлялось в том, что TextView для отображения посылал текст своему Контроллеру. Так осталось ли что нибудь от Контроллера? На мой взгляд да. То, что команды в систему вводятся \"с помощью мыши и клавиатуры\" это ведь тоже своего рода миф. В действительности Контроллер всегда предоставлял пользователю некие средства (как правило графические) помогающие вводить команды. Сначала это были текстовые редакторы, отображающие нажатые пользователем клавиши, затем pop-up меню, отображающие список доступных команд и дающие возможность вводить их «в один клик». Современные gui-библиотеки предоставляют уже целый арсенал средств — кнопки, переключатели, текстовые и графические меню, слайдеры… И вот эта часть работы Контроллера – поиск все более наглядных, удобных и «интуитивно понятных» средств/форм для ввода команд и управления системами, продолжает быть актуальной. И каждый раз когда мы разрабатываем интерфейс или ui-компонент мы явно или неявно ее решаем. Интерфейс как Composite Еще одна важная идея, которая безусловно осталась – это понимание того, что интерфейс нужно делить на модули, а не делать единым блоком или страницей. Как видно из примеров, большинство интерфейсов были составными и включали в себя множество Видов и Контроллеров. Для обозначения подобных сложно-составных Интерфейсов Реенскауг в своей второй более поздней работе использует специальный термин \"Tool\" (инструмент пользователя) и у него этой теме посвящен отдельный раздел, который так и называется “Tool as a Composite”. Замечание: обратите внимание на подпись на рисунке Реенскауга – \"User in the driver's seat\". Если уж рассматривать MVC на избитом примере машины или самолета, то довольно наивно ассоциировать Вид с их дизайном и внешним видом. Пользовательский интерфейс машины это прежде всего водительское место, включающее в себя: приборную панель со спидометром и прочими шкалами, отражающими состояние машины и то, что с ней происходит (именно отсюда и был заимствован термин «Dashboard» для информационных панелей) – Вид; рычаги/элементы управления машиной, такие как руль, педали газа и тормоза, переключатель скоростей и т.п. – Контроллер. Ну а доменной моделью машины будет все то, что составляет ее основу и позволяет ей ездить. На рисунке в качестве Tool приведен уже знакомый нам интерфейс из первого доклада Реенскауга состоящий из трех блоков (которые Реенскауг называет Editor). И вот что по этому поводу пишет Реенскауг: The Model that is responsible for representing state, structure, and behavior of the user’s mental model. One or more Editors that present relevant information in a suitable way and support the editing of this information when applicable. A Tool that sets up the Editors and coordinates their operation. (E.g., the selection of a model object that is visible in several Editors). Complex Editors may again be subdivided into a View and a Controller.This solution is a composite pattern. Как видите, шаблон Composite у Реенскауга относится ко всему интерфейсу, а вовсе не к Виду. Откуда же взялась идея что Composite в MVC относится исключительно к Виду? Все просто – поскольку в SmallTalk-80 с каждым Видом обязательно был связан свой Контроллер, то в явном виде композиция там действительно задавалась только для Видов, а соответствующая композиция (иерархия) Контроллеров просто «вычислялась»: \"Since each view is associated with a unique controller, the view/subView tree induces a parallel controller tree within each topView\". Это решение было не особо удачным и приводило к хрупкости системы – стоило какой-нибудь Вид оставить без Контроллера и вся система рушилась. Поэтому в SmallTalk-80 был придуман «костыль» – Контроллер, который назывался «NoController». Этот контроллер ничего не делал и по умолчанию связывался с Видами, которые “исключительно отображали информацию” и не нуждались в контроллере. Его единственное назначение состояло в том, чтобы цепочка контроллеров, соответствующих Видам, не прерывалась. (подробно можно почитать У Стива Барбека в разделе “Communication Between Controllers”). Так что в действительности и в SmallTalk-80, и у Реенскауга, пользовательский интерфейс всегда включал в себя не только композицию Видов но и точно такую же композицию соответствующих им Контроллеров. И если уж говорить о шаблоне Composite, то, конечно же, более корректно относить его не к Виду а ко всему пользовательскому интерфейсу, как это сделано у Реенскауга и как это делается в современных GUI библиотеках. Интерфейсы больших приложений делятся на «gui-компоненты» или виджеты, которые в свою очередь могут делиться на более простые компоненты и образовывать древовидную структуру. Каждый такой gui-компонент является автономным модулем, который одновременно отображает некую информацию и обрабатывает относящиеся к нему действия пользователя, порождая высокоуровневые события к которым удобно привязывать выполнение команд (View+Controller). Также он инкапсулирует свою логику работы как правило в виде внутренней GUI-state модели. То есть по сути представляет собой MVC (вернее «упрощенный MVC»). И вот такие полноценные gui-компоненты уже действительно можно разрабатывать параллельно и независимо, а также переиспользовать Следствием шаблона Composite и того, что каждый gui-компонент может быть реализован в виде небольшого MVC является некая иерархичность или рекурсивность MVC. Но из-за того что об этом редко пишут, аналоги этой идеи разработчики вынуждены пере-открывать. Вот известная статья на эту тему – Hierarchical model–view–controller и интересная дискуссия – Recursive Model View Controller. А вот картинка из статьи: В реальных же проектах непонимание этого аспекта приводит к следующим крайностям: Сделать весь проект в виде одного большого MVC (разбив его на три «класса») Сначала проект разбивается на страницы а затем каждая страница реализуется в виде одного средних размеров MVC (некрасивое но жизнеспособное решение) Все приложение сходу бьется на сотни крошечных MVC (соответствующих каждому графическому компоненту в интерфейсе) Хотя все что нужно, это разбивать приложение на функциональные модули иерархически. Сначала исходное приложение делится на доменную модель (функциональное ядро приложения) и пользовательский интерфейс. Связь между этими двумя модулями ослабляется за счет использования шаблонов Фасад и Наблюдатель. А затем сам пользовательский интерфейс делится модули. А эти модули, в свою очередь, на более мелкие подмодули и тд. Причем любой ui-модуль на любом уровне иерархии может быть реализован как угодно и иметь любую структуру, но чаще всего реализуется тоже в виде MVC. На практике такой подход к построению интерфейсов из независимых полноценных ui-компонент активно использует и развивает ebay. Подробно об этом можно почитать в их замечательной статье Don’t Build Pages, Build Modules: \"Когда дело касается view люди все еще мыслят страницами вместо того чтобы строить UI модули. Мы обнаружили что с ростом сложности страниц их становится экспоненциально сложнее поддерживать. Что мы хотим это разделить страницу на маленькие управляемые части, каждую из которых можно разрабатывать независимо. Мы хотим уйти от идеи непосредственно строить страницы. Вместо этого мы разбиваем страницу на логические UI модули и делаем это рекурсивно до тех пор пока модуль не станет FIRST. Это означает что страница строится из высокоуровневых модулей, которые в свою очередь строятся из подмодулей\". Суммируя Я вовсе не хочу сказать что «original MVC» единственно правильный. Моя цель состояла лишь в том, чтобы показать, что он был намного сложнее и богаче чем те упрощенные схемки, которые нам обычно преподносятся в качестве MVC. Создавать на их основе реальные приложения это все равно что строить самолет на основе схем из детского конструктора и удивляться что он не летает. С другой стороны, если рассматривать MVC не как схему, а прежде всего, как набор архитектурных идей, то он действительно становится прост, логичен и очень понятен, и буквально выводится из этих идей. И когда есть понимание, что же именно делается, с помощью каких «инструментов», ради чего, то тогда MVC перестает быть «догмой» и его можно варьировать в зависимости от потребностей конкретного проекта. Да и термины становятся не так важны. Когда я слышу или читаю про фронт-контроллер или что \"контроллер это единая точка входа в систему\", то мне понятно что термином Контроллер у этих ребят называется Фасад. И это вовсе не означает что их архитектура неправильная. Наоборот, как минимум хорошо, что у них Фасад вообще есть, ну а дальше нужно смотреть, как он реализован, не оттягивает ли на себя реализацию бизнес логики и тп Когда говорится, что Контроллер это бизнес-логика, то и это не страшно… Просто принимаешь что в этом случае Контроллером называется доменная модель. Важно не то, как она называется, а то как она реализована и отделена ли от пользовательского интерфейса. Часто термин Вид используют как синоним термина Пользовательский Интерфейс. Так тоже \"можно\". Лишь бы он при необходимости был грамотно разбит на модули и не трактовался исключительно как графика. Дело ведь не в терминах, а в сути. Мне кажется, что корень большинства проблем заключается в том, что как раз о сути MVC мало кто пишет. Вместо этого термины Модель, Вид и Контроллер вырываются из контекста архитектурных идей, им даются какие-то формальные определения, а затем они нередко применяются для обозначения модулей в некачественной декомпозиции, навешивается шаблон Наблюдатель и все это преподносится под «брендом MVC». Как писал Вирт: \"Самой трудной проектной задачей является нахождение наиболее адекватной декомпозиции системы на иерархически выстроенные модули, с минимизацией функций и дублирования кода\". Поэтому основная мысль, которую мне хотелось донести, заключается в том, что MVC он не про Модель, Вид и Контроллер, не про то как они связаны между собой и не про шаблон Наблюдатель. MVC он про то, как нужно грамотно разбивать систему на функционально осмысленные модули, слабо связанные друг с другом и полезные сами по себе. Модули, которые на самом деле можно разрабатывать и использовать независимо, переиспользовать… Что собственно и является основой любой хорошей архитектуры. А Модель, Вид и Контроллер это всего лишь результат начальной декомпозиции, предложенной нам талантливыми людьми. Также как паттерны Фасад, Наблюдатель, Компоновщик — это просто инструменты, которые ими были использованы для ослабления связанности и уменьшения сложности. Так что не бойтесь думать самостоятельно, не верьте «брендам» и анализируйте предлагаемые рынком решения, хотя бы грубо, на соответствие принципам. Архитектурные принципы не всегда говорят, как нужно делать, но часто хорошо детектируют то, как делать не нужно. Спасибо всем кто «дотянул» до конца. Будем очень признательны за обоснованную критику. Особенно интересно мнение людей близко знакомых со SmallTalk."], "hab": ["Проектирование и рефакторинг", "Анализ и проектирование систем"]}{"url": "https://habrahabr.ru/post/322272/", "title": ["Как накрутить 40к просмотров на Хабрахабр. Баг или фича?"], "text": ["Всем доброго времени суток, скриншот выше сделан как раз перед публикацией статьи, о нём сегодня и пойдёт речь. В процессе создания и публикации статей на Хабре, я заметил одну очень интересную особенность работы счетчика просмотров. Заключалась она в том, что каждый раз при любом редактировании статьи, которая ещё не опубликована и сохранена как черновик, счётчик каждый раз увеличивается на +1. Получалось, что к примеру к моменту публикации, статья уже могла иметь от 1 до N просмотров. Я решил проверить свою догадку, и создал тестовую статью, которую сохранил как черновик: Вносим несколько изменений, каждый раз сохраняя статью, чтобы удостовериться в том, что счетчик просмотров действительно увеличивается: Хорошо, а что если создать скрипт, который будет делать тоже самое, но без участия пользователя? Наиболее простым вариантом тут было бы использовать JavaScript и запустить исполнение прямо в браузере. Скачав плагин Tampermonkey, я набросал в нём небольшой скрипт: // ==UserScript== // @name New Userscript // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match https://habrahabr.ru/* // @grant none // ==/UserScript== var postID = 322272; (function() { 'use strict'; // Your code here... setInterval(fakeEdit, 1000); })(); function fakeEdit() { if (location.href.indexOf('post/' + postID.toString()) > 0) location.href = 'https://habrahabr.ru/topic/edit/' + postID.toString() + '/'; else { text = document.getElementById('text_textarea'); text.value = Math.random().toString(36).substring(2) +'\\n'+ Math.random().toString(36).substring(2); to_draft = document.getElementsByName('draft')[0]; to_draft.click(); } } Что тут происходит: Мы запускаем бесконечный цикл с интервалом итерации в 1 секунду, цикл в свою очередь выполняет функцию fakeEdit Функция fakeEdit проверяет текущий адрес страницы: 2.1. если на данный момент это страница редактирования, то мы изменяем содержимое поля text_textarea, в котором как раз расположен текст статьи, затем имитирует сохранение, путём клика по кнопке «В черновики»; 2.2. если адрес текущей страницы содержит post, то переходим к редактированию статьи Таймаут тут нужен, для того, чтобы после загрузки страницы, все элементы успели прогрузиться. Запускаем и оставляем его на несколько дней. В результате через небольшой промежуток времени получаем примерно вот такой результат: Я не считаю описанное выше мной — уязвимостью, но всё же перед публикацией этой статьи, уведомил администрацию Хабра о таком нестандартном поведении счётчика, и вот их ответ: Здравствуйте! Приносим извинения за задержку с ответом. Счетчик просмотров, действительно, считает не только уникальные просмотры (собственно, как и подобные счетчики на большинстве ресурсов в сети Интернет). До вашего обращения нам не приходило в голову рассматривать это как уязвимость, ведь злоупотребить этим в нашем сообществе довольно трудно: если плохой материал попадет в «самое читаемое», то привлечет внимание большого числа пользователей, которые, в свою очередь, «сольют» рейтинг материала и карму автора, так что он сам себя накажет, а если попадет хороший, то и не жалко. Конечно каждый решает сам, использовать полученные знания или нет, но главное помнить, что у всего есть последствия. Я решил остановить скрипт на 40000 просмотрах, но вопрос о том есть ли предел, всё ещё остаётся, а так же что произойдёт при превышении этого передела?"], "hab": ["Тестирование веб-сервисов"]}{"url": "https://habrahabr.ru/post/322424/", "title": ["Серия видеоуроков по Git для новичков", "tutorial"], "text": ["Скорее всего, если вас привлекло название статьи, то вы начинаете свой путь знакомства с системой контроля версий Git. В данной статье я приведу 10+ видео о пошаговом вхождении в контроль версии используя Git. Данного курса будет вполне чем достаточно для работы с такими популярными сервисами как GitHub и Bitbucket. Однажды мой знакомый, который только начинал свой путь в ИТ кинул мне данный мемчик что слева, с вопросом \"А чем плохо то?\", поэтому чтобы понимать данную шутку и уметь работать с самым популярным на сегодня VCS (Version Control System) рекомендую к ознакомлению серии видеоуроков, которую я привел ниже. Прежде хочу сказать, что серия по Git не завершена и новые видео активно публикуются каждую неделю. Для тех кто желает следить за серией прошу перейти в плейлист по Git куда добавляются новые видео. Содержание: Урок 0. Подготовка и Введение Урок 1. Первый коммит Урок 2. Проверка состояния Урок 3. Индексация файлов Урок 4. История коммитов Урок 5. Git checkout - Назад в будущее Урок 6. Отмена индексированных файлов Урок 7. Revert - Отмена коммита Урок 8. Решение простого конфликта Урок 9. Ветки и их применение Урок 10. Слияние веток и решение конфликтов слияния Урок 11. Rebase vs. Merge - Что такое git rebase? Очень надеюсь данная серия видео кому-то поможет изучить Git либо улучшить его понимание. Приятного изучения!"], "hab": ["GitHub", "Git"]}{"url": "https://habrahabr.ru/post/322656/", "title": ["Как я писал компилятор С++. Пересказ спустя 15 лет"], "text": ["15 лет назад не было Хабрахабра, не было фейсбука, и что характерно, не было компилятора С++, с выводом диагностических сообщений на русском. С тех пор, вышло несколько новых стандартов С++, технологии разработки сделали гигантский скачок, а для написания своего языка программирования или анализатора кода может потребоваться в разы меньше времени, используя существующие фреймворки. Пост о том, как я начинал свою карьеру и путем самообразования и написания компилятора С++, пришел к экспертному уровню. Общие детали реализации, сколько времени это заняло, что получилось в итоге и смысл затеи — тоже внутри. С чего все начиналось В далеком 2001-ом году, когда мне купили первый компьютер Duron 800mhz/128mb ram/40gb hdd, я стремительно взялся за изучение программирования. Хотя нет, сначала меня постоянно мучил вопрос, что же поставить Red Hat Linux, FreeBSD или Windows 98/Me? Ориентиром в этом бесконечном мире технологий для меня служил журнал Хакер. Старый такой, стебный журнал. К слову, с тех пор, стиль изложения в этом издании почти не поменялся. Виндузятники, ламеры, трояны, элита, линух — вот это все сносило крышу. Реально хотелось поскорей освоить этот весь стек, которые они там печатали и хакнуть Пентагон (без интернета). Внутренняя борьба за то, становиться ли Линуксоидом или рубится в игры на винде продолжалась до тех пор, пока в дом не провели интернет. Модемный, скрежечащий 56kb/s интернет, который занимал телефон на время подключения, и качал mp3-песню в районе получаса. При цене порядка 0.1$/mb, одна песня вытягивала на 40-50 центов. Это днем. А вот ночью, были совсем другие расценки. Можно было с 23.00 до 6.00 залипать во все сайты не отключая изображения в браузере! Поэтому все что можно было скачать из сети за ночь, качалось на винт, и далее прочитывалось уже днем. В первый день, когда мне домой провели и настроили сеть, админ передо мной открыл IE 5 и Яндекс. И быстро ретировался. Думая, что же первым делом искать в сети, я набрал что-то вроде «сайт для программистов». На что первой ссылкой в выдаче выпал совсем недавно открывшийся rsdn.ru. И на нем я стал зависать продолжительное время, испытывая чувство неудовлетворенности, от того, что мало что понимаю. На то время флагманом и самым популярным языком на форуме (да и вообще) был С++. Поэтому вызов был брошен, и ничего не оставалось, как догонять бородатых дядек в их знаниях по С++. А еще был не менее интересный сайт на то время — firststeps.ru. Я до сих пор считаю их метод подачи материала наилучшим. Маленькими порциями (шагами), с небольшими конечными результатами. Тем не менее все получалось! Активно скупая книги на барахолке, я стремился постичь все азы программирования. Одной из первых купленных книг было «Искусство программирования» — Д. Кнут. Не помню точную мотивацию купить именно эту книгу, а не какой-нибудь С++ для кофейников, наверное продавец порекомендовал, но я со всем своим усердием школьника взялся за изучение первого тома, с обязательным выполнением задач в конце каждой главы. Это была самая мякотка, и хотя с математикой у меня в школе не ладилось, но зато с мат.аном Кнута прогресс был, потому что было огромное желание и мотивация писать программы и делать это правильно. Осилив алгоритмы и структуры данных, я купил уже 3-ий том «Искусства программирования» Сортировка и поиск. Это была бомба. Пирамидальная сортировка, быстрая сортировка, бинарный поиск, деревья и списки, стеки и очереди. Все это я записывал на листочке, интерпретируя результат в своей голове. Читал дома, читал когда был на море, читал везде. Одна сплошная теория, без реализации. При этом я даже не догадывался, какую огромную пользу принесут эти базовые знания в будущем. Сейчас, проводя собеседования с разработчиками, мне еще не встретился человек, который смог бы написать реализацию бинарного поиска или быстрой сортировки на листочке. Жаль. Но вернемся к теме поста. Осилив Кнута, надо было двигаться дальше. Попутно я сходил на курсы Turbo Pascal, прочитал Кернигана и Ритчи, а за ними С++ за 21 день. Из С и С++, мне было не все понятно, я просто брал и переписывал тексты из книг. Загуглить или спросить было не у кого, но зато времени было вагон, так как школу я забросил и перешел в вечернюю, в которую можно было практически не ходить, или появляться на 3-4 урока в неделю. В итоге с утра до ночи, я фанатично развивался, познавая все новые и новые темы. Мог написать калькулятор, мог написать простое приложение на WinApi. На Delphi 6 тоже получалось что-то нашлепать. В итоге, получив диплом о среднем образовании, я уже был подготовлен на уровне 3-4 курса университета, и разумеется на какую специальность идти учится вопроса не стояло. Поступив на кафедру Компьютерных систем и сетей, я уже свободно писал на С и С++ задачи любого уровня сложности университета. Хотя, зайдя на тот же rsdn.ru, понимал, как много еще нужно изучить и насколько бывалые форумчане прокаченней меня в плюсах. Это задевало, непонимание и вместе с тем жгучее желание знать все, привело меня к книге «Компиляторы. Инструменты. Методы. Технологии» — А.Ахо, Рави Сети. В простонародье именуемой книгой Дракона. Вот тут и началось самое интересное. Перед этой книгой, был прочитан Герберт Шилдт, Теория и практика С++, в которой он раскрывал продвинутые темы разработки, такие как шифрование, сжатие данных, и самое интересное — написание собственного парсера. Начав скрупулезно изучать книгу дракона, двигаясь от лексического анализа, затем к синтаксическому и наконец к проверке семантики и генерации кода, ко мне пришло судьбоносное решение — написать свой компилятор С++. — А почему бы и нет, спросил себя? — А давай, ответила та часть мозга, которая с возрастом становится все скептичней ко всему новому. И разработка компилятора началась. Подготовка Модемный интернет к тому времени мне перекрыли, в силу смены телефонных линий на цифровые, поэтому для ориентира был скачан стандарт ISO C++ редакции 1998 года. Уже полюбившимся и привычным инструментом стала Visual C++ 6.0. И по сути задача свелась к тому, чтобы реализовать то, что написано в стандарте С++. Подспорьем в разработке компилятора была книга дракона. А отправной точкой, был парсер-калькулятор из книги Шилдта. Все части пазла собрались воедино и разработка началась. Препроцессор nrcpp\\KPP_1.1\\ Во 2-ой главе в стандарте ISO C++ 98 идут требования к препроцессору и лексические конвенции (lexical conventions). Вот и славно, подумал я, ведь это наиболее простая часть и может реализоваться отдельно от самого компилятора. Другими словами, сначала запускается препроцессинг файла, на вход которому поступает С++ файл в том виде, котором вы привыкли его видеть. А после препроцессинга, на выходе мы имеем преобразованный С++ файл, но уже без комментариев, подставленными файлами из #include, подставленными макросами из #define, сохраненными #pragma и обработанной условной компиляцией #if/#ifdef/#endif. До препроцессинга:#define MAX(a, b) \\ ((a) > (b) ? a : b) #define STR(s) #s /* This is the entry point of program */ int main() { printf(\"%s: %d\", STR(This is a string), MAX(4, 5)); } После препроцессинга:int main() { printf(\"%s: %d\", \"This is a string\", ((4) > (5) ? 4 : 5)); } В довесок, препроцессор делал еще много полезной работы, вроде вычисления константных выражений, конкатенации строковых литералов, вывода #warning и #error. Ах да, вы когда нибудь видели в С-коде Диграфы и триграфы? Если нет, знайте — они существуют! Пример триграфов и диграфовint a<:10:>; // эквивалент int a[10]; if (x != 0) <% %> // эквивалент if (x != 0) { } // Пример триграфа ??=define arraycheck(a,b) a??(b??) ??!??! b??(a??) // проеобразуется в #define arraycheck(a,b) a[b] || b[a] Подробнее в вики. Разумеется, основной пользой от препроцессора С++, является подстановка макросов и вставка файлов обозначенных в #include. Чему я научился в процессе написания препроссора С++? Как устроена лексика и синтаксис языка Приоритеты операторов С++. И в целом как вычисляются выражения Строки, символы, контанты, постфиксы констант Структура кода В целом, на написание препроцессора ушло порядка месяца. Не слишком сложно, но и нетривиальная задача, тем не менее. В это время, мои одногруппники пытались написать первый «Hello, world!», да хотя бы собрать его. Далеко не у всех получалось. А меня ждали следующие разделы стандарта С++, с уже непосредственной реализацией компилятора языка. Лексический анализатор nrcpp/LexicalAnalyzer.cpp Тут все просто, основную часть анализа лексики я уже написал в препроцессоре. Задача лексического анализатора — разобрать код на лексемы или токены, которые уже будет анализироваться синтаксическим анализатором. Что было написано на этом этапе? Конечный автомат для анализа целочисленных, вещественных и символьных констант. Думаете это просто? Впрочем просто, когда ты это прошел. Конечный автомат для анализа строковых литеров Разбор имен переменных и ключевых слов С++ Что-то еще, как пить дать. Вспомню допишу Синтаксический анализатор nrcpp/Parser.cpp Задача синтаксического анализатора — проверить правильность расстановки лексем, который были получены на этапы лексического анализа. За основу синтаксического анализатора были взяты опять же простенький парсер из Шилдта, прокаченный до уровня синтаксиса С++, с проверкой переполнения стека. Если мы например напишем: (((((((((((((((((((((((((((((0))))))))))))))))))))))))))))))))); // кол-во скобок может быть больше То мой рекурсивный анализатор съест стэк, и выдаст, что выражение слишком сложное. У внимательного читателя, может возникнуть вопрос. А зачем изобретать велосипед, ведь был же yacc и lex. Да, был. Но на том этапе, хотелся велосипед с полным контролем над кодом. Разумеется в производительности он уступал сгенерированному этими утилитами коду. Но не в этом была цель — техническое совершенство. Цель была — понять все. Семантика nrcpp/Checker.cpp nrcpp/Coordinator.cpp nrcpp/Overload.cpp Занимает соотвественно главы с 3-ей по 14-ую стандарта ISO C++ 98. Эта наиболее сложная часть, и я уверен, что >90% С++ разработчиков не знает всех правил описанных в этих разделах. Например: Знали ли вы, что функцию можно объявлять дважды, таким образом: void f(int x, int y = 7); void f(int x = 5, int y); Есть такие конструкции для указателей: const volatile int *const volatile *const p; А это указатель на функцию-член класса X: void (X::*mf)(int &) Это первое, что пришло в голову. Стоит ли говорить, что при тестировании кода из стандарта в Visual C++ 6, я не редко получал Internal Compiler Error. Разработка анализатора семантики языка заняла у меня 1.5 года, или полтора курса универа. За это время меня чуть не выгнали, по другим предметам кроме программирования, за счастье получалась тройка (ну, ок четверка), а компилятор тем временем разрабатывался и обрастал функционалом. Генератор кода nrcpp/Translator.cpp На этом этапе, когда энтузиазм немного начал угасать, уже имеем вполне рабочую версию фронт-енд компилятора. Что дальше делать с этим фронт-ендом, разработчик решает сам. Можно распространять его в таком виде, можно использовать для написания анализатора кода, можно использовать для создания своего конвертера вроде С++ -> C#, или C++ -> C. На этом этапе у нас есть провалидированное синтаксически и семантически AST (abstract syntax tree). И на этом этапе разработчик компилятора понимает, что он постиг дзен, достиг просветления, может неглядя понять почему код работает именно таким образом. Для добивания своей цели, создания компилятора С++, я решил закончить на генерации С-кода, который затем можно было бы конвертировать в любой существующий ассемблерный язык или подавать на вход существующим Сишным компиляторам (как делал Страуструп в первых версиях «С с классами»). Чего нет в nrcpp? Шаблоны (templates). Шаблоны С++, эта такая хитровымудренная система с точки зрения реализации, что мне пришлось признать, без вмешательства в синтаксический анализатор и смешивания его с семантикой — шаблоны должным образом работать не будут. namespace std. Стандартную библиотеку без шаблонов не напишешь. Да впрочем и заняло бы это еще много-много месяцев, так как занимает львиную долю стандарта. Внутренние ошибки компилятора. Если вы будете играться с кодом, то сможете увидеть сообщения вроде: внутренняя ошибка компилятора: in.txt(20, 14): «theApp.IsDiagnostic()» --> (Translator.h, 484) Это либо не реализованный функционал, либо не учтенные семантические правила. Зачем писать свой велосипед? А в заключении хочу отметить то, ради чего писалась этот пост. Написание своего велосипеда, даже если на это потрачено 2 с лишним года, кормит меня до сих пор. Это бесценные знания, база, которая будет с Вами на протяжении всей карьеры разработчика. Будут меняться технологии, фреймворки, выходить новые языки — но фундамент в них будет заложен из прошлого. И на их понимание и освоение уйдет совсем немного времени. github.com/nrcpp/nrcpp — исходники компилятора. Можно играться правя файл in.txt и смотреть вывод в out.txt. github.com/nrcpp/nrcpp/tree/master/KPP_1.1 — исходники препроцессора. Собирается с помощью Visual C++ 6."], "hab": ["Программирование", "Компиляторы", "C++"]}{"url": "https://habrahabr.ru/post/320988/", "title": ["Gitlab «лежит», база уничтожена (восстанавливается)"], "text": ["Вчера, 31 января, сервис Gitlab случайно уничтожил свою продакшн базу данных (сами гит-репозитории не пострадали). Дело было примерно так. По какой-то причине стала отставать hot-standby реплика базы (PostgreSQL) (реплика была единственная). Сотрудник gitlab какое-то время пытался повлиять на ситуацию различными настройками и т.д, потом решил всё стереть и налить реплику заново. Пытался стереть папку с данными на реплике, но перепутал сервера и стёр на мастере (сделал rm -rf на db1.cluster.gitlab.com вместо db2.cluster.gitlab.com). Интересно, что в системе было 5 разных видов бекапов/реплик, и ничего из этого не сработало. Был лишь LVM snapshot, сделанный случайно за 6 часов до падения. Вот, привожу сокращенную цитату из их документа. Обнаруженные проблемы: 1) LVM snapshots are by default only taken once every 24 hours. 2) Regular backups seem to also only be taken once per 24 hours, though YP has not yet been able to figure out where they are stored. 3) Disk snapshots in Azure are enabled for the NFS server, but not for the DB servers. 4) The synchronisation process removes webhooks once it has synchronised data to staging. Unless we can pull these from a regular backup from the past 24 hours they will be lost 5) The replication procedure is super fragile, prone to error, relies on a handful of random shell scripts, and is badly documented 6) Our backups to S3 apparently don’t work either: the bucket is empty 7) We don’t have solid alerting/paging for when backups fails, we are seeing this in the dev host too now. Таким образом, делают вывод gitlab, из 5 бекапов/техник репликации ничего не сработало надежно и как надо => поэтому идет восстановление из случайно сделанного 6-часового бекапа → Вот полный текст документа"], "hab": ["Системное администрирование", "Администрирование баз данных", "DevOps"]}{"url": "https://habrahabr.ru/post/322674/", "title": ["Синдром самозванца: сражение с усталостью от фронтенда", "перевод"], "text": ["Недавно я разговаривал с другом из бэкенд-разработки о том, сколько часов провожу за программированием и изучением кода в свободное время. Он показал отрывок из книги Дяди Боба «Чистый код». Там разработчики, которые репетируют код перед запуском в работе, сравниваются с музыкантами, которые много часов готовят инструменты к концерту. Мне понравилась аналогия, но я не уверен, что готов полностью подписаться на такое; это тот самый тип мышления, который в первую очередь приводит к выгоранию. Хорошо, если вы хотите углубить своё мастерство и расширить навыки, но если делать это непрерывно в течение всего дня — долго не протянешь. Усталость от фронтенда очень реальна. Я видео много постов об усталости от JavaScript, но мне кажется, что проблема распространяется за пределы этого конкретного языка. Для ясности: это не очередная тирада о том, как всё плохо и быстро меняется — мне нравится, что технология развивается так быстро. Но я в равной степени я могу оценить, насколько проблема может подавлять и временами полностью промывать мозг. Насколько я могу сказать, здесь два уровня проблемы. Во-первых, фронтенд-разработчик думает, что в его арсенале полагается иметь следующее: HTML (писать с листа, семантическая разметка) CSS (масштабируемая и модульная архитектура) CSS-методологии (BEM, SMACSS, OOCSS) CSS-препроцессоры (что-то вроде LESS, SCSS, PostCSS) Современный CSS (Flexbox, Grid) JS Современный JS (ES6, Typescript) Фреймворки JS (Angular, React, Vue [вставьте сюда последние] JS-методологии (функциональное программирование, ООП) JS-библиотеки (Immutable, Ramda, Lodash) Принципы отзывчивого дизайна Тестирование (TDD) Фреймворки для тестирования (Jasmine, Karma) SVG WebGL Техники анимации Доступность Удобство использования Производительность Инструменты для сборки (Grunt, Gulp, скрипты NPM) Сборщики модулей (WebPack, Browserify) Экосистема NPM Знание различных трюков браузера Методологии Agile Сестемы контроля версий (обычно Git) Основы графического дизайна Навыки межличностного общения, тайм-менеджмент Базовое понимание всего, о чём говорят в бэкенде И поверх этого вы возитесь, или пока присматриваетесь к вещам вроде этого: Сервис-воркеры Прогрессивные веб-приложения (PWA) Веб-компоненты Второй пункт — это то, с чем вы не работаете ежедневно и вам не выделяют рабочего времени на их изучение, так как же вы собираетесь убедиться, что в вашем распоряжении есть все инструменты? Термин вроде «прогрессивное веб-приложение» может прозвучать довольно обескураживающе для разработчика. Новые техники и технологии ведут к чувству усталости — усталости от фронтеда (источник картинки) Теперь, как потребитель информации вы можете: Подписаться на ряд различных почтовых рассылок по разработке Изучить свой твиттер-фид Посетить еженедельное образовательное занятие на работе для разработчиков фронтенда Открыть нерабочий канал Slack, где общается горстка разработчиков Следовать онлайновым руководствам (в надежде, что они не устарели) Использовать обучающий видеокурс на сайте вроде Frontend Masters Купить книги по веб-разработке (в надежде, что они не устарели) Посетить митапы Посетить конференции Посетить обучающие курсы Как автор вы можете: Писать статьи в блоги/журналы Выступать с докладами Запустить подкаст Участвовать в open-source проектах Открыть собственные сторонние проекты Недавно я обнаружил, что моё внимание разделяется натрое: я писал код, слушая в наушники вполуха разговоры о коде и обсуждая программирование в чате Slack. Я решил, что это предел — все мои отверстия были забиты кодом и я был умственно опустошён. Хотя это явно экстремальный случай, но я уверен, что некоторые из вас испытывали нечто похожее. Кроме всего этого, у вас вероятно есть работа на полный день, друзья, хобби. Неудивительно, что столь многие из нас ощущают выгорание и сомневаются в правильном выборе профессии. Некоторые из моих знакомых-фронтендщиков выражали желание закончить всё это и перейти на работу, где можно отключаться в пять часов. Но что-то внутри меня подсказывает, что эта работа привлекает определённый тип людей, и даже если вы станете агентом недвижимости, всё равно захотите быть лучшим агентом, насколько это возможно. Будете посещать митапы в агентстве недвижимости, а в свободное время отслеживать тенденции изменения стоимости домов. Много месяцев назад я работал в финансах и по-прежнему изучал их по вечерам и читал книги, чтобы стать самым продвинутым, насколько возможно, в выбранной отрасли. Мы не одиноки в таком поведении. Многие профессии требуют уделять им много времени и изучения за пределами работы. Проблема с фронтенд-разработкой может быть в том, что технология развивается настолько быстро, что возникает ощущение несправедливости, словно кто-то расширяет футбольные ворота во время матча. Похоже, что каждый божий день я получаю письмо с уведомлением, что очередная технология \"XYZ\" мертва. Что никак не может быть правдой, потому что в таком случае я остался бы без рабочих инструментов. Экосистема постоянно изменяется, и я думаю, это хорошо. Лично мне нравится постоянно изучать программирование и подталкивать себя, но нужно признать, что временами это переходит рамки. Имею в виду вышесказанное, вот несколько вещей, которые я стараюсь держать в памяти, чтобы не дать голове взорваться. Некоторые общие советы, как избежать этой усталости. Мы все в одной лодке Все известные мне программисты, как на работе, так и за её пределами, — одни из умнейших людей, которых я знаю. Но все они чувствуют себя перегруженными. У большинства есть некий «список пожеланий» с технологиями, которые он пытается освоить. Может быть, имеется горстка людей наверху, знающих весь свой список, но большинство из нас находятся именно в такой позиции. Мы всё ещё уверены, что Google и Stack Overflow помогут, и у нас слишком много открытых вкладок с ответами на вопросы по веб-технологиям. Вы не одиноки! Нужно понять, что ты не становишься плохим разработчиком оттого, что ещё не попробовал все эти модные штучки. Да, даже «знаменитые технари» в той же лодке. Невозможно знать всё. И рок-звёзды программирования, на которых вы подписаны в твиттере, по-настоящему хороши только в нескольких областях каждый. Вы заметите, что это именно те области, которые сделали их знаменитыми за хорошую осведомлённость. И снова, будут исключения, но все они такие же люди как мы. :) Синдром самозванца есть у каждого из нас Я знаю нескольких отличных фронтенд-разработчиков, которые не хотят претендовать на вакансию, потому что считают чем-то вроде мошенничества претендовать на работу, не соответствуя всем заявленным требованиям. Процитирую одного из них: «90% джуниров, которых я вижу, заставляют меня чувствовать себя далеко позади. В реальности это настолько сильно меня беспокоит, что я думаю о том, как сохранить нынешнюю должность, и просто пытаюсь заработать побольше денег, потому что есть чувство, что с этими деньгами я и уйду» Факт в том, что большинство требований к вакансиям — фарс. Мой друг Бард хорошо проиллюстрировал это на одной картинке, которая даёт перевод текста в вакансии фронтенда. Перевод описания вакансии Просто помните, всё будет нормально. На каждой работе я поначалу чувствовал недостаточное погружение, но в конце концов вы привыкаете к их инструментам и рабочему процессу, вы обучаетесь и становитесь лучшим разработчиком для этого. Не бойтесь учиться на работе, лучший способ освоить новые навыки — ежедневно использовать их. Если у вас синдром самозванца, скорее всего, вы на самом деле приличный разработчик, потому что иначе у вас не хватило бы способностей самоанализа, чтобы осознать это. Придерживайтесь основ Легко отвлечься на блестящие новые штучки, но если базовые принципы слабы, то ваша разработка может не выдержать проверки временем. Как хороший друг однажды сказал мне: «Внимание к основам всегда было моей мантрой. Если ты можешь создать хорошую вещь и решить проблемы, то всё остальное неважно. Инструменты меняются и всегда будут меняться» Например, когда React взлетел к славе, он всегда как будто работал в связке с ES6, и я сконцентрировался на изучении этих изменений или добавлений к языку, а не на особенностях самого фреймворка. Когда React умрёт и исчезнет, со мной останутся знания, которые я приобрёл о последнем стандарте JavaScript. С многими функциями вы можете поиграться прямо в Chrome, так что необязательно запускать Babel и вязнуть в болоте зависимостей. Не нужно изучать всё Это действительно самое главное. Думаю, что нас убивают вовсе не новые фреймворки, библиотеки и модули, а именно наша вера в то, что мы должны все их изучить. С опытом я пришёл к мнению, что лучше всего фокусироваться на чём-то одном — в данный момент я копаюсь в функциональном программировании JavaScript в ES6. В моём списке много вещей, которые хотелось бы изучить, но я стараюсь не отвлекаться. Например, было бы хорошо освежить мои знания по доступности, поиграться с Polymer и углубиться в некоторые из последних техник CSS вроде Grid, но если я научну читать о слишком многих темах одновременно, то не усвою всю информацию. Другие вещи никуда не исчезнут, я займусь ими когда придёт время. Избегайте спешки, пытаясь потребить всё по данной теме. Не спешите и убедитесь, что совершенно поняли её. Если вы похожи на меня, то список тем для изучения у вас будет всё время расти, но не бойтесь его сокращать. Не каждая тема стоит потраченного на неё времени, и следует научиться распознавать, что стоит изучения, а что может исчезнуть в течение нескольких лет. Уделить время изучению шаблонов проектирования и технологий архитектуры всегда будет более полезно в долговременной перспективе, чем прыжки по модным фреймворкам. Всё закончится тем, что вскоре вам придётся снова играть в бинго с умными словечками. Большинство компаний не используют последние технологии Постоянно появляется много новых вещей, веб развивается с ошеломляющей скоростью, но обычно проходит длительное время, прежде чем компании реально начинают внедрять эти новые технологии. Большинство компаний подождёт некоторое время, пока технология созреет и докажет себя на практике. Angular создан шесть лет назад, и я впервые начал работать с ним в стартапе, который выбрал этот фреймворк для себя три года назад. Reactjs существует более трёх лет, а моя текущая компания начала использовать его перед Рождеством. Уверен, что большинство других фреймворков родились и исчезли за это время. Если бы я занялся ими, я был бы сумасшедшим. В сфере CSS Flexbox был доступен с 2010 года — шесть лет назад! Поддержка браузеров до сих пор неполная. Мы начали использовать его в продакшне недавно в этом году, но я не вижу, чтобы другие особо использовали его. Мой тезис в том, что не нужно спешить с изучением всего подряд. Когда технология быстро развивается, ваш работодатель может развиваться гораздо медленнее. Не нужно скакать впереди лошади, просто держи в поле зрения траекторию её движения. Чем больше ты знаешь, тем лучше ты понимаешь, что ничего не знаешь, и это нормально Это абсолютно нормально. Когда ты начинал, ты не знал, чего именно не знаешь. Потом изучил некие вещи и подумал, что ты гений. Затем шаг за шагом эта фантазия исчезает. Ты начинаешь понимать, как на самом деле много вещей вокруг, которых ты не знаешь. По существу, чем больше опыта, тем шире бездна. С этим нужно смириться, иначе она поглотит тебя. Если что, это чувство должно дать тебе уверенность, что ты двигаешься в правильном направлении. В нашей профессии фронтенда ты никогда не будешь комфортно сидеть на троне из накопленных знаний. Не трать всё свободное время на учёбу Несложно почувствовать, что ты настолько позади всех, что нужно постоянно программировать и учиться. Это билет в деревню выгорания. Установи определённое время для развития своих навыков и подумай, можно ли переговорить с начальником насчёт того, чтобы выделить это время, а в остальные часы работай над любимым делом. Некоторые из моих прозрений насчёт программирования пришли в тренажёрном зале. Физические упражнения чрезвычайно важны для вашего ума, как и для тела. Попробуйте 20-30 минут в день, чтобы держать не терять концентрацию и предотвратить выгорание. Уделите время семье и друзьям — не тратьте с ними время на пустую говорильню. Здесь рынок разработчика В наше время не нужно беспокоиться о том, чтобы найти работу. В данный момент мы находимся в очень удачной позиции, когда вакансий больше, чем разработчиков. Не знаю, как долго это продлится, но получайте пользу! Вы можете получить работу, не обладая полными знаниями. Я обнаружил, что во время интервью превосхожу 99% народу, которые только болтают. В крайнем случае, помните о золотом запасе легаси-кода. Если вы из тех, которые любят программировать по старинке, всегда найдутся компании, которые застряли на старом ПО, и им нужны разработчики. Заключение Надеюсь, что некоторые из этих наводок помогут избежать некоторых разочарований, которые могли вас настигнуть. Самое худшее — это дойти до предела и полностью выгореть, потому что если такое произойдёт, очень трудно будет вернуть былую страсть, из-за который вы начали заниматься программированием в первую очередь. Дэвид Бернер — разработчик фронтенда из Великобритании, который программирует с 1998 года, когда ещё были прозрачные разделительные gif'ы и теги <blink>. Он писатель, спикер и программист, увлечённый тем, чтобы двигать веб вперёд. В ы можете найти его бессвязные истории об этом (и любых вопросах фронтенда) на Fed || Dead."], "hab": ["Карьера в IT-индустрии"]}{"url": "https://habrahabr.ru/post/322598/", "title": ["Анализ исходного кода движка Doom: рендеринг", "перевод"], "text": ["От экрана дизайнера к экрану игрока Карты разрабатывались дизайнером уровней в 2D с помощью редактора Doom Editor (DoomED). LINEDEFS описывали замкнутые секторы (SECTORS в исходном коде), а третье измерение (высота) указывалась посекторно. Первый уровень Doom E1M1 выглядит так: После завершения работы над картой она нарезается методом двоичного разбиения пространства (Binary Space Partitioning, BSP). LINEDEF рекурсивно выбирались и их плоскости превращались в секущие плоскости. То есть LINEDEF разрезались на сегменты (SEGS) до тех пор, пока не оставались только выпуклые подсектора (SSECTOR в коде). Интересный факт: И DoomED, и iBSP писались на… Objective-C на рабочих станциях NextStep. Пятнадцать лет спустя тот же язык почти в той же операционной системе выполняет игру на мобильном устройстве! [прим. пер.: в 2010 году Doom вышел на iPhone] Я немного поработал веб-археологом и мне удалось найти исходный код idbsp. На него стоит посмотреть. Ниже представлен пример рекурсивного разделения карты первого уровня. Уровень рекурсии 1 Синим отмечена выбранная стена, превращённая в секущую плоскость (красная). Секущая плоскость выбиралась таким образом, чтобы сбалансировать BSP-дерево, а также для ограничения количества создаваемых SEGS. Зелёные ограничивающие прямоугольники использовались позже для отбрасывания целых фрагментов карты. Уровень рекурсии 2 (только для правого подпространства) В результате секторы SECTORS разделялись на выпуклые подсекторы (обозначаемые как SSECTORS), а LINEDEFS разрезались на сегменты (обозначаемые как SEGS): Процесс работы в целом Вот как выглядит главный метод рендеринга (R_RenderPlayerView): void R_RenderPlayerView (player_t* player) { [..] R_RenderBSPNode (numnodes-1); R_DrawPlanes (); R_DrawMasked (); } Здесь выполняется четыре операции: R_RenderBSPNode: все подсекторы на карте сортируются с помощью BSP-дерева. Большие фрагменты отбрасываются с помощью ограничивающих прямоугольников (показаны на предыдущем изображении зелёным). R_RenderBSPNode: видимые сегменты SEGS проецируются на экран через таблицу поиска и обрезаются с помощью массива отсечения. Стены рисуются как столбцы пикселей. Размер столбца определяется по расстоянию от точки обзора игрока, позиция Y столбца через высоту связана с игроком. Основания и вершины стен создают плоскости visplanes. Эти структуры используются для рендеринга пола и потолка (они называются в коде flats). R_DrawPlanes: Плоскости visplanes преобразуются из столбцов пикселей в строки пикселей и рендерятся на экране. R_DrawMasked: Выполняется рендеринг «предметов» (врагов, объектов и прозрачных стен). Сортировка двоичного разбиения пространства Два примера с E1M1 (первой картой Doom) и BSP выглядят следующим образом: //Начало системы координат находится в левом нижнем углу // Уравнение плоскости ax + by + c = 0 с // единичным вектором нормали = (a,b) // Корневая плоскость (разделяющая карту на зоны A и B): normal = (-1,0) c = 3500 // Плоскость A (разделяющая зону A на зоны A1 и A2): normal = (1,0) c = -2500 // Плоскость B (разделяющая зону B на зоны B1 и B2): normal = (-0.24,0.94) c = -650 // Подстановка координаты любой точки (x,y) в // уравнение плоскости даёт нам расстояние от этой плоскости. Обход BSP-дерева всегда начинается с корневого узла с сортировкой обоих подпространств. Рекурсия выполняется для обоих дочерних узлов. Пример 1: Игрок (зелёная точка) смотрит сквозь окно из точки p=(2300,1900): // Позиция игрока = ( 2300, 1900 ) // R_RenderBSPNode выполняется для секущей плоскости AB (-x + 3500 = 0): -2300 + 3500 = 1200 Результат положителен, значит, ближайшее подпространство находится ПЕРЕД секущей плоскостью. (A ближе, чем B). // Затем R_RenderBSPNode рекурсивно выполняется для двух дочерних узлов корневого узла: секущих плоскостей A1/A2 и B1/B2. // R_RenderBSPNode выполняется для A1/A2 (x - 2500 = 0): 2300 - 2500 = -200 Результат отрицателен, поэтому ближайшее подпространство находится СЗАДИ от секущей плоскости. (A1 ближе, чем A2). // R_RenderBSPNode выполняется для B1/B2 (-0.24x +0.97y - 650 = 0): -0.24 * 2300 + 0.97 * 1900- 650 = 641 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (B1 ближе, чем B2). Результат: зоны отсортированы от самой близкой до самой далёкой: { A1, A2, B1, B2 } Пример 2: Игрок (зелёная точка) смотрит с секретного балкона в точке p=(5040, 2400): // Позиция игрока = ( 5040, 2400 ) // R_RenderBSPNode выполняется для секущей плоскости AB (-x + 3500 = 0): -5040 + 3500 = -1540 Результат отрицателен, поэтому ближайшее подпространство находится СЗАДИ от секущей плоскости. (B ближе, чем A). // Затем R_RenderBSPNode рекурсивно выполняется для двух дочерних узлов корневого узла: секущих плоскостей A1/A2 и B1/B2. // R_RenderBSPNode выполняется для B1/B2 (-0.24x +0.97y - 650 = 0): -0.24 * 5040 + 0.97 * 2400 - 650 = 468 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (B1 ближе, чем B2). // R_RenderBSPNode выполняется для A1/A2 (x - 2500 = 0): 5040 - 2500 = 2540 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (A2 ближе, чем A1). Результат: зоны отсортированы от самой близкой до самой далёкой: { B1, B2, A2, A1 } BSP-деревья позволили сортировать SEGS из любой точки карты с постоянной скоростью, вне зависимости от позиции игрока. Ценой за это стали одна операция умножения и одна операция суммирования для каждой плоскости. Кроме того, благодаря тестированию ограничивающими прямоугольниками отбрасываются крупные части карты. Примечание: Не сразу очевидно, но BSP сортирует все сегменты SEGS вокруг игрока, даже те, на которые он не смотрит. При использовании BSP необходимо применение отсечения по пирамиде видимости. Стены При сортировке BSP стен (SEGS) с ближней до дальней, рендерятся только ближайшие 256 стен. Две вершины каждого SEGS преобразуются в два угла (относительно позиции игрока). Примечание: В 1993 году только самые мощные машины 486DX имели FPU (сопроцессор для чисел с плавающей точкой), поэтому движок Doom вычислял все углы с помощью двоичного измерения углов (Binary Angular Measurement, BAM), работавшего только с числами int, формат float использовался редко. По той же причине точность, превышающая точность целых чисел, достигалась с помощью fixed_t, двоичного формата 16.16 с фиксированной запятой (подробнее об этом можно почитать здесь и здесь). После преобразования в углы координаты X экранного пространства получались с помощью таблиц поиска (viewangletox). Поскольку BAM выполнялись в int, углы сначала масштабировались с 32 до 13 бит с помощью 19-битного смещения вправо, чтобы уместиться в таблицу поиска размером 8 КБ. Затем стены обрезались согласно массиву отсечения (solidsegs. В некоторых статьях о движке Doom упоминается связный список, но не похоже на то, что он использовался). После обрезки оставшееся пространство интерполировалось и отрисовывалось как столбцы пикселей: высота и координата Y столбца пикселей были основаны на высоте сектора SEGS и расстоянии от точки обзора игрока, соответственно. Примечание об отсечении поверхностей: Отсечение невидимых поверхностей выполнялось с помощью angle2-angle1 > 180 . Рендерились только стены, находящиеся в области видимости. Примечание: Не все стены состояли из единых текстур. У стен могла быть нижняя текстура, верхняя текстура и средняя текстура (которая могла быть прозрачной или полупрозрачной). Как заметно на видео ниже, это было удобно для симулирования окон: «окно» на самом деле является сектором с высоким полом и отсутствующей средней текстурой. Интересный факт: Поскольку стены рендерились как вертикальные колонны, текстуры стен хранились в памяти повёрнутыми на 90 градусов влево. Этот трюк позволял полностью использовать функцию предварительного кэширования центрального процессора: процесс считывания тексела стены из ОЗУ также предварительно заполнял кэш ЦП восемью соседними текселами с каждой стороны. Поскольку последующие данные чтения уже находились в кэше ОЗУ, достигалось значительное снижение латентности считывания. Подробнее о предварительном кэшировании и выравнивании данных в памяти можно почитать в книге «The Art of Assembly Language programming» (раздел 3.2.4 Cache Memory). Плоские поверхности (пол и потолок), или печально известные visplanes При отрисовке столбцов стен верхние и нижние координаты экранного пространства использовались для генерирования «visplanes», областей в экранном пространстве (не обязательно непрерывных горизонтально). Вот как объявляется visplane_t в движке Doom. // // Что же такое visplane? // typedef struct { fixed_t height; int picnum; int lightlevel; int minx; int maxx; byte top[SCREENWIDTH]; byte bottom[SCREENWIDTH]; } visplane_t; Первая часть структуры хранит информацию о «материале», (height, picnum, lightlevel). Четыре последних члена определяют покрываемую зону экранного пространства. Если два подсектора имеют одинаковый материал (высоту, текстуру и уровень освещённости), то движок Doom пытался слить их вместе, но из-за ограничений структуры visplante_t это было не всегда возможно. Для всей ширины экрана visplane может хранить местоположение столбца пикселей (поскольку visplanes получаются проецированием стен на экран, они создаются как столбцы пикселей). Вот три основные visplanes начального экрана: Зелёная плоскость особенно интересна: она демонстрирует, что visplane_t может хранить прерывистые (но только в горизонтальном направлении) области. Поскольку столбец непрерывен, visplane может хранить его. Это ограничение проявляется в движке: некоторые подсекторы можно слить и рендерить с помощью одной visplane, но если между ними есть что-нибудь по вертикали, то слияние невозможно. Вот скриншот и соответствующее видео, показывающие фрагментацию visplane. Интересный факт: Жёстко заданный предел visplanes (MAXVISPLANES 128) был большой головной болью для моддеров, потому что игра вываливалась и возвращалась в DOS. Могут возникнуть две проблемы: \"R_FindPlane: no more visplanes\": Общее количество различных материалов visplanes (высота, текстура и уровень освещённости) больше 128. R_DrawPlanes: visplane overflow (%i): при фрагментации visplanes их количество превысило 128. Зачем ограничиваться числом 128? Два этапа конвейера рендеринга требовали выполнения поиска по списку visplanes (с помощью R_FindPlane). Поиск был линейным, и, возможно, для чисел больше 128 оказался слишком затратным. Ли Киллоу (Lee Killough) позже расширил этот предел, заменив линейный поиск реализацией хеш-таблицы с цепочками. Предметы и прозрачные стены После того, как все сплошные стены и стены с «прозрачной средней текстурой», а также поверхности потолков и полов будут отрендерены, остаются только «предметы»: враги, бочки, боеприпасы и полупрозрачные стены. Они рендерятся от самых дальних к самым ближним, но не проецируются в экранное пространство с помощью таблицы поиска стен. Процесс рендеринга выполняется вычислениями двоичных чисел 16.16 с фиксированной запятой. Примечание: В этом видео показан один из наихудших сценариев, когда некоторые пиксели приходится перерисовывать три раза. Профайлинг Загрузка Chocolate Doom в Instruments под Mac OS X позволила выполнить кое-какой профайлинг: Похоже, что порт [на iPhone] довольно точно соответствует «ванильному» Doom: бóльшую часть времени выполняется отрисовка стен (R_DrawColumn), потолка/пола (R_DrawSpan) и предметов (R_DrawMaskedColumn ). Кроме отрисовки я заметил высокие затраты ресурсов на интерполяцию стен (R_RenderSegLoop) и преобразование visplane из столбцов в строки пикселей (R_MakeSpans). Затем, наконец, дело доходит до AI (R_MobjThinker) и обход BSP-дерева (R_RenderBSPNode). С помощью инвертированного дерева вызовов можно увидеть, что бóльшая часть работы и в самом деле заключается в обходе BSP-дерева, рендеринге стен и генерировании visplanes: R_RenderBSPNode (второй столбец — процент потраченного времени). Всё вместе И вот, наконец, видео генерирования легендарного первого экрана, в котором можно увидеть по порядку: Стены как строки пикселей, с ближних до дальних Плоские поверхности как строки пикселей, с ближних до дальних Предметы с дальних до ближних. Интересные факты Поскольку Doom разрабатывался на системе NeXTSTEP с линейной моделью виртуальной памяти, id Software решила отказаться от EMS и XMS, которые использовались в большинстве игр того времени. Вместо этого разработчики использовали DOS/4G, расширитель памяти, позволявший ПО получать доступ к ОЗУ в защищенном режиме в операционной системе с реальным режимом (DOS). Рабочая станция NexT была настолько мощной, что оказалась способна выполнять редактор, игру и отладчик одновременно. Когда игра стала достаточно стабильной, код отправили по сети в PC, где он был скомпилирован под DOS/x86 компилятором Watcom. Благодаря DOS/4G код выполнялся в одинаковой модели памяти и на PC, и на NeXT. Интересное видео, дополняющее книгу «Masters of Doom»: Многие подробности можно изучить на сайтах Джона Ромеро (John Romero) rome.ro и planetromero.com. Рекомендуемое чтение Оригинальный исходный код, выпущенный в 1997 году, удобен для чтения, но в нём нет или почти нет комментариев, он не компилируется, в нём отсутствует исходный код звуковой подсистемы (из-за проблем с лицензированием). Chocolate Doom: О, ДА! Это просто потрясающий порт, он основан на SDL и с brio скомпилируется практически на любой платформе. Именно этот порт я хакнул, чтобы генерировать видео для статьи. Книга «Graphics Programming Black Book» Майкла Абраша. Помогает понять BSP-деревья и является отличным источником вдохновения. Этот парень может даже заставить вас полюбить ассемблер. Masters of Doom: история id Software со множеством подробностей о создании Doom"], "hab": ["Реверс-инжиниринг", "Разработка игр", "Алгоритмы"]}{"url": "https://habrahabr.ru/post/322622/", "title": ["Линус Торвальс высказался о коллизиях SHA-1 в репозиториях Git: бояться нечего"], "text": ["Несколько дней назад сотрудники компании Google и Центра математики и информатики в Амстердаме представили первый алгоритм генерации коллизий для SHA-1. За десять лет существования SHA-1 не было известно ни об одном практическом способе генерировать документы с таким же хешем SHA-1 и цифровой подписью, как в другом документе, но теперь такая возможность появилась. Хеш-функция SHA-1 используется повсеместно, поэтому известие о генерации документов с идентичным хешей вызвало естественную обеспокоенность у пользователей. В том числе у пользователей системы управления версиями Git, в которой тоже используются хеши SHA-1. Развёрнутый ответ на эти опасения дал Линус Торвальс. Если вкратце, то бояться нечего. Линус считает, что ничего критически важного эта атака на поиск коллизий не сделает. По его словам, есть большая разница между использованием криптографического хеша для цифровых подписей в системах шифрования и для генерации «идентификации контента» в системе вроде Git. В первом случае хеш — это некое заявление доверия. Хеш выступает как источник доверия, который фундаментально защищает вас от людей, которых вы не можете проверить иными способами. Напротив, в проектах вроде Git хеш не используется для «доверия». Здесь доверие распространяется на людей, а не на хеши, говорит Линус. В проектах вроде Git хеши SHA-1 используются совершенно для другой, технической цели — просто чтобы избежать случайных конфликтов и как действительно хороший способ обнаружения ошибок. Это просто инструмент, который помогает быстро выявить искажённые данные. Речь не о безопасности данных, а о техническом удобстве дедубликации и выявления ошибок. Другие системы контроля версий часто используют для выявления ошибок методы вроде CRC. Линус признаёт, что SHA-1 используется в Git также для подписи веток, так что в этом смысле он тоже является частью сети доверия, поэтому появление атаки на поиск коллизий действительно имеет негативные последствия для Git. Но в реальности нужно иметь в виду, что этой конкретной атаки очень легко избежать по нескольким причинам. Во-первых, в этой атаке злоумышленник не может просто создать документ с заданным хешем. Ему нужно создавать сразу два документа, поскольку атака проводится по идентичному префиксу. Во-вторых, разработчики атаки на поиск коллизий SHA-1 опубликовали научную статью и выложили инструменты, чтобы распознать признаки атаки. Можно очень легко распознать документы, в которых есть этот префикс, пригодный для генерации второго документа с идентичным хешем. То есть на практике, если внедрить соответствующие меры защиты против документов с этим префиксом, атака будет неосуществима. Кстати, такая защита уже реализована в Gmail и GSuite. Детектор уязвимых документов работает в открытом доступе на сайте shattered.io. Библиотека для обнаружения коллизий sha1collisiondetection опубликована на Github. Когда все данные лежат в открытом доступе, то реальная атака практически невозможна. Авторы научной работы приводят пример атаки на документы PDF с идентичным префиксом. Эта атака успешна, потому что сам префикс «закрыт» внутри документа, как блоб. Если же у нас открытые исходники в репозитории, то это совсем другое дело. Вряд ли можно сделать такой префикс из исходного кода (только из блоба). Другими словами, для создания идентичного префикса и последующей генерации веток кода с одинаковыми хешами SHA-1 придётся внедрить в код некие случайные данные, что сразу же будет замечено. Линус говорит, что есть места, куда можно спрятать данные, но git fsck уже вылавливает такие фокусы. Линус Торвальдс признаёт, что реальным опасением может быть только отслеживание документов PDF средствами Git. Здесь можно порекомендовать использовать инструменты для обнаружения признаков атаки, указанные выше. Такие патчи уже созданы для хостингов github.com и kernel.org, скоро они станут активными, так что здесь нечего волноваться. Ну и кроме всего прочего, Git в будущем уйдёт от использования SHA-1, сказал Линус, есть план, чтобы никому даже не пришлось конвертировать свои репозитории. Но как уже понятно, это не такая уж критическая вещь, чтобы спешить с ней. Кстати, упомянутая Торвальсом проблема отслеживания PDF-документов с идентичными хешами SHA-1 уже проявила себя в системе контроля версий Apache SVN, которая применяется в репозитории WebKit и других крупных проектах. В пятницу вечером на информационном сайте атаки на поиск коллизий SHA-1 появилась новая информация относительно действия атаки на систему контроля версий SVN. Там указано, что PDF-файлы с одинаковыми хешами SHA-1 уже ломают репозитории SVN. Оказалось, что если залить два разных файла с одинаковыми хешами, то система контроля версий не справляется с багом. Кто-то залил такие файлы в репозиторий WebKit, после чего он сглючил и прекратил приём новых коммитов. Вот эти два файла PDF с одинаковыми хешами: https://shattered.it/static/shattered-1.pdf https://shattered.it/static/shattered-2.pdf $ls -l sha*.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:01 shattered-1.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:14 shattered-2.pdf $shasum -a 1 sha*.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-1.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-2.pdf"], "hab": ["Криптография", "Информационная безопасность", "Git"]}{"url": "https://habrahabr.ru/post/322332/", "title": ["Почему не нужно учить python первым языком"], "text": ["Если вы будете искать ответ на вопрос: «Какой язык программирования выбрать первым», то где-то в 90% всех случаев вам будет предложен Python — как наиболее простой в изучении язык. И очевидно, что определенное число людей, которые до этого не учили программирование, выберут Python из-за этих рекомендаций. И вот тут у нас начинается проблема, о которой пойдет речь ниже. Конечно, с описанием того, как я дошел до такой жизни. О себе Еще в студенческие годы я понял, что моя специальность не такая уж радужная, как мне казалось в 18 лет. Поэтому я стал думать о том, как заработать адекватные деньги. И наслушавшись историй о том, как мой двоюродный брат получал безумные на то время деньги в 1С, я также решил связать свою жизнь с IT. Изначально это были шаблонные сайты на конструкторах и wordpress, потом я занялся SEO, и в один момент наткнулся на Хабр, после чего решил стать полноценным программистом. Высшей математики у меня не было, поэтому я решил выбрать сферу, где она не требуется – веб-разработка. У меня появился очевидный вопрос: какой язык выбрать – php/python/ruby. Насмотревшись статей на Хабре, почитал хейт в сторону php, посмотрев пару мотивационных роликов от Yandex. Я выбрал Python. Преимущества языка, я надеюсь, вы знаете, поэтому не буду про это говорить. Первичное обучение языку Обучение языку я совмещал с основной работой, поэтому читал книжки, смотрел туториалы, пилил небольшие проекты в вечернее время. В общем, за год я 1) Изучил книги: Марк Лутц — Изучаем Python Марк Лутц — Программирование на Python Чед Фаулер – Программист Фанатик Билл Любанович – Простой Python 2) Изучил множество роликов от Украинских/Буржуйских авторов по Django 3) Прошел курс от codeacademy 4) Освоил PyCharm Свой первый проект Далее у меня появилась идея небольшого сервиса на весьма специфичную тематику, который я решил сделать, чтобы закрепить знания Python + Django. В создания сайта я 1) Изучил книги: Джон Дакетт — HTML и CSS. Разработка и дизайн веб-сайтов Дэвид Флэнаган — JavaScript. Подробное руководство Бен Форта — Освой самостоятельно SQL. 2) Изучил документацию Django под свои задачи 3) Изучил деплой проектов на Django Gunicorn + nginx + centOS Свой первый нормальный проект После того, как первый адекватный сайт провалился, я решил создать уже что-то стоящее, выбрал идею, выбрал схему реализации и за 3 месяца по вечерам его сделал. Проект показал свою жизнеспособность (по сей день приносит мне определенные деньги, чему я безумно рад). И я решил уже его прокачать получше. После прочтения книги «Percival H. — Test-Driven Development with Python», решил написать тесты сначала на основе компонентов Django, потом поднял документацию селениума, и уже сделал внешние тесты. Я хочу быть крутым Открыв вакансии по Python-Django разработчикам, я посмотрел что еще обычно требуется в таких вакансиях: Django Rest Framework Celery Tornado/Twisted/ asyncio (На выбор что-то одно) Class-based view Django Angular/React (На выбор что-то одно) Потратил 3 месяца на знакомство/пробование с этими штуками. Также поднял стандартную библиотеку Python + внешняя библиотека для парсинга beautifulSoup. Ты не тру без C/C++ Бытует мнение, что без знания C/C++ программист не может называть себя программистом. Поэтому когда у меня было свободное время, я познакомился с книгами: Брайн Керниган – Язык программирования С Стенли Б ЛиппМан – Язык программирования С++. Базовый курс Прочитал книги, поковырялся с кодом, посмотрел на компиляцию, посмотрел примеры кода. В общем, теперь я не делал большие глаза при упоминании ссылок, указателей, сортировок, ООП и туче разных массивов с разными скоростями обработки элемента, в зависимости от его позиции. Я готов к бою! И вот тут мы приходим к самому важному моменту. Потратив в общей массе 2 года на изучение всех элементов веб-программирования, о которых я говорил выше. Я посчитал себя достаточно готовым, чтобы претендовать на позицию веб-разработчика на Python. Конечно, что-то я знал не очень хорошо, что-то поверхносто, а что-то вообще не знал (например, Flask), но общее понимание и навыки были неплохими. И вот тут начались проблемы с Python, на которых люди чаще всего не заостряют внимание. А именно на востребованности бизнеса в Python-разработчиков junior/pre-middle уровня. С этим вопросом я вышел на рынок Хотя на первый взгляд кажется, что вакансий на Python достаточно много, но когда начинается конкретика, все резко меняется. 1. Сразу идет большой отсев вакансий, где Python является исключительно вспомогательным языком. Чаще всего это позиции Java-разработчиков, Системных Администраторов, QA-Автоматизация. Также сейчас идет большой отсев по Data Learning, где требуется мат-образование + язык R. Т.е. с одним Python вы эту вакансию не сможете подобрать. 2. Оказалось, что в моем городе вакансий под Python нет, от слова вообще нет. Расширив поиск по всей области, я также получил неудовлетворительный результат. Пару вакансий на PHP, где Python шел «будет плюсом». Открыв фильтр за последние 3 года, я также обнаружил, что вакансий на Python не было вовсе. Т.е. бизнес в провинции чаще всего выбирает более простые и популярные технологии, нежели Python. 3. Открыв вакансии на Python в общем поисковике, я обнаружил следующие тенденции: 90% + вакансий находятся в Москве или Санкт-Петербурге 90% + вакансий требуют уровень middle+ / seniour ~100% вакансий junior позиций в Москве или Санкт-Петербурге (чаще всего от гигантов) Другими словами получилась ситуация, что если ты не живешь в Москве, Санкт-Петербурге и не собираешься ехать их «покорять», то тебе практически негде получить свою первую работу. Конечно, есть пару очагов, где Python еще используется, например, в Казани. Но чаще всего это какая-то одна фирма, где с Вакансиями тоже весьма middle+ / seniour. 4. Вариант поиска удаленки на текущий уровень также показал, что работодатели не готовы идти на такой риск. Мало опыта + удаленка = это какая-то фантастика. Тем не менее, я все же смог найти пару вариантов, но уже в ходе первичного собеседования стало понятно, что это ерунда по типу: «Ты у нас три месяца поработай, и если клиент заплатит за твою работу, мы тебе тоже заплатим». Не самый лучший вариант. 5. Поговорил с парой HR из крупных компаний, они высказали такую тенденцию. «Мы обычно берем людей с опытом на Python от года, плюс опытом на другого языке (3+ года). Чаще всего php/Java». Другими словами, они вообще не рассматривали варианты, чтобы взять человека с одним лишь Python. 6. Поговорив с ребятами с профильных форумов, стало понятно, что это достаточно типичная ситуация. Из их рассказов стало понятно, что люди после тщетных поисков либо шли работать на php/1c, либо как-то пролазили через upwork/собственный проект/автоматизацию тестирования. Но опять же от случая к случаю. В общем, оказалось, что Python – это отличный язык, который позволяет делать мощные проекты. И так уж сложилось, что их концентрация находится в столицах. И раз это сложные проекты, то и сотрудники туда требуются уже уровня middle+. Готов ли человек, который только что изучил Python получить такую вакансию? Трудно! Но есть другой путь! В настоящий момент только в моем городе находится 24 вакансии на php различного уровня (начиная от небольших компаний, которым нужно поддерживать текущий сайт, заканчивая гигантами e-commerce, которые предлагают последовательное расширение функционала). И примерно столько же вакансии на 1С. И где-то на половине из этих вакансий готовы взять человека, который хотя бы что-то знает в программировании. Скорее всего, это не самые лучшие места, но это уже первая работа, после который вы официально для HR станете программистом с опытом. Так что в итоге Получается ситуация, что можно изучить клевый язык программирования Python и остаться на улице. А можно выучить «ненавистный» php/1c и получить работу. Качество этой работы, конечно же, оставляет много вопросов – но это уже опыт. Что касается меня, то в моих условиях (не ехать в Москву/СПб) я фактически потратил время на изучение языка, который сейчас востребован исключительно в моих собственных проектах. Найти работу на месте или удаленке у меня не получилось. Сейчас иду в сторону php, так как на нем банально есть работа. Поэтому если вы не живете в Москве, СПб, не являетесь студентом тех-вуза, то я бы не советовал вам учить Python первым языком. Обратите внимание на PHP – под него всегда есть места, есть работа, есть опыт. А дальнейший путь развития уже за вами. P.S. Как подсказал мне мой знакомый, на Ruby почти такая же ситуация. Но тут я уже говорить с уверенностью не могу."], "hab": ["Разработка веб-сайтов", "Python", "PHP"]}{"url": "https://habrahabr.ru/post/322704/", "title": ["Игра-головоломка NeoAngle. Работа с уровнями в Unity"], "text": ["Всем доброго времени суток! Я бы хотел вам рассказать историю своей новой игры-головоломки NeoAngle, а также поделиться опытом импортирования, хранения и генерации уровней в Unity. Начну с краткой предыстории 5-летней давности, когда я решил заняться геймдевом, впервые познакомившись с языком программирования action script 3.0 (для разработки flash-игр) в университете. Закончив семестр, я решил, что смог бы осилить собственную игру. Так, посидев какое-то время за блокнотом и карадашом, пришла идея головоломки с треугольниками, где игроку нужно путем переворачивания треугольного камня заполнить заданную форму и закончить уровень на финишной клетке. Таким образом, затратив неделю на разработку в одиночку, была выпущена моя первая flash-игра SeaQuest. И понесло меня разрабатывать дальше. Выпустив еще несколько портальных флэшек, я вернулся к идее с треугольниками, что привело к новой части, где геймплей был полностью переработан, а именно добавлены дополнительные интерактивные объекты: кнопки с препятствиями, телепорты и вращатели. Цель уровня также изменилась, теперь нужно было собрать все жемчужины и прийти к финишу. Результат под названием Stone Quest На этом линейка логических игр завершилась и была успешно мной позабыта. Далее последовал обещанный обвал флэш индустрии, который перекинул меня в unity разработку для мобильных. И вот, в декабре 2016 года, совершенно случайно мне в голову вернулась мысль о треугольно-ориентированных головоломках. Особенно подтолкнуло к разработке то, что геймплей отлично подходит под тачскрин. Было решено использовать механику предыдущей игры, но с другой стилизацией. Вот такая предыстория, которая привела к активной полуторамесячной разработке. Большая часть работы, как ни странно, ушла на геймдизайн. Из предыдущей версии почти все уровни мне показались недостаточно интересными, хотя возможности геймплея имеют гораздо больший потенциал. В следствие чего разработка началась с редактора уровней, который я написал за вечер на старом добром флэше, с возможностью тестирования написанных уровней и их экспорта/импорта в xml. Результат работы в редакторе продемонстрирован ниже: Прежде чем перейти к работе с Unity, мне необходимо было убедиться в том, что я смогу предоставить достаточное количество уровней для мобильной игры, которая требует разнообразный контент. Поэтому первые полторы недели я даже не открывал Unity, а работал в своем редакторе, параллельно занимаясь графикой. К слову, был выбран набирающий популярность ретро-стиль synthwave 80-ых. Он достаточно прост в исполнении, являясь при этом очень привлекательным. Таким образом, создав допустимый набор уровней, я приступил к их портированию в Unity, продолжив тратить часть времени на доработку и создание новых. В связи с этим, возникли следующие вопросы: каким образом импортировать, хранить и генерировать уровни в Unity из xml файла? Найденных вариантов решения было три: 1. При старте игры вычитывать xml файл и каждый раз динамически создавать уровни на runtime. 2. В edit mode сгенерировать уровни и создать на каждый по сцене. 3. В edit mode сгенерировать уровни и создать для каждого префаб. Очевидно, что первый вариант не самый лучший, т.к. все уровни заранее подготовлены и нету смысла их каждый раз заново создавать. Второй вариант больше подходит для более сложных и масштабных проектов, где каждый уровень имеет свою логику, интерфейс, структуру и т.п. В моем случае пришлось бы дублировать интерфейсы и ключевые объекты в каждую сцену с уровнем и при каждом изменении ui нужно было бы обновлять все сцены. Остается последний третий вариант, который как раз очень удачно подошел. О его реализации я и продолжу повествование. Для работы с объектами в edit mode добавим кастомную панель в редакторе Unity. Для этого создаем класс наследуемый от EditorWindow в папке Assets/Editor и добавляем туда следующий метод: [MenuItem (\"Window/Level Loader\")] // Level Loader - название панели в списке Window. public static void ShowWindow () { EditorWindow.GetWindow(typeof(LevelLoader)); // LevelLoader - имя созданного класса } Сразу же проверим, что панель создана в Window -> Level Loader: Далее, в методе OnGUI можно начинать добавлять кнопки и поля для нашего окна. Базовые примеры: void OnGUI() { GUILayout.Label (\"Custom Label\", EditorStyles.boldLabel); // добавление заголовка // создание поля для выбора GameObject GameObject customGO = EditorGUILayout.ObjectField(customGO, typeof(GameObject), true) as GameObject; int customInt = EditorGUILayout.IntField(customInt); // создание поля для ввода int значения EditorGUILayout.Space(); // добавление вертикального отступа if (GUILayout.Button(\"Custom Button\")) // добавление кнопки с обработчиком нажатия { ButtonHandler(); } } Больше информации по компонентам можно найти в официальной документации. А я продолжу описание моего Level Loader’a, продемонстрировав принцип работы ниже: Сперва, мы вычитываем уровни из xml файла, путем нажатия на Read Levels, после этого создается выпадающий список с возможностью выбора уровня и появляются дополнительные контролы, которые позволяют сгенерировать уровень на сцене, создать префаб выбранного уровня, пересоздать префабы всех уровней, показать/скрыть номера полей (для тестирования отрисовки и отслеживания ошибок). Для создания объектов на сцене в edit mode используется стандартная функция Instantiate, а для удаления DestroyImmediate. Какое-то время я создавал префабы уровней вручную, перетягивая их со сцены в папку Resources. Однако, это быстро мне надоело и я полез в интернет за информацией о том, как создавать префабы в edit mode программными средствами. Ниже конструкция, позволяющая это сделать: private void CreateLevelPrefab() { GameObject levelGO = GameObject.FindGameObjectWithTag(\"Level\").gameObject; // игровой объект Level содержит сгенерированный на сцене уровень Object levelPrefab = EditorUtility.CreateEmptyPrefab(\"Assets/Resources/\"+levelGO.name+\".prefab\"); EditorUtility.ReplacePrefab(levelGO, levelPrefab, ReplacePrefabOptions.ConnectToPrefab); } Далее, в режиме игры уровни добавляются на сцену следующим образом: GameObject levelGO = Instantiate (Resources.Load (\"Level\"+levelNum) as GameObject); Так, этапы добавления уровней получились следующие: Создание/редактирование уровня во flash-редакторе с последующим экспортом в xml Считывание xml уровня в unity Генерация уровня на сцене в edit mode Создание/обновление префаба уровня в папке Resources На этом всё. Для меня это был первый опыт работы с объектами в режиме редактирования. Буду рад ознакомиться с вашими идеями по поводу реализации редактора уровней прямо в Unity, миновав Flash. Спасибо за внимание! Поиграть в игру можно в Google Play по запросу NeoAngle."], "hab": ["Разработка мобильных приложений", "Разработка игр", "Unity3D"]}{"url": "https://habrahabr.ru/post/322702/", "title": ["Механический Шекспир: способны ли машины на литературное творчество?"], "text": ["Была ночь, огни Бориспольской трассы пролетали мимо окон такси. Водитель выключил музыку, невыносимо давившую мне на мозг после тяжелого перелета, и, чтобы не заснуть, начал говорить. Сначала, конечно, о политике, «довели страну», и все в таком роде, потом о чем-то личном. Я тоже не хотел отключаться прямо на переднем сидении, поэтому пытался его слушать. —… И тогда нам всем придет конец, — донеслись до меня обрывки фразы. — Точнее только им, не мне. Я надежно подстраховался. Когда их всех: водителей такси, маршруток, даже трамваев выкинут на улицу, меня уже там не будет. Я буду сидеть в тепле, пить кофе и громко-громко смеяться. — Почему-почему их выкинут на улицу? — заспанно переспросил я. — Ты что, про Убер не слышал? Что они с водителями делают — только репетиция, да. Скоро, уже очень скоро они запустят свои автопилоты. Это будет дешевле, безопаснее, круче! Всех этих бездарностей ждет работа на стройке. Или бомжатник. Но не меня, я умнее их. — Да? — протер я глаза. — Да! Я роман начал писать! Остросюжетный! Знаешь, у меня тут, — постучал пальцем по виску, — столько сюжетов сидит — закидайся. Вожу это разных людей, каждый что-то ляпнет. А память у меня с детства — дай Бог каждому. Кто у меня здесь только не ездил: трансвестит, сделавший уже три операции, все никак определиться не может; элитная проститутка — всю дорогу тараторила о шумерской клинописи; любовник министра одного ехал, столько всего нарассказывал… Имена только меняй и пиши — блокбастер будет! —А ... — Единственное, что пишу я пока не очень. Скачал себе пару учебников, читаю после работы. Каждый вечер для тренировки записываю, что услышал за день. Кстати, может ты что-нибудь интересное расскажешь, а то сегодня почему-то голяк полный. Могу даже скидку сделать. — Что у меня интересного… Я программист, лечу вот из ... — Слышь, программист, — перебил меня водитель, — о жизни своей можешь не рассказывать. Она у вас всех одинаковая, неинтересно. Скажи мне лучше другое: эти роботы, которые машины научились водить, они романы скоро писать начнут? Только честно. Я не обижусь, не боись. Просто если скоро — в моем плане нет никакого смысла, нужно  на программиста идти тогда, кому-то ж этих роботов ремонтировать потом, настраивать ... Я, честно сказать, растерялся. С одной стороны, насколько я на тот момент знал, вроде ни один алгоритм еще не умел писать даже приблизительно осмысленный текст, не говоря уже о художественном. А с другой, брать на себя сейчас такую ​​ответственность за чужую жизнь, сказать, что профессия писателя в безопасности, а потом, спустя пару лет, ждать этого таксиста с обрезом у своего дома? — Давай так, — сказал я ему после неловкой паузы. Я тут немного погуглю, почитаю, осмыслю. И напишу статью. Может кому-то еще будет интересно, у кого те же проблемы. Потом я расплатился и ушел досыпать после тяжелой дороги. Утром о своем обещании конечно же забыл. Вспомнил чуть позже, а там уже совесть замучила: пришлось действительно писать. Надеюсь, мой случайный друг, (я так и не спросил, как тебя зовут), ты поверил невыспавшемуся пассажиру и читаешь сейчас эту статью. А если нет — черт с тобой. Кому-то пригодится. Так вот. Пессимисты прошлого На первый взгляд, задача программного написания художественных произведений выглядит просто: Создаем Супер-Мега-Искусственный Интеллект. Заставляем его написать роман. Ну нет, скажете вы, это же робот! Белая пластиковая фигня! Он только команды выполнять умеет, куда ему творить? Он не понимает ни человеческих эмоций, ни мотивов, о чем он вообще может написать? Об электроовцах? С таким мнением вы не останетесь в одиночестве. Также полагала одна очень умная женщина: дочь известного поэта, а заодно первый в истории программист — Ада Лавлейс. Она утверждала, что компьютер, даже очень мощный, не способен ни на творчество, ни даже на то, что мы называем «интеллектом». Ведь любая машина — всего лишь бездумный исполнитель алгоритмов, заложенных в нее человеком. То есть даже если компьютер когда-нибудь и напишет стихотворение или роман — все равно авторские права будут принадлежать кодеру, который его на это запрограммировал. Понять мы ее, конечно, можем. Перед глазами благородной дамы стоял пример батюшки, известного повесы и дебошира: только такой склад характера может быть у настоящего поэта. Если ее любимая, логическая и точная машина обречена стать чем-то подобным — лучше уничтожить ее прямо сейчас, чтобы потом не пришлось отправлять десант в прошлое. С другой стороны, ее ключевой аргумент о «выполнении программой только заранее заданных алгоритмов» в последнее время трещит по швам. Началось все с того, что над сложными программными комплексами начали работать сотни, а то и тысячи людей. И программистов, которые бы понимали, что делает абсолютно каждая функция, в крупных проектах обычно не водится. То есть за счет сотрудничества разных людей, программы уже стоят на уровень выше того, что способен создать один человек. Представьте себе роман с тысячей авторов. То-то же. Дальше — больше. Последние достижения машинного интеллекта базируются на алгоритмах, которые никто и никогда не писал. Программы создают их сами, основываясь на данных, которые им скармливают в невероятных количествах. Дошло уже до того, что на вопрос «как работает программа?» ее создатели лишь разводят руками. Назад в человеческие знания таинственные нолики и единицы просто так не конвертируются. Преемник Ады, Алан Тьюринг, был настроен немного оптимистичнее. Он даже посвятил несколько строк доказательству того, что бессмертная душа может жить не только в человеческом теле, но и в любом другом сосуде. Например, в боевом человекообразном роботе. За такие пассажи его не очень любили на родине, в консервативной Англии. Даже несмотря на то, что, взломав «Энигму», он серьезно повлиял на результаты Второй Мировой, гендерные предпочтения оказались для публики важнее научных достижений, и в конце концов «народ-победитель» довел вчерашнего героя до самоубийства. Программистам Тьюринг известен прежде всего своей «машиной», на основе которой возникли все существующие языки программирования, но популярной культуре более знакомо другое его творение — знаменитый Тест Тьюринга. Мнение о том, что оценить «интеллект» робота можно просто с ним побеседовав, захватила умы ведущих фантастов середины прошлого века и настолько успела всем надоесть, что когда несколько лет назад тест таки был пройден, никто, в конце концов, ничего и не заметил. Дело в том, что специально наученные лексические генераторы кроме как «выдать себя за человека»  собственно ничего и не умеют. А для повседневных проблем гораздо полезнее оказались системы, которые не пытались скопировать человеческое поведение, а просто решали поставленные перед ними задачи. Кстати, похожая революция произошла на столетие раньше в автомобиле- и самолетостроении. Как только конструкторы перестали пытаться создать «металлического коня» или «деревянную птицу», сразу же удалось построить механизмы, оказавшиеся гораздо полезнее произведений слепой эволюции. Человек тоже, если честно, — так себе вычислительный механизм. Будучи предназначенным для конкретной цели — выживания среди диких животных и себе подобных, человеческий организм в некоторых областях безнадежно отстает от примитивнейших механизмов. Какой смысл пытаться имитировать то, что можно сделать лучше? Именно поэтому после Тьюринга понятие «искусственный интеллект» поселилось разве что в фантастических романах, а программисты тем временем взялись за решение конкретных задач, стоявших перед экономикой. В космос летали ракеты, на биржах торговались акции. Компьютеры успешно рассчитывали оптимальные расписания движения поездов и самолетов. Пока романтики зачитывались правилами робототехники, простые работники бобины и перфокарты вводили в ламповые машины алгоритмы разной степени сложности. Логично было бы предположить, что проблему сочинения литературных произведений можно решить аналогичным способом. Мол, выпишем алгоритмы, согласно которым писатель или поэт составляет слова из букв, закинем их в жерло компьютера и вуаля — держите новую поэму. Или стихотворение. Или строчку текста хотя бы, нет? По привычке программисты обратились к экспертам, ожидая получить четкий набора правил бизнес-логики. Когда-то они так же ходили к физикам, железнодорожникам и даже биологам. Последние с математикой дружили так себе, но среди них оказалось много физиков, говорить с которыми программистам было проще. Но с филологами вышел полный швах. Формальные признаки текста, сказали они, конечно же существуют. Стихотворный размер, жанры и поджанры, стили речи и письма, в конце концов. Но если расставить ударения машину научить еще можно (после долгого прописывания примеров вручную), то сформулировать в математических терминах особенности стиля Бодлера филологи отказывались наглухо. Мол, ни в какие цифры настоящее искусство загнать нельзя. И не надо! Ибо кто тогда нам, филологам, зарплату платить будет? Идите словом, дорогие программисты, рассчитывать траектории ядерных ракет. Это нам в ближайшее время понадобится гораздо больше. Вот они и пошли. Единственной составляющей художественного текста, хоть как-то подходящей для алгоритмизации, долгое время был сюжет. Ведь в большинстве случаев в произведении были главные герои, которые совершали определенные действия. Например, двигались из точки А в точку Б. Прямо как ракеты, подумали программисты и взялись за различные генераторы сюжетов. Некоторые, честно говоря, даже давали неплохие результаты. Кто-то умудрялся оформить их в целые книги. Только вот Ада Лавлейс, критически прищурив глаз, заметила бы, что такие программы — не более, чем вариации «шляпы сюжетов», когда автор пишет на клочках бумаги сначала имена персонажей, затем события, а потом по очереди вытягивает бумажки, описывая в литературной форме, что произошло и с кем. И если и есть какое-то творчество в этом процессе, то исключительно со стороны автора, который на этих листочках пишет. А со стороны машины здесь один лишь генератор случайных чисел. Доливали масла в огонь мистики: творчество, мол, — это медитативный процесс, своеобразное общение писателя с духами. Если существуют какие-то правила, то только в их эфирном мире. Пока мы не научим наши компьютеры подключаться туда по HDMI, никакого творчества от компьютеров ждать не стоит. Эту позицию разделяют и авторы учебников по \"creative writing\". Большинство советов там о том, как заставить себя сесть за стол, расслабиться и начать «слушать свою брокколи». После того как она надиктует вам текст, стоит только исправить ошибки, выбросить все наречия и можно отправлять рукопись в издательство. Что ж, метод может и действенный, но для написания программы непригоден абсолютно. Ведь, чтобы запрограммировать что-то, нужно знать, как это что-то работает. Не так ли? На самом деле… не совсем. Но об этом немного позже. Мир как правила и представление Если копнуть глубже, проблема формализации возникает не только в литературе. Философы Древней Греции тоже очень любили давать определение всему, с чем встречались на своем пути. Сначала получалось неплохо. Только вот оказалось, что наибольшие проблемы возникают с простыми понятиями, понятными даже ребенку. Добро, зло, красота, уродство, добродетель, порок, Бог. После нескольких анекдотичных случаев вроде «человек — это двуногое без перьев», Платон устал работать над определениями и постановил, что настоящие правила этого мира, так называемые Идеи, существуют где-то в параллельном мире, а к нам долетают только их тени. Человеческая душа, имея доступ к этому «Идеальному миру», может без труда их различать, а наше мышление, ограниченное миром материальным, понять этого величия не может. Поговорить об Идеях под хорошее вино, впрочем, никто не запрещает. Хотя нет, все-таки запрещает. Витгенштейн, один из ведущих философов двадцатого века, в свое время безапелляционно заявил: «О чем невозможно говорить — о том лучше молчать». Нет, он не был врагом свободы слова. Просто речь, отметил он, в отношении нашего мышления, — откровенно вторична. Она может помочь в базовом общении и даже передаче какой-то части знаний в печатном виде, но то главное, что происходит с нами, — в слова конвертируется слабо. Максимум, что мы можем, — это давать названия неким фактам, элементарной единице этого мира. В отличие от «понятий», «факты» описывают то, что мы видим, без лишних обобщений. Конкретный стол стоит в конкретной комнате при конкретной температуре, атмосферном давлении и уровне освещения. Никакой Идеи Стола в комнате нет, а значит и говорить о ней нет смысла. Весь мир вокруг — просто конфигурация атомов и волн, в которую болезненное человеческое воображение пытается впихнуть термины, которые само не понимает. Ребенок приходит к отцу и говорит: «Папа, я придумал слово карчапан. Что оно значит?». Такими Витгенштейн видел всех философов прошлого. Зато «факты» такими проблемами не страдают. Мы уже знаем, что мы видим. Остается понять — почему? На язык описания «фактов» серьезно претендовала математика. Стройными рядами формул пытались объяснить все: от взаимодействия атомов до несчастных браков. Только вот когда модель становилась хоть немного сложнее, формулы сразу теряли свою элегантность и превращались в десятиэтажные нотации тензоров и «странных аттракторов» (единственный термин, который я до сих пор помню из курса функционального анализа). Таким образом, перевести авангард современной математики в булеву логику оказалось еще сложнее, чем художественные тексты. К тому же и сама математика, как показал Гедель, оказалась не такой уж и всесильной. Словом, полное фиаско. Настолько полное, что часть философов окончательно пустилась во все тяжкие и создала странное учение под названием «постструктурализм». Проблему описания окружающего мира в текстовом формате они решили очень просто. Текст, мол, не описывает, а порождает этот мир. Своеобразное учение Витгенштейна наоборот: если об этом никто не написал — этого на самом деле не существует. Эта точка зрения могла бы очень понравиться программистам, ведь ввести в компьютер текст гораздо проще, чем заставить собирать информацию о мире миллионами разнообразных датчиков. Только вот сами постулаты теории оказались настолько близки к полной чуши, что, кроме самих создателей, их, кажется, так никто и не понял. Поэтому после смерти Фуко и Делеза течение медленно, но верно растворилась в ризоме. Окончательно разочаровавшись в философах, программисты обратились к представителям более приземленных специальностей. Например, к уже знакомым биологам. Уподобиться мозгу Двадцатый век, кроме прорывов в кибернетике и способах убивать друг друга, принес еще и неплохие результаты в понимании того, как устроены люди. Все больше и больше процессов, которые ранее могли объяснить только общением с духами, теперь приписывали мозгу. До полного понимания того, как работает наше сознание, еще, конечно, очень и очень далеко, но в базовых вещах, вроде строения нейронов, наука уже ориентируется достаточно неплохо. Так, судя по всему оказалось, что знаменитая Пещера идей — это, на самом деле, просто область внутри нашего черепа. Мозг, получая из органов чувств информацию об окружающем мире, записывает ее на «жесткий диск»в только ему понятной форме из нейронов и связей между ними. Про «чтение» такой записи речь пока не идет, но мы знаем, что ключевыми характеристиками нашей «пещеры» являются: а) абстракция, то есть объединение черт различных предметов и явлений под одним «ключом» и б) постоянное обновление данных на основе входящей информации. Система хранения данных типа «мозг» — на несколько порядков лучше всего, пока что созданного человеком. И единственным, хотя и очень неприятным недостатком является то, что как в язык, так и в математику все наши знания и весь наш жизненный опыт конвертируются очень и очень плохо. Возникает даже забавная ситуация, когда специалист экстра-класса, способный выполнять сложнейшие задачи, не может объяснить ученикам, как это у него получается. Бедолагам остается только наблюдать за Мастером и пытаться воспроизвести каждое его движение. Как перевести это все на машинный язык, вопрос пока даже не стоит. Казалось бы, очередной тупик. Но ученые, почесав затылок, задумались. А почему бы нам не убрать посредников, неспособных даже между собой договориться, и не выпустить наши программы напрямую во внешний мир? Пусть они сами берут — и учатся! Соблазнительно, не так ли? Первой попыткой было сотрудничество Уоррена Маккалока и Уолтера Питтса в 1943 году. Стоит отметить, что ни один из них не был программистом. Маккалок — нейропсихолог, нейрофизиолог, философ, поэт, имел с математикой достаточно сложные отношения. Но это компенсировал Питтс — вундеркинд-математик, от которого в восторге был сам Рассел. Питтс сбежал из дома в 15 лет, чтобы ошиваться вокруг MIT, куда его упорно не принимали из-за возраста и отсутствия формального образования. Маккалок всю жизнь мечтал построить модель человеческого мозга. Питтс помог ему в этом, упростив нейрон до булевого оператора, получающего на вход двоичные числа и на выходе либо активирующийся (результат «да») или остающийся спокойным (результат «нет»). Такая модель мозга была слишком упрощенной, но вместе с тем ее значимость трудно переоценить. На тот момент самым популярным психологическим течением был психоанализ — полумистическое учение об Иде, Эго и Оргоне, понятиях несовместимых с научным взглядом на мир. Попытка формализовать самый таинственный процесс во Вселенной, человеческое мышление, не могла не выстрелить. После публикации первой статьи «A logical calculus of the ideas immanent in nervous activity» работой пары заинтересовался Норберт Винер, впоследствии сделавший Питтса своей правой рукой. Позже к ним присоединился и сам фон Нейман. Без преувеличения можно сказать, что модель Питтса позволила построить компьютер таким, как мы знаем его теперь. Начиная с определенного этапа, нейронные сети отделились от кибернетики. Моделирование человеческого мозга перестало быть самоцелью, к тому же, новые результаты ученых-биологов показали, что мозг работает несколько сложнее, чем модификация линейной регрессии. Тем не менее, определенные задачи даже таким приемам удавались на ура: базовое распознавание изображений, математическое прогнозирование. Даже медицина получила свою порцию инноваций: нейросети заставляли анализировать кардиограммы. После стремительного взлета часто бывает очень болезненное падение. Так случилось и в этот раз. Известный и очень авторитетный в своих кругах математик и философ Марвин Мински (сосед Питтса по комнате) в 1969 году издал книгу «Перцептроны». В отличие от широкой публики, он понимал, чем именно занимаются программисты, и спокойно, без эмоций, доказал, что в данной интерпретации так называемые «нейронные» сети имеют очень четкие ограничения. Даже оператор «XOR», базовое понятие программирования, однослойными моделями, которые тогда использовались, описать было невозможно. Наложившись на первые неудачи в промышленном использовании и отсутствие новых прорывов в лабораториях, эта книга стала могильщиком нейросетей: отсутствие грантов и, как следствие, отток специалистов отправили отрасль в длительный криогенный сон, из которого она стремительно вышла только полстолетия спустя. Второе пришествие, как водится, стало еще ослепительнее предыдущего. За последние годы нейросети ворвались практически во все области компьютерных наук, и ситуация везде выглядела одинаково: десятилетия исследований, тысячи экспертов, миллионы страниц доводили эффективность алгоритма до фантастической цифры в 80%. А потом приходила нейросеть и без специальных знаний, простым перебором данных показывала результат 90-95%. Так произошло с распознаванием изображений, машинным переводом, генерацией и пониманием человеческой речи, прогнозированием и профессиональной аналитикой. Да что там, сейчас вообще трудно назвать отрасль, куда еще не пролезло вездесущее «машинное обучение». Нейросети играючи раскусили ранее нерешаемую задачу распознавания изображений: научились отличать кота от собаки — им просто показали миллион картинок и сказали, где что изображено. Все характеристики «кошачести», которые не смогли сформулировать сотни ученых за десятилетия исследований, сеть определила для себя сама. Удивив весь мир, несколько лет назад команда AlphaGo преодолела одного из сильнейших игроков в Го. Эта игра, в отличие от шахмат, математическому решению не подлежит — слишком велико число вариантов развития событий. Программе математика и не понадобилась. Вместо этого, она просто «прочитала» все матчи между людьми, доступные в записи, а затем еще несколько миллионов раз сыграла сама с собой. Самым оскорбительным для свергнутого чемпиона стало то, что в обучении программы даже не участвовали известные гроссмейстеры — ей объяснили только базовые правила и критерий победы. Все остальное — набор проб и ошибок. Подход оказался достаточно общим. Совсем недавно таким же образом программа обыграла игроков людей в покер. Что же поменялось, спросите вы, по сравнению с серединой двадцатого века? И почему методы, признанные ранее полностью провальными, теперь вдруг заработали так эффективно? Во-первых, улучшились сами методы. Даже несмотря на отсутствие финансирования, фанатики в пыльных лабораториях одна за другой выписывали сложные формулы, оптимизируя быстродействие и разрабатывая методы избавления от шума. Во-вторых, конечно, железо. Всем известен пример айфона, вычислительная мощность которого превышает суммарные ресурсы машин, отправивших человека на Луну. Как всегда, двигателем прогресса стала индустрия развлечений: современные системы машинного обучения работают в основном на видеокартах, производители которых сходили с ума, пытаясь выдать геймерам картинку пореалистичнее. Третьим, и самым главным фактором, стали данные. Ребенку, чтобы он понял, что такое «стол», достаточно показать один-два примера. Нейросети хорошо учатся на выборках, начинающихся от десятков тысяч. Можно, конечно, попенять на несовершенство современных методов, но в защиту бедного алгоритма следует отметить, что ребенок на самом деле получает гораздо больше информации о предмете. Он видит его с разных сторон, может понюхать, потрогать и даже порой отгрызть кусочек лака, чтобы получить как можно больше разных характеристик. Сеть мы учим на фотографиях — количество каналов уменьшено до одного, еще и без пространственной ориентации. Вот и приходится компенсировать качество количеством. Сейчас человечество выбрасывает в интернет больше данных, чем было создано за всю историю. Развитие цифровой фотографии привело к тому, что, наверное, ни один уголок планеты не остался незадокументированным. С другой стороны, радикально выросло и число текстов, написанных человеком. Теперь значительная часть общения между людьми происходит в сети, а художественных текстов в последнее время стало на порядки больше. Казалось бы, ничто не мешает обучить нейросеть на них и вперед — к новым литературных вершинам. На самом деле, не все здесь так просто. Давайте разберемся почему. В поисках нейропегаса Несмотря на всю свою мощность, сети не могут пока и близко сравниться с человеком в некоторых сферах. Создание осмысленных текстов — в их числе. Причин этому много, но самые важные — две. Первая — те же пресловутые правила. Люди не учатся исключительно на примерах. Им объясняют учителя, дают информацию книги, у людей есть физические и химические формулы, синтаксические законы, словари и энциклопедии. А вот нейронную сеть, обученную на живых примерах, подкорректировать вручную уже невозможно — разве что сгенерировать на основе «правил» дополнительные примеры, но даже в этом случае есть большая вероятность переобучения и, как следствие, — полной непригодности системы для реальной жизни. (Также, кстати, нет никакой возможности перевести правила, обнаруженные машиной, назад в человеческую речь. Поэтому победа над Ли Седолем вряд ли научит чему-то новому игроков-людей: почему сеть сделала тот или иной ход навсегда останется для нас тайной). Вторая и главная разница между нашим мозгом и битами в памяти компьютера — исходные данные. Человек не получает абсолютно все свои знания из окружающего мира. Большинство информации идет в виде «заводской прошивки» — инстинктов, эмоциональных реакций, базовых правил поведения. Этими данными человек обязан миллионам поколений предков, вплоть до бактерий, которые своими постоянными попытками, победами и неудачами формировали генетический код — наверное, самый плотный носитель информации из всех нам известных. Мы не то, что не можем внедрить эти данные в нейросеть, мы и осознать их не способны. Конечно же, обе эти проблемы решаются любимым методом нейронных сетей: «Нам надо больше, больше, БОЛЬШЕ данных», но оказывается, что для решения в лоб задачи такой сложности, как генерация прозы, даже всех букв, когда-либо написанных человечеством, пока недостаточно. Дело в том, что текст — не герметичный набор данных, а скорее отражение внешнего мира, и, чтобы постичь «связи» в тексте, нужно понять «связи» самого мира, или, по сути, построить Универсальную Модель, которая будет знать о мире практически все, что знает среднестатистический человек. То есть опять же построить Сильный Искусственный Интеллект, до чего сейчас еще очень и очень далеко. (На самом деле задачи на понимание мира могут быть вполне тривиальными. Классический пример — предложение “Игрушка не влезла в коробку, потому что она слишком большая”. Что здесь — “она”? Ребенок ответит сразу: он понимает значение слова “большая” и умеет соединить его со значением “не влезла”. Для компютера же оба смысла равны, на уровне самого текста ответа на эту задачу не существует.) Но с другой стороны, что мешает программам просто сгенерировать набор символов, не заморачиваясь их смыслом? Ничего, этим они в последнее время и занимаются. Давайте разберемся как. У продвинутого читателя уже давно должен был возникнуть один вопрос. Нейросети, мол, в большинстве случаев строятся для классификации и прогнозирования. То есть, мы имеем много данных на входе и ограниченный (часто скалярный) результат на выходе. Мы скармливаем сети сотню тысяч квартир с набором параметров и цену каждой из них. Затем натравливаем на еще одну позицию, цены которой мы не знаем. Результат — одно число, прогнозируемая стоимость квартиры. Мы показываем алгоритму миллион картинок кошек и миллион картинок собак, указывая, кто есть кто. Затем показываем новую картинку, и алгоритм должен угадать, кот на ней изображен, или собака. Как такой механизм в принципе можно использовать для генерации хоть чего-то? Первый метод — обратные вычисления. Нейросеть по своей сути — система линейных уравнений, а, следовательно, ее можно спокойно вывернуть в противоположную сторону. Мы не ставим задачу «что на этой картинке?». Вместо этого мы указываем готовый результат (например, «кот») и спрашиваем: «Как по-твоему он должен выглядеть?». Именно так поступили разработчики Гугла в 2015 году. Их результаты, напоминающие скорее произведения сюрреалистов, хоть и продемонстрировали наглядно работу нейросети, на какую-то эстетическую ценность претендуют со скрипом. К тому же, один раз обученная нейросеть с таким подходом может дать только один, детерминированный результат для каждого набора входных данных — ограничение не из приятных. Подходит такой способ для генерации текста? Честно говоря, вряд ли. Вообще говоря, обработка текста по сравнению с обработкой изображений имеет несколько достаточно неприятных аспектов. Во-первых, картинки можно легко сжимать. Вследствие особенностей человеческого зрения, предназначенных для рассматривания далеких предметов, мы даже на иконках 16х16 способны разглядеть какие-то детали. Для текста сжатия без потерь — абсолютно нетривиальная задача. Нельзя просто взять и объединить соседние символы, как мы поступаем с пикселями. Вместо этого нужно искать обходные пути — анализировать части речи, чтобы выбросить эпитеты, строить модель событий, чтобы объединить несколько предложений в одно. Словом, вся «слепота» нейросетей к бизнес-логике сразу куда-то исчезает. Во-вторых, текст гораздо чувствительней к шуму. Если для картинки несколько не слишком ярких точек не на своих местах погоды не сделают, то слу»ча%йны*е си$мво^лы, разбросанные по тексту, абсолютно неприемлемы — читать такие опусы чрезвычайно трудно, а порой от замены всего нескольких брюкв вообще меняется весь смысл. Второй способ генерирования чего-то нового построен на умении нейронной сети прогнозировать. Пусть текст, предполагают адепты этого метода, является определенным временным рядом. И каждая следующая буква каким-то образом зависит от предыдущих. Теперь остается только научить систему на заданных «взаимозависимостях» и заставить ее прогнозировать, какие символы должны быть в тексте следующими. Несмотря на тривиальность этого подхода, результаты, полученные с его помощью, впечатляют. Первая программа-генератор, описанная Андреем Карпати, умышленно оперирует исключительно символами в отличие от аналогов, пытавшихся искать взаимные связи между словами. И даже в такой постановке удалось получить хоть и бессмысленный, но читабельный и даже стилистически близкий к оригиналу кусок текста. Генерации подверглись Шекспир, речи Обамы, рекламные заголовки и даже тексты выступлений на конференции TED. Интересные выводы можно сделать из генерации карт для игры Хартстоун. На первых порах система выдает полную ерунду, но уже после нескольких итераций некоторые карты начинают выглядеть осмысленно — их уже спокойно можно включать в игру. Мощности современных алгоритмов хватает даже для поэзии (в том числе, китайской и хокку ) или текстов песен, но успех там, честно говоря, достигается в основном за счет ручного выбора исследователем приличных вариантов. Хорошую прозу так пока сгенерировать не получится. Во-первых, проблема в объеме. Как говорилось ранее, сжимать текст, особенно художественный, — достаточно трудно. А количество взаимосвязей длинного прозаического текста пока превышает вычислительные возможности даже самых мощных кластеров. Вторая проблема — обучение с учителем. Большинство данных, которые сейчас используются для машинного обучения, были когда-то размечены людьми вручную. Существует даже специальный ресурс «Механический турок», где люди за небольшую плату помогают ученым со всего мира обучать свои алгоритмы. Для размещения на этом сайте задачи должны занимать максимум несколько секунд эффективного времени обычного человека: «Есть ли на картинке лицо?», «В какую сторону поворачивает дорога?», «Какое слово говорит голос на записи?». За счет простоты, можно получить очень большую базу данных за относительно короткое время. Заставить пользователей читать рассказы, сгенерированные работами, трудно. Во-первых, текст, составленный наполовину из тарабарщины, а наполовину из гениальных предложений, все равно остается бредом. Во-вторых, даже если роботы когда-нибудь научатся писать что-то осмысленное, второй вариант того же текста не будет иметь того же эффекта, что первый, — читать одно и то же скучно. Значит, придется наполнять базу произведениями людей. Но и здесь возникает вопрос: какими должны быть критерии оценки? Как мы уже упоминали ранее, машинное обучение конвертирует большие объемы данных в небольшой результат. Как выглядит результат для прозы? Первое и наиболее логичное предположение — написанное должно иметь хоть какой-то смысл. То есть текст, пусть даже сгенерированный с помощью машинного обучения, нужно сначала преобразовать в последовательность событий и определить, не противоречат ли они друг другу. Понимание текста, в отличие от генерации, продвинулось гораздо дальше. IBM Watson недавно победил в викторине, Google построил базу данных смыслов слов разных языков. На рынок постепенно выходят анализаторы текста для воспроизведения событий на основе тысяч статей в интернет-изданиях. Чрезвычайно важной является задача понимания человеческих команд — интернет вещей постепенно перекочевывает из фантастических сериалов в реальный мир, и ваше общение с микроволновкой через какие-то год-два превратится из первых признаков помешательства в практическую необходимость. Людей трудно переучивать на язык инструкций — роботы должны просто понимать, что от них хочет хозяин. Последствия этой революции могут сыграть значительную роль в генерации художественных текстов — программа сможет прогонять свои «творения» через базовые фильтры и абсолютного бреда на выходе уже не будет. Но этого, безусловно, мало. Ведь художественная литература — это много больше, чем просто описание событий. Поэзия — вообще в большинстве своем несюжетная. Попытки разобраться, что на самом деле означает та или иная метафора — работа не из легких даже для исследователей из плоти и крови. Однако, одно правило остается практически неизменным: любое искусство существует для того, чтобы вызвать у нас эмоции. Печаль, радость, злость, страх — читатель должен сопереживать героям. Соответственно, значения слов можно и не знать, достаточно просто провести параллели между предложениями и реакцией на них. Осталось только посадить миллион человек за машины и заставить их читать тексты, описывая после этого свои эмоции. Хотя нет, учитывая масштабность задачи, миллиона будет мало — надо как минимум миллиард. Вряд ли «Механический турок» потянет такие объемы — понадобится новая система, полностью заточенная под эту цель. Еще одна проблема — сами тексты. Можно начинать с каких-то базовых кусочков — старых анекдотов или газетных вырезок, но нашему миллиарда «подопытных» такое быстро надоест. Желательно, чтобы тексты они генерировали сами, хотя бы по несколько десятков каждый. Объем не так важен, главное — количество. Чем больше, тем лучше. Теперь главное — как заплатить миллиарду людей? Где взять деньги на такое масштабное исследование? Может попросить их работать бесплатно? Ради блага науки, например. Или для собственного развлечения. И чтобы весь эксперимент выглядел веселее, можно вместо обычного интерфейса добавить иконки. Например, такие: Итак, основной проблемой сегодняшнего дня является не генерация, а именно анализ текстов. Как только машина научится понимать и даже в каком-то смысле «сопереживать» творением людей, выделяя гениальные произведения среди гигабайт чуши и графомании, проблемы генерации уже не будет. Да что там, издатели и сейчас готовы заплатить миллионы за программу, которая будет способна выбрать шедевры из тысяч опусов, ежедневно появляющихся на самиздате Амазона, причем желательно, чтобы она делала это быстрее и эффективнее, чем бедные студенты-филологи, которые занимаются этим сейчас. Развитие индустрии, таким образом, будет двигаться с двух сторон: нейросети генерации будут совершенствоваться, чтобы извлекать больше информации из данных (RNN, LSTM), а нейросети распознавания будут приближаться к истинному пониманию текста, и, как следствие — настоящему пониманию окружающего мира. На уровне человека, а скорее сильно превосходя нас самих. *** В конце хочу отметить одну очень важную вещь. Большинство результатов, описанных в этой статье — очень новые. Речь идет даже не о десятилетиях: вышеупомянутая статья Андрея Карпати вышла в мае 2015 года! Прямо сейчас мы находимся на очередном витке взрывного роста: растет количество ученых, занятых в индустрии, компании тратят миллиарды долларов, чтобы не отставать в гонке Искусственного Интеллекта. Технологии тоже не стоят на месте: хардварные компании одна за другой выпускают железо, предназначенное в первую очередь для машинного обучения. И поэтому, хоть на данный момент и не существует ни одной системы, способной на создание хоть немного осмысленной прозы, шансы на то, что подобная появится в ближайшее время —очень и очень высоки. (Программистов угроза, кстати, тоже не миновала). 11 сентября 1933 на ежегодном съезде Британской Академии Наук лучший физик-ядерщик того времени Эрнест Резерфорд безапелляционно заявил, что все, кто занимается добычей энергии из трансформации атомов, — шарлатаны. И все их исследования — не более, чем очередной виток лженауки. Уже на следующее утро молодой исследователь Лео Силард описал принципы цепной реакции, благодаря которым через несколько лет появится первый ядерный реактор."], "hab": ["Машинное обучение"]}{"url": "https://habrahabr.ru/company/everydaytools/blog/322694/", "title": ["Как двухлетний репозиторий на GitHub стал трендовым за 48 часов", "перевод"], "text": ["GitHub предоставил возможность миллионам разработчиков с легкостью публиковать свои проекты и тем самым привлекать пользователей и единомышленников. Часто перед разработчиками возникает проблема неэффективного использования ресурсов — они тратят сотни часов на создание проекта с целью продвинуть его на GitHub, а получают максимум две звезды. Я оказался точно в такой ситуации, разрабатывая проект для некоммерческой организации Hack4Impact. Это студенческая группа, которая работает над техническими проектами для общественных организаций. Вместе мы разработали flask-base, которая использовалась как шаблонный код для всех наших продуктов. База данных включала в себя основные элементы flask веб-приложения: SQLAlchemy, Redis Queue и аутентификацию пользователя (наряду с несколькими другими функциями). Вы можете ознакомиться с нашим репозиторием по ссылке. Демонстрация бэкенда flask-base Разработанная нами flask-base относится к категории «plug and play», что является главным его преимуществом. Установить работоспособную версию на компьютер не составляет большого труда (и ее можно запустить на хостинге, таком как Heroku). Кроме того, база крайне минималистична по сравнению с аналогами, ее очень удобно кастомизировать. Разработка flask-base заняла два года и послужила шаблоном для 90% наших технических проектов. Проект помог претворить в жизнь продукты для таких организаций как Kiva, OSET, Juvenile Law Center, Givology. Мы смогли помочь общественным организациям Америки достичь социального влияния, к которому они стремились. Несмотря на все наши попытки популяризировать наш код, flask-base осталась неизвестным в широких кругах, но стало полезным для людей, работающих с ней. С чего мы начинали Наши провальные попытки расширить аудиторию приводили к отчаянию, так как мы были уверены, что другие разработчики и небольшие компании могли бы использовать плод нашего труда для значимых проектов. Но мы никак не могли найти способ продвижения нашего продукта. В результате этого, мы подвергли сомнению честность источников с открытым исходным кодом, которые должны освещать отличные идеи для широкого круга потребителей. То самое чувство Впоследствии мы обнаружили ошибку нашего подхода. Мы внимательно изучили, каким образом работает открытый исходный код с точки зрения пользователя. Мы нашли ключевые области, которые нужно было пофиксить и усовершенствовали наш проект, чтобы он стал пригодным для реального мира.Мы попали в точку. Я запостил наш продукт в сабреддит /r/Python. В течение 48 часов наш репозиторий получил более 200 звезд по сравнению со стартовыми 9. И мы продолжали расти. Внезапно мы начали получать комментарии и предложения от людей, которые были заинтересованы в нашем проекте, и это было превосходно. прошлые дни += 544 stars, 74 forks, and 16 watches В этой статье мы хотим показать, каким образом наша flask-base заняла свою лидирующую позицию на GitHub. Если у вас есть значимый проект, вы можете легко использовать наш опыт и получить из него максимум. Путь начинается с исследования Мы начинали с анализа историй успеха. Популярные репозитории на GitHub имеют сходства: Обзор продукта содержит иллюстрации и гифки; Документация; Статистический анализ кода; Уточняющие инструкции; Хорошо описанная инструкция к установке; Логотип. Разбивка первых строк README Хочу обратить ваше внимание на некоторые из лучших репозиториев GitHub: React-Router: более 19 тысяч звезд, 4,5 тысячи forks. ReactnRouter полезен в контексте управления отдельными страницами веб-приложений, а также является редким репозиторием, служащий туториалом по использованию структуры. Также он содержит исчерпывающий гид по установке, наряду с рекомендациями к ошибкам, с которыми могу столкнуться пользователи. Webpack: 23,5 тысячи звезд, 2,7 тысячи forks. Webpack можно назвать одним из лучших инструментов фронтенд-разработки, благодаря надежности и возможности разработки для разнообразных браузеров. README состоит из десятков знаков и примеров использования кейсов со ссылками на документацию. Webpack также подчеркивает роль общества в поддержке проекта (в частности, они имеют Sponsor и Backer секции) Теперь приведем пример неудачного репозитория: abhisuri97/leARn: я выбрал свой собственный репозиторий в качестве плохого примера – Hackathon проект, который выиграл PennApps XIII VR/AR и попал в Топ 10. Это был единственный проект, который я разрабатывал на Unity, имеющий огромное количество посторонних ненужных файлов. Наряду с подробным описанием функционала проекта, он не объясняет, как проект работает на конкретных системах и в чем его функции. После анализа множества репозиториев и мониторинга основных трендов мы выделили ключевую концепцию: разработчик, изучающий ваш репозиторий нуждается в веской причине, почему ему стоит попробовать именно ваш проект, то есть в понимании, что продукт максимально прост и удобен в установке. A.G.D. (Attention Grabbing Device – Методы привлечения внимания) Красноречие и README: В прошлом я принимал участие в конкурсах ораторского искусства, один из них назывался «Original Oratory» («Подлинное Красноречие»). Требовалось продекламировать жюри 10-минутную речь собственного написания. Каждое мое выступление начиналось с 2х минутного введения. Чаще всего это была история, предшествующая тезисам для речи, которые я собираюсь освещать. По сути, README является A.G.D. вашего проекта! README – первое, что увидит пользователь, обращаясь к вашему репозиторию. В связи с этим, стоит обратить пристальное внимание на его наполнение. Но что является первостепенным и как завладеть вниманием пользователя? При ознакомлении с вашим проектом, пользователь должен знать: что это; насколько хорош код; какой саппорт доступен; что входит в проект; как он выглядит; как установить проект. Давайте пройдемся по всем пунктам подробнее. Что это Огромный логотип сразу расскажет о вашем продукте Самый простой вопрос о репозиториях, но многие люди неправильно его толкуют. Ваш проект один из миллионов существующих. У вас крайне мало времени, чтобы произвести впечатление. Опишите ваш проект в твите (около 140 символов) – без лишних деталей: для них отведен раздел фич. Логотип также поможет, т.к. он выделяет название проекта среди черно-белого текста README (а также показывает ваши усилия, приложенные для его создания). Насколько хорош код. Тот самый вопрос, на который 90% репозиториев не в состоянии ответить. В то время как определение «хорошего» кода субъективно, есть несколько основных характеристик: он хорошо протестирован; пройдена проверка стиля (ESlint); он может быть составлен в текущем виде; он прошел статистический анализ (через такие сервисы как (Code Climate). Дашборд Code Climate выдает вам cредний балл качества кода Досконально изучать ваш код перед его использованием разработчикам не интересно. Равно как и «символы» на первой строке проекта. Особенность этих символов в легкости их применения, что делает проект действительно надежным в глазах посетителей, и им не нужно тратить время на изучение самого кода. Какая техподдержка доступна Саппорт состоит из двух видов: проблемы и обучение для пользователя. Саппорт по проблемам может быть реализован в FAQ. Но для новых проектов нет возможности узнать скрытые проблемы старого кода (и нет контента для FAQ). Единственное решение в данном случае – отвечать на вопросы, по ходу их возникновения и оперативно фиксить баги. Документация flask-base, созданная с помощью mkdocs Второй тип саппорта — это документация. Данная задача очень трудоемкая для разработчиков, но она крайне важна в контексте популярности проекта (и должна быть написана в любом случае). Проще всего создать документы в mkdocs, и можно сгенерировать gh-страницы из mkdocs CLI, которую впоследствии можно перенести на хостинг GitHub. Пользователи получат хорошие примеры того, как можно использовать проект и ознакомятся с объяснением тонкостей благодаря грамотно составленной документации. А также она будет детальным гидом к запуску проекта (если это веб-приложение). Что входит в проект X для Y Список фич должен содержать ключевые особенности, входящие в демо-версию, то есть не должен быть чрезмерно обширным. Максимум 10 пунктов в формате «Х для фичи Y» Как он выглядит Демонстрационный пример функции редактирования страницы администратора flask-base Картинка всегда лучше тысячи слов, поэтому лучше сделать .gif. Покажите как приложение работает, даже если это выход из командной строки. Данная информация предоставит возможность разработчику увидеть а) как будет выглядеть проект б) подходит ли для его нужд. Не стоит недооценивать влияние хорошей графики на выбор разработчика в пользу вашего проекта. Как установить проект Копирование репозитория Инициация Virtualenv (Если вы работаете на маке) Убедитесь, что у вас установлен xcode Добавьте переменные окружения Cоздайте файл типа .env, содержащий переменные окружения, используя следующей синтаксис: ENVIROMENT_VARIABLE=value. Например, переменные окружения почты могут быть заданы таким образом: Мы рекомендуем использовать Sendgrid в качестве почтового SMTP сервера, но любой другой также будет работать без проблем. В процессе разработки вы работаете на одном компьютере, на котором уже установлены все нужные утилиты. Но пользователь должен иметь возможность установить ваш проект и начать с ним работу за 3-4 шага. Если это предполагает создание MakeFile, сделайте это. Обязательно стоит предупредить, если вы использовали «глобальные» инструменты, такие как babel-cli, babel-core. Согласно основному правилу, если пришлось их использовать, то и другим придется это делать. Не забудьте сжать скрипты в единый файл (для Python это requirements.txt, а для node/javascript – package.json). Короче говоря, установка проекта должна занимать не более 5 минут. Как попасть в тренд Удержать пользователей поможет вам A.G.D. (README). Но как привлечь пользователей в ваш проект? Есть три решения: Hacker News/Product Hunt: оба предоставляют отличную возможность осветить проект для заинтересованного круга разработчиков (и получить медиа обзор). Но существует сложность попадания в топы – размещение и продвижение проекта требует тщательного планирования и помощи от пользователей на старте. Reddit: самый эффективный метод получить стартовые звезды для вашего репозитория. Но нужно определить целевую аудиторию. Для нашего продукта этой аудиторией был /r/Python, где мы без труда вышли в топы.Важно обратиться к аудитории, заинтересованной в вашем проекте. Но нужно быть осторожным, если вы публикуете свой пост на таких крупных сабреддитах, как /r/Programming, где ваш пост может запросто затеряться среди прочих. Workshops: не секрет, что воркшопы – отличный способ получить десятки звезд. Сделайте воркшоп о том, как вы создавали проект, как он функционирует и как его использовать. Veronica Wharton and я проводим воркшоп по Flask на PennApps XV Мы использовали этот метод на PennApps XV: с помощью обучающего воркшопа создания веб-приложения с Flask. Аудитория состояла из примерно 40 человек, и мы показали свой продукт как пример Flask-приложения, которое они могли использовать во время хакатона. Через 5 минут после окончания воркшопа мы проверили свои показатели: мы получили 17 звезд и 8 forks, что прибавило нам оптимизма. Мониторинг статуса Комментарии по улучшению. Будьте милы, делитесь своим мнением и радуйтесь обратной связи. Неизбежно появится тот, кто обнаружит баги после запуска вашего проекта. Удостоверьтесь в том, что вы ответите на все комментарии и учтете фидбек. Находиться в контакте с аудиторией является ключевым моментом получения отдачи от пользователей. Если вы получите рост с 30 до 40 звезд за короткий период (1-2 часа), значит ваш проект имеет отличный шанс стать трендом (естественно, данная информация касается алгоритмов работы трендов в GitHub). Топ трендинг Flask-base среди репозиториев Python на Github после 24 часов Наши достижения Проект вышел в топы для репозиториев python, 3 место в тренде и топ для /r/Python за неделю. Hack4Impact стал 4м топовым python-разработчиком и 5м среди всех разработчиков. Кроме этого, у нас более 80 клонов и 40 forks в настоящий момент. Могу сказать, что это безумно приятное ощущение, когда ты видишь, что люди используют код, который ты помогал писать. Письмо с благодарностью, полученное мной Наша аналитика на GitHub. Reddit реально помог. Если вы не смогли попасть в тренды, не отчаивайтесь. Просто встряхнитесь и повторите. Всем иногда везет, а иногда нет. Если вы предприняли попытку создать открытый исходный код, полезный для людей, вы и так уже вносите свой большой вклад в мир open source."], "hab": ["Python", "Open source", "Блог компании Everyday Tools"]}{"url": "https://habrahabr.ru/post/322676/", "title": ["Анимационная пасхалка, или дань уважения студии при увольнении из нее"], "text": ["На Хабре много статей. Но не каждая показывает, как размышлял автор, его грабли и действия. Здесь я хочу вам рассказать, как я делал пасхалку для логотипа веб-студии в которой я работал. Как то раз увидев завораживающую анимацию от создателя библиотеки mo-js для svg-эфектов, я загорелся и решенил сделать что-то подобное. Как раз мне на глаза попалась обновленная главная страничка нашей студии, недолго думая я собрался сделать анимацию для логотипа. И раз эта статья рассказывает, как все это происходило, то к моему сожалению я не стану пересказывать весь мануал по данной библиотеки. Его вам придется прочитать самим, если конечно захотите сделать что-то подобное. Ну а чтобы сразу вникнуть в статью: → Ссылка на демо просмотр только от 1204x400, кликнуть на колокольчик → Ссылка на GitHub Первые начинания Мы все ошибаемся, но не всегда понимаем, что идем по каменистой дорожке. Изначально я хотел разбивать логотип на мелкие кусочки, сжимать их в 1 точку, и только потом как пазл собирать в единое целое со всевозможными эффектами (думаю это было бы красиво). Но быстро понял, что делаю очень быструю анимацию и естественно она будет без звука. Когда логотип создателя библиотеки, как я написал, был «Завораживающим». Свое второе вдохновение я нашел в одной навязчивой мелодии (Clannad песенка булочек), думаю многие щас вздохнули в меланхоличном припадке. Да она просто умиляющая. И раз это колыбельная, то и тематика ей должна быть подстать! Сюжет выдумывать особо и не пришлось. Колыбельная — «ночь, звезды, луна», замечательные ассоциации, большие варианты развития сюжета моей пасхалки. Первым делом я нашел минус этой песенки и нарезал ее до 30 секунд, это максимум времени, сколько может выдержать любой гость, не закрыв страницы (думаю на большее меня бы и самого не хватило). Далее сделал метки эффектов, наиболее подходящим инструментом я выбрал Adobe Audition CC 2017 (честно говоря другого инструмента я и не искал, просто он мне попался на глаза первым). Выглядело это так: Библиотека предоставляет вложенные временные шкалы объекта TimeLine(), я бы мог разделить все на отдельные блоки, и начинать их анимацию по отдельности. Но т.к. у меня была привязка к мелодии, я счел необходимым сделать все на 1 единой временной шкале, дабы не запутать себя самого с таймингом. Просто сделал большие блоки массивов с временем начала в виде предоставляемого библиотекой параметра задержки (delay). Радужный дождь и взращивание букв С точки зрения обычного пользователя, сюжет анимации всегда довольно простой. Падают радужные лучи, вырастают буквы, появляется рельеф, звезды и выходит луна, финиш, еще можно что-то там повозить. Но редко кто описывает, как он это делал, сколько вложил сил, терпения и слез своей музы. Структуризация кода всегда помогает мыслить в рамках отдельных модулей и у меня получилось 5 объектов для манипуляций над элементами анимации. FirstRainbow Анимация падения радуги мне далась довольно легко, я банально скопировал ее с примера в тутореал. Сделал массив с координатами и описал это в 1 методе rainbow(200, 500, 'A', 'str1') где параметры — это продолжительность, время задержки (в моем случае начала) анимации и последние 2 описывают координату. Эффект падения получился невзрачный: Скрытый текст Изменили на улучшенный: Скрытый текст Letters Описывает анимацию букв. Он стал первым по сложности для меня объектом. Концепция анимации заключалась в росте, будто растения. С буквами пришлось изрядно повозиться, думаю не каждый умеет работать в Illustrator (я в том числе). И как человек получивший в этом опыт, советую: Всегда вычищайте код svg после редактора. Переименовывайте классы в уникальные Не забывайте удалять лишнии слои. Потратил 60% общего времени только на синхронизацию, сверку координат, отрисовку отдельных линий (которые нельзя сымитировать стандартными фигурами). Мне приходилось по 20 раз менять 1 параметр, ради достижения хорошего эффекта. Но мой внутренний перфекционист пошел на компромисс со временем и сдался. Код же прост для обзора: ...that.plant_D(800, 10111), ...that.plant_A(800, 10200), ...that.plant_R(800, 10300), ...that.plant_N(800, 10400), ...that.plant_E(800, 10500), ...that.plant_O(800, 10600), ...that.plant_S(800, 12500), ...that.plant_T(800, 13100), ...that.plant_U(800, 13400), ...that.plant_D2(800, 13700), ...that.plant_I(800, 14000), ...that.plant_O2(800, 14300) Вторым с чем я столкнулся — это с собственной невежественностью в чтении мануалов, мы все в той или иной степени этим страдаем. Банально упустил пункт о том, что собственные заготовки svg должны быть в разрешении 100x100 и первая буква «D» пошла наперекосяк. Ну и конечно, в таких случаях мы всегда делаем костыль, что и случилось. Просто поменял ей размер, после инициализации (а раз я использовал объект Burst, то и менять мне надо сразу всех потомков). for (let el of equal.el.children) { el.style['height'] = '180px'; } В дальнейшем я задал вопрос на GitHub, на что меня ткнули в пунктик мануала) Добавление собственной svg происход след. образом: class elipceR2 extends mojs.CustomShape { getShape () { return '<path d=\"M0,120.5h13.9c39.6,0,59.6-30.6,59.1-61C73,29.8,52.8-0.2,13.3,0L0.1,0\"/>'; } getLength () { return 200; } // optional } добавить и использовать (простота во всем): mojs.addShape( 'elipceR2', elipceR2'); new mojs.Shape({ shape: 'elipceR2'}); Либо использовать дефолтные svg-фигуры. MoonRise Описывает горизонт, горы, выход луны. С луной возился долго, надо было соблюдать стилистику и луна с детальным рельефом тут не подходила: Скрытый текст Но, я удачно нагуглил более лучший вариант: Скрытый текст немного поправить напильником и получаем замечательную луну, хорошо подходящую к нашей стилистике. Для приличия код: that.moon(5000, 20000), that.mountains(1500, 18500), that.horizonLine(1600, 18500), Stars Второй по сложности объект. Стал основными источником синхронизации эффектов с музыкальной нарезкой «Семейства булочек». Я сделал несколько массивов звезд и вывел их в заданный момент, попутно закинув их в контекст объекта для дальнейших манипуляций c цветом, размером и их координатами. Где, параметры это время анимации, начало, и кол-во звезд. that.curentStars = [ ...that.star(200, 17050, 5), ...that.star(200, 17300, 15), ...that.star(200, 17600, 25), ...that.star(200, 17900, 30), ...that.star(200, 18200, 35), ]; Как-то размыть svg средствами библиотеки мне не удалось, пришлось их просто увеличивать. Как всегда, длительность анимации, ее начало, массив и кол-во звезд, которые мы берем рандомно, that.shineStars(10, 21200, that.curentStars, 3); that.shineStars(10, 21600, that.curentStars, 3); that.shineStars(10, 21900, that.curentStars, 3); that.shineStars(10, 22100, that.curentStars, 4); that.shineStars(10, 22400, that.curentStars, 4); that.shineStars(10, 22700, that.curentStars, 4); that.shineStars(10, 23300, that.curentStars, 10); that.shineStars(10, 23900, that.curentStars, 15); that.shineStars(10, 24500, that.curentStars, 12); that.shineStars(10, 25150, that.curentStars, 9); that.shineStars(10, 25700, that.curentStars, 6); that.shineStars(10, 26300, that.curentStars, 7); that.shineStars(10, 26600, that.curentStars, 4); that.shineStars(10, 27200, that.curentStars, 8); that.shineStars(10, 28170, that.curentStars, 3); попутно закидывая в свойство для падения this.curRimShineStar. Оказалось, что если выбирать звезды рандомно, то бывают моменты, когда звезда имеет очень маленькое расстояние к координате к которой она должна была начать движение. И как следствие я просто видел падающую точку. Отфильтровав массив по координатам на правую зону, я добился более продолжительной анимации и показу хвоста за звездой. that.shootingStar(500, 29600, {y: 52, x: PARAMS.COORDINATES_X.str1.D}); that.shootingStar(500, 29800, {y: 52, x: PARAMS.COORDINATES_X.str1.A}); that.shootingStar(500, 30100, {y: 52, x: PARAMS.COORDINATES_X.str1.R[0]}); that.shootingStar(500, 30400, {y: 52, x: PARAMS.COORDINATES_X.str1.N[0]}); that.shootingStar(500, 30700, {y: 52, x: PARAMS.COORDINATES_X.str1.N[1]}); that.shootingStar(500, 31070, {y: 52, x: PARAMS.COORDINATES_X.str1.E}); that.shootingStar(500, 31370, {y: 52, x: PARAMS.COORDINATES_X.str1.O}); Очень огорчило отсутствие некоторых подробностей по местоположению параметров в объектах, но все решилось банальным выводом его в консоль и чтением исходников (долго, но всегда гарантированный результат). Мне потребовалось узнать, когда объект завершит свою анимацию после всех манипуляций с ним и я не сразу понял что свойство Object.timeline._props.time его показывает, изначально я искал его в Object._o Где то я конечно схалтурил в коде, за что трудно себя простить. Но в целом анимация удалась замечательной. PointsTimer Чтобы гость мог покрутить ползунок и самому посмотреть, как это происходило, пришлось добавить и таймер, который завершал анимацию через пару секунд простоя. Тут моя фантазия уже закончилась и я просто добавил 3 уменьшающихся круга. По истечению которых, при помощи той же JQuery.animate() мы растворяем буквы в логотипе и удаляем все теги, что добавила библиотека. К сожалению библиотека не может делать очень долгих анимаций, это можно заметить, есть дернуть ползунок очень быстро в сторону, вы увидите как все пойдет кусками, где то будут элементы, где то нет. Решить это можно думаю лишь принудительным манипулированием скорости ползунка."], "hab": ["Программирование", "JavaScript"]}{"url": "https://habrahabr.ru/post/321632/", "title": ["OpenPapyrus: [yet another] ERP-система с открытым исходным кодом"], "text": ["Добрый день. Мы опубликовали систему управления предприятием Papyrus в исходных кодах под именем OpenPapyrus. Проект так же опубликован на сервере sourceforge. Технические аспекты я распишу в отдельной статье, а здесь кратко расскажу о том, что такое OpenPapyrus и почему эта система стоит того, чтобы с ней ознакомиться и начать ее использовать. OpenPapyrus — инфраструктурная система для управления бизнесом, над которой мы работаем более 20 лет (с 1996 года), развив в ней обширный функционал для управления бизнесом в широком наборе сегментов. До текущего момента мы продавали систему Papyrus только как проприетарный продукт. В дальнейшем мы будем развивать одновременно оба продукта — проприетарный и открытый (тем более, что это — почти одно и то же). Мы считаем (Open)Papyrus одной из лучших систем такого класса на российском рынке. Утверждение, конечно, сильное и субъективное, но теперь, когда все карты на столе все исходные коды в github'е, оно может быть проверено кем угодно. Вот, возможно не полный, список сегментов бизнеса, в которых OpenPapyrus превосходно работает: Розничная торговля Аптеки Оптовая торговля Кафе и рестораны Салоны красоты Фитнес клубы и спортивные центры Небольшое производство OpenPapyrus умеет очень-очень-очень много всего и почти все, из этого много, делает хорошо. Попытка перечислить функциональные возможности системы лежит там же на github. Мы постарались классифицировать весь функционал и составить перечень, по которому можно было бы сравнивать систему с конкурирующими решениями. Но на мой взгляд, попытка оказалась не очень успешной (хотя этот документ мы, по-возможности, поддерживаем в актуальном состоянии). Очень тезисная колонка функциональных блоков такова: Бухгалтерский учет Управление закупками Управление продажами Управление расчетами с контрагентами Управление розничными продажами Point-of-sale Управление производством Управление персональными событиями Управление проектами и задачами Инфраструктурный функционал Будет, вероятно, уместным обратить внимание, что у нас есть решение для мобильных торговых агентов (StyloAgent), мобильный официант для ресторанов (StyloWaiter) и модуль для терминалов сбора данных (BHT). Эти продукты мы пока в открытый доступ не выкладываем, но не потому, что жалко, а из-за технических препятствий. Система разрабатывается и развивается в соответствии с несколькими не сложными принципами, которые, в основном, и определяют ее облик: Концептуальная целостность и непротиворечивость. Если по-человечески, то это значит, например, что мы не делаем заплаток для того, чтобы удовлетворить какой-либо запрос клиента, но применяем и(или) расширяем существующие понятия для этого. Если же разработка новой концепции все же необходима, то планируем и прорабатываем ее с расчетом на будущее использование. Максимальная унификация. Жаль, что термин «принцип Оккама» измотали применением к месту и не к месту, а то бы он здесь подошел. Но в общем, идея та же: если некая сущность может быть отражена в системе одни раз и затем повторно использоваться, то незачем ее и множить. Простой пример: персоналии, как субъекты гражданского права, представлены одноименным объектом данных, а не набором «покупатели», «поставщики», «физики», «юрики» и т.д. Вопрос классификации при использовании — техническая проблема. Аналогично, понятия «склад», «адрес», «складская ячейка» и «подразделение организации» все представляются через единый функциональный объект «локация». Похожие подходы используются и в разработке кода системы — большинство блоков строятся по шаблонным методам с предельно унифицированными интерфейсами. Результат легко заметен по размеру дистрибутива — он очень скромный. Примат снижения себестоимости поддержки. Выражаясь русским языком: если два и более клиента обратились по одному поводу, то дешевле что-то поменять в системе, чем отвечать на такие же обращения в будущем. Устранение дефектов имеет максимальный приоритет перед всеми остальными проблемами и выполняется без каких либо условий. С этим все более или менее понятно. Очевидно, этот принцип туго связан с предыдущим. Учитывая возраст системы и то, что некоторые компании используют ее по 10-20 лет, можно предположить, что эти принципы работают. У них есть и побочный эффект: система в конце концов оказалась весьма не очевидной в настройке, что, однако, перекрывается простотой использования для конечных пользователей. Функциональных отличий между Papyrus и OpenPapyrus нет. Мы, правда, пока не до конца понимаем как будут уживаться оба этих варианта с маркетинговой точки зрения. Поддержка остается платной, но мы очень рассчитываем на то, что будет достаточно людей и компаний, которые сами смогут разобраться что и как, и, того паче, смогут продавать свои консультационные услуги другим компаниям. Инструкции по установке есть и на github'е и на sourceforge. Инсталляция для ознакомления очень простая. Инструкций по сборке из исходных кодов пока нет, но клон исходных кодов точно собирается — мы это тщательно проверили. Документация (большая, но все равно, не полная) есть там же. Перед тем, как закруглиться, приведу пару любопытных фактов: Papyrus умеет управлять web-контентом. Мы эту feature не позиционируем как тиражируемую, но содержанием сайта компании Петроглиф и сайтом Universe-HTT (там, например, есть один из лучших справочников штрихкодов в сети) управляет сервер Papyrus, накрытый обвязками из Java из т.д. Протокол версий Papyrus ведется с самого его рождения. В удобоваримом виде он доступен на сайте компании Петроглиф. Конвертация в html осуществляется автоматически тем же самым Papyrus'ом. В релизах на github мы даем соответствующую ссылку. Ну и напоследок. Ради чего? Основная причина — экзистенциальная: стоит поделиться тем, что делали 20 с лишним лет, с остальным миром, тем более, что мы сами пользуемся результатами работы open source-сообщества. Остальные причины мелкие и скучные — вы и сами о них знаете."], "hab": ["ERP-системы", "CRM-системы"]}{"url": "https://habrahabr.ru/post/322170/", "title": ["«Hello, (real) world!» на php в 2017 году"], "text": ["Вы наверняка думаете, что писать на php — это просто. И «hello, world» выглядит примерно так так: <?php echo 'Hello, world!'; Конечно, чего еще ожидать от языка с низким порогом входа. Ну да, именно так и было раньше. Много лет назад. Но теперь, в 2017 году никто так уже не делает. Давайте рассмотрим, почему, и попробуем построить наше более реалистичное hello-world приложение по шагам, а их, скажу сразу, получилось не мало. → Полный исходный код «hello,world» можно посмотреть здесь. Для начала надо осознать тот факт, что без фреймворка сейчас приложения никто не делает. Если вы пишете вручную \"echo 'hello, world'\", то обрекаете проект на говнокод на веки вечные (кто потом этот велосипед за вас переписывать будет?). Поэтому возьмем какой-нибудь современный, распространенный в мире фреймворк, например Symfony. Но прежде, чем его устанавливать, надо бы создать базу данных. Зачем базу данных? Ну не хардкодить же строку «hello, world» прямо в тексте программы! База данных В 2017 году принято использовать postgresql. Если вы вдруг еще не умеете его устанавливать, я помогу: sudo apt-get install postgresql Убунта при установке создаст юзера postgres, из под которого можно запустить команду psql с полными правами на базу. sudo -u postgres psql Теперь создадим юзера базы с паролем (придумайте какой-нибудь посложнее). CREATE ROLE helloworlduser WITH PASSWORD '12345' LOGIN; И саму базу: CREATE DATABASE helloworld OWNER helloworlduser; Также надо убедиться, что в pg_hba.conf у вас разрешены коннекты к базе с localhost (127.0.0.1). Там должно быть что-то вроде этого: host all all 127.0.0.1/32 md5 Проверим соединение: psql -h localhost -U helloworlduser helloworld после ввода пароля должно пустить в базу. Сразу создадим таблицу: CREATE TABLE greetings ( id int, greeting text, primary key(id) ); INSERT INTO greetings (id, greeting) VALUES (1, 'Hello, world!'); Ну, супер, с базой всё. Теперь перейдем к фреймворку php-фреймворк Надеюсь, что в 2017 году у всех стоит composer на компьютере. Поэтому сразу перейдем к установке фреймворка composer create-project symfony/framework-standard-edition helloworldphp При установке он сразу спросит параметры соединения с базой: host: 127.0.0.1 database_name: helloworld database_user: helloworlduser database_password: 12345 остальное по умолчанию/по усмотрению. Надо только в конфиге config.yml поменть драйвер на driver: pdo_pgsql. (У вас ведь установлено php-расширение pdo_pgsql ?) Проверим, что всё более менее работает, запустив cd helloworldphp bin/console server:start Симфони запустит свой собственный сервер, который слушает порт 8000 и на нем можно дебажить код. Таким образом в браузере по адресу http://localhost:8000/ должно быть что-то вроде «Это симфони, блаблабла». Уфф! Казалось бы всё, контроллер уже есть, подправить вьюху, создать модель и понеслась, хелло ворлд уже близко! Но… нет. Извините, но не в 2017-ом. В этом году все делают SPA (single page application). Php-программист в 2017 году не может обойтись без js и верстки, теперь мы все full stack, а значит и helloworld должен быть соответствующий. Ну ладно, ладно, еще бывают чистые php-бекенд-разработчики, но давайте возьмем более общий случай JavaScript и его многочисленные друзья Поэтому находим в симфони вьюху (а дефолтная вьюха лежит в app/Resources/view/default/index.html.twig) и стираем там всё, заменяя на: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> </head> <body> <div id=\"root\"></div> <script src=\"js/bundle.js\"></script> </body> </html> Т.е. всё будет лежат в bundle.js: сжатые javascript файлы прямо вместе со стилями и всем, чем нужно. Как нам создать этот бандл? Нужно написать приложение и настроить webpack для сборки. Webpack (или его аналоги) нам все равно бы понадобились, мы же не будем писать код на чистом javascript в 2017-году, когда typescript явно в тренде. А typescript надо как-то преобразовать в обычную js-ку. Это удобно делать, используя webpack. Разумеется, на чистом typescript тоже никто не пишет. Нужен какой-то фреймворк. Одна из самых модных связок сейчас — это react + redux. А для верстки, так и быть, будем использовать старый добрый олдскульный bootstrap (через sass, конечно же). Нам понадобится куча js-библиотек. У вас ведь стоит nodejs и npm? Убедитесь, что у вас свежий npm и установите пакеты: npm init в зависимостях (в файле package.json) пропишем примерно такое: \"dependencies\": { \"@types/react\": \"^15.0.11\", \"@types/react-dom\": \"^0.14.23\", \"babel-core\": \"^6.23.1\", \"babel-loader\": \"^6.3.2\", \"babel-preset-es2015\": \"^6.22.0\", \"babel-preset-react\": \"^6.23.0\", \"bootstrap-sass\": \"^3.3.7\", \"css-loader\": \"^0.26.1\", \"node-sass\": \"^4.5.0\", \"react\": \"^15.4.2\", \"react-dom\": \"^15.4.2\", \"react-redux\": \"^5.0.2\", \"redux\": \"^3.6.0\", \"resolve-url-loader\": \"^2.0.0\", \"sass-loader\": \"^6.0.1\", \"style-loader\": \"^0.13.1\", \"ts-loader\": \"^2.0.0\", \"typescript\": \"^2.1.6\", \"url-loader\": \"^0.5.7\", \"webpack\": \"^2.2.1\", \"@types/node\": \"^7.0.5\" } И выполним npm install и еще нужно установить: npm install webpack -g чтобы была доступна команда webpack. Увы, это еще далеко не всё. Так как у нас typescript, еще надо создать файл tsconfig.json, примерно такой: tsconfig.json{ \"compilerOptions\": { \"module\": \"es6\", \"moduleResolution\": \"node\", \"sourceMap\": false, \"target\": \"esnext\", \"outDir\": \"web/ts\", \"lib\": [ \"dom\", \"scripthost\", \"es5\", \"es6\", \"es7\" ], \"jsx\": \"react\" }, \"include\": [ \"frontend/**/*.ts\", \"frontend/**/*.tsx\" ] } С конфигами пока что ок, теперь займемся нашим приложением на typescript. Сначала создадим компонент для отображения нашего текста: // файл frontend/components/Greetings.tsx import * as React from 'react'; export interface GreetingsProps { text: string; isReady: boolean; onMount(); } class Greetings extends React.Component<GreetingsProps, undefined> { componentDidMount() { this.props.onMount(); } render() { return ( <h1>{this.props.text}</h1> ); } } export default Greetings; Наше SPA будет подгружать текст надписи через Rest API. React — это просто view-компоненты, а нам еще нужна логика приложения и управление состоянием. Так что будем использовать redux, а также пакет для связи redux и react (react-redux). Поэтому надо будет еще создать компонент, который будет создавать наш компонент Greetings с нужными properties, и сможет сообщить хранилищу (store) состояния, что появилось новое действие (получены данные для отображения). Disclaimer: я только начал изучать redux, поэтому наверняка тут есть за что «бить по рукам». Выглядит этот компонент, допустим, примерно так: // файл frontend/components/App.tsx import * as React from 'react'; import {connect} from 'react-redux' import Greetings from './Greetings'; const mapStateToProps = (state) => { return state; } const mapDispatchToProps = (dispatch) => { return { onMount: () => { fetch(\"/greetings/1\").then((response) => { return response.json(); }).then((json) => { dispatch({type: 'FETCH_GREETING', text: json.greeting}) }); } } } export default connect(mapStateToProps, mapDispatchToProps)(Greetings); Ну и точка входа приложения, создание redux-стора, диспатчера и т.д. Тут всё сделано немного по рабоче-крестьянски, но для хелловорлда сойдет, пожалуй: // подгружает стили bootstrap import 'bootstrap-sass/assets/stylesheets/_bootstrap.scss'; import * as React from 'react'; import * as ReactDOM from \"react-dom\"; import {Provider} from 'react-redux'; import App from './components/App'; import {createStore} from 'redux'; const app = (state = {isReady: false, text: ''}, action) => { switch (action.type) { case 'FETCH_GREETING': return Object.assign({}, state, {isReady: true, text: action.text}); } return state; } const store = createStore(app); ReactDOM.render( <Provider store={store}> <App/> </Provider>, document.getElementById(\"root\") ); Примерно здесь происходит следующее: Первоначальное состояние системы — {isReady: false, text: ''}. Создан reducer под названием app, который умеет обрабатывать действие FETCH_GREETING и возвращать новое состояние системы. Создан store для обработки состояний. Всё отрендеривается в элемент, который мы прописали во вьюхе <div id=\"root\"></div> Ах да, совсем забыл. Конфиг вебпака: webpack.config.jsconst webpack = require('webpack'); const path = require('path'); const ENVIRONMENT = process.env.NODE_ENV || 'development'; let config = { context: path.resolve(__dirname, \"frontend\"), entry: './index.tsx', output: { filename: 'bundle.js', path: path.resolve(__dirname, \"web/js\") }, resolve: { extensions: [ \".js\", \".jsx\", '.ts', '.tsx'] }, module: { rules: [ { test: /\\.tsx?$/, use: [{ loader: 'babel-loader', query: { presets: ['es2015', 'react'] } }, { loader: 'ts-loader' }] }, { test: /\\.woff($|\\?)|\\.woff2($|\\?)|\\.ttf($|\\?)|\\.eot($|\\?)|\\.svg($|\\?)/, loader: 'url-loader' }, { test: /\\.scss$/, use: [ { loader: \"style-loader\" }, { loader: \"css-loader\" }, { loader: \"resolve-url-loader\" }, { loader: \"sass-loader\" } ] } ] }, plugins: [ new webpack.DefinePlugin({ 'process.env.NODE_ENV': JSON.stringify(ENVIRONMENT) }) ], node: { process: false } }; if (ENVIRONMENT == 'production') { config.plugins.push( new webpack.optimize.UglifyJsPlugin({ compress: { drop_console: false, warnings: false } }) ); } module.exports = config; Теперь мы можем запустить webpack или NODE_ENV=production webpack (чтобы получить минифицированную версию bundle.js) Pomodoro Не знаю как вы, а я уже задолбался писать этот hello, world. В 2017 году надо работать эффективно, а это подразумевает, что надо делать перерывы в работе (метод Pomodoro и т.д.). Так что, пожалуй, прервусь не надолго. [прошло какое-то время] Давайте продолжим. Мы уже умеем подгружать код с /greetings/1 на стороне javascript, но php-часть еще совершенно не готова. Doctrine Уже потрачено много времени, а в php-коде не создано ни одной сущности. Давайте исправим положение: <?php // src/AppBundle/Entity/Greeting.php namespace AppBundle\\Entity; use Doctrine\\ORM\\Mapping as ORM; /** * @ORM\\Entity * @ORM\\Table(name=\"greetings\") */ class Greeting { /** * @ORM\\Column(type=\"integer\") * @ORM\\Id */ private $id; /** * @ORM\\Column(type=\"string\", length=100) */ private $greeting; public function getId() { return $this->id; } public function getGreeting() { return $this->greeting; } } Супер. Осталось совсем чуть-чуть. REST Надо сделать-таки простенький REST API, который может хотя бы отдать json по запросу GET /greetings/1 Для этого в контроллере (файл src/AppBundle/Controller/DefaultController.php) добавим метод с роутом: /** * @Route(\"/greetings/{id}\") */ public function greetings($id) { $greeting = $this->getDoctrine()->getRepository(\"AppBundle:Greeting\")->find($id); return new JsonResponse(['greeting' => $greeting->getGreeting()]); } Всё, можно запускать. На экране отображается «Hello, world!». Внешне он, конечно, выглядит почти также как результат <?php echo «hello, world» ?> (если не считать бутстраповского шрифта), но теперь это современное приложение по всем канонам. Ну, скажем так, почти по всем канонам (не хватает тестов, проверок ошибок и много чего еще), но я уже задолбался это делать :) Выводы В последнее время сильно участились споры «зачем нужен php, если есть java». Уж не знаю, кто прав, а кто нет, холивары — дело такое. Но в каждом споре один из аргументов в пользу php — это простота для новичков. Как мне кажется, этот аргумент уже давно не валиден, что я и хотел показать этой статьёй. Новичку все равно придется кучу всего узнать и 100500 конфигов настроить: фреймворки (очень похожие на фреймворки java), базы данных, linux, javascript со всем своим зоопарком, верстка, http-протокол, различный тулинг и многое-многое другое. Даже если это не SPA. Upd. Статья уходит в глубокий минус, но я не собираюсь менять мнение. Оно примерно такое: 1) SPA всё больше проникает в наш мир, и надо это уметь, хотя бы в общих чертах. 2) Без фреймворков не построишь хорошее современное приложение."], "hab": ["Разработка веб-сайтов", "Symfony", "ReactJS", "PHP", "JavaScript"]}{"url": "https://habrahabr.ru/company/mailru/blog/322416/", "title": ["Выбор правильной стратегии обработки ошибок (части 1 и 2)", "перевод"], "text": ["Существует две фундаментальные стратегии: обработка исправимых ошибок (исключения, коды возврата по ошибке, функции-обработчики) и неисправимых (assert(), abort()). В каких случаях какую стратегию лучше использовать? Виды ошибок Ошибки возникают по разным причинам: пользователь ввёл странные данные, ОС не может дать вам обработчика файла или код разыменовывает (dereferences) nullptr. Каждая из описанных ошибок требует к себе отдельного подхода. По причинам ошибки делятся на три основные категории: Пользовательские ошибки: здесь под пользователем подразумевается человек, сидящий перед компьютером и действительно «использующий» программу, а не какой-то программист, дёргающий ваш API. Такие ошибки возникают тогда, когда пользователь делает что-то неправильно. Системные ошибки появляются, когда ОС не может выполнить ваш запрос. Иными словами, причина системных ошибок — сбой вызова системного API. Некоторые возникают потому, что программист передал системному вызову плохие параметры, так что это скорее программистская ошибка, а не системная. Программистские ошибки случаются, когда программист не учитывает предварительные условия API или языка программирования. Если API требует, чтобы вы не вызывали foo() с 0 в качестве первого параметра, а вы это сделали, — виноват программист. Если пользователь ввёл 0, который был передан foo(), а программист не написал проверку вводимых данных, то это опять же его вина. Каждая из описанных категорий ошибок требует особого подхода к их обработке. Пользовательские ошибки Сделаю очень громкое заявление: такие ошибки — на самом деле не ошибки. Все пользователи не соблюдают инструкции. Программист, имеющий дело с данными, которые вводят люди, должен ожидать, что вводить будут именно плохие данные. Поэтому первым делом нужно проверять их на валидность, сообщать пользователю об обнаруженных ошибках и просить ввести заново. Поэтому не имеет смысла применять к пользовательским ошибкам какие-либо стратегии обработки. Вводимые данные нужно как можно скорее проверять, чтобы ошибок не возникало. Конечно, такое не всегда возможно. Иногда проверять вводимые данные слишком дорого, иногда это не позволяет сделать архитектура кода или разделение ответственности. Но в таких случаях ошибки должны обрабатываться однозначно как исправимые. Иначе, допустим, ваша офисная программа будет падать из-за того, что вы нажали backspace в пустом документе, или ваша игра станет вылетать при попытке выстрелить из разряженного оружия. Если в качестве стратегии обработки исправимых ошибок вы предпочитаете исключения, то будьте осторожны: исключения предназначены только для исключительных ситуаций, к которым не относится большинство случаев ввода пользователями неверных данных. По сути, это даже норма, по мнению многих приложений. Используйте исключения только тогда, когда пользовательские ошибки обнаруживаются в глубине стека вызовов, вероятно, внешнего кода, когда они возникают редко или проявляются очень жёстко. В противном случае лучше сообщать об ошибках с помощью кодов возврата. Системные ошибки Обычно системные ошибки нельзя предсказать. Более того, они недетерминистские и могут возникать в программах, которые до этого работали без нареканий. В отличие от пользовательских ошибок, зависящих исключительно от вводимых данных, системные ошибки — настоящие ошибки. Но как их обрабатывать, как исправимые или неисправимые? Это зависит от обстоятельств. Многие считают, что ошибка нехватки памяти — неисправимая. Зачастую не хватает памяти даже для обработки этой ошибки! И тогда приходится просто сразу же прерывать выполнение. Но падение программы из-за того, что ОС не может выделить сокет, — это не слишком дружелюбное поведение. Так что лучше бросить исключение и позволить catch аккуратно закрыть программу. Но бросание исключения — не всегда правильный выбор. Кто-то даже скажет, что он всегда неправильный. Если вы хотите повторить операцию после её сбоя, то обёртывание функции в try-catch в цикле — медленное решение. Правильный выбор — возврат кода ошибки и цикличное исполнение, пока не будет возвращено правильное значение. Если вы создаёте вызов API только для себя, то просто выберите подходящий для своей ситуации путь и следуйте ему. Но если вы пишете библиотеку, то не знаете, чего хотят пользователи. Дальше мы разберём подходящую стратегию для этого случая. Для потенциально неисправимых ошибок подойдёт «обработчик ошибок», а при других ошибках необходимо предоставить два варианта развития событий. Обратите внимание, что не следует использовать подтверждения (assertions), включающиеся только в режиме отладки. Ведь системные ошибки могут возникать и в релизной сборке! Программистские ошибки Это худший вид ошибок. Для их обработки я стараюсь сделать так, чтобы мои ошибки были связаны только с вызовами функций, то есть с плохими параметрами. Прочие типы программистских ошибок могут быть пойманы только в runtime, с помощью отладочных макросов (assertion macros), раскиданных по коду. При работе с плохими параметрами есть две стратегии: дать им определённое или неопределённое поведение. Если исходное требование для функции — запрет на передачу ей плохих параметров, то, если их передать, это считается неопределённым поведением и должно проверяться не самой функцией, а оператором вызова (caller). Функция должна делать только отладочное подтверждение (debug assertion). С другой стороны, если отсутствие плохих параметров не является частью исходных требований, а документация определяет, что функция будет бросать bad_parameter_exception при передаче ей плохого параметра, то передача — это хорошо определённое поведение (бросание исключения или любая другая стратегия обработки исправимых ошибок), и функция всегда должна это проверять. В качестве примера рассмотрим получающие функции (accessor functions) std::vector<T>: в спецификации на operator[] говорится, что индекс должен быть в пределах валидного диапазона, при этом at() сообщает нам, что функция кинет исключение, если индекс не попадает в диапазон. Более того, большинство реализаций стандартных библиотек обеспечивают режим отладки, в котором проверяется индекс operator[], но технически это неопределённое поведение, оно не обязано проверяться. Примечание: необязательно бросать исключение, чтобы получилось определённое поведение. Пока это не упомянуто в исходных условиях для функции, это считается определённым. Всё, что прописано в исходных условиях, не должно проверяться функцией, это неопределённое поведение. Когда нужно проверять только с помощью отладочных подтверждений, а когда — постоянно? К сожалению, однозначного рецепта нет, решение зависит от конкретной ситуации. У меня есть лишь одно проверенное правило, которому я следую при разработке API. Оно основано на наблюдении, что проверять исходные условия должен вызывающий, а не вызываемый. А значит, условие должно быть «проверяемым» для вызывающего. Также условие «проверяемое», если можно легко выполнить операцию, при которой значение параметра всегда будет правильным. Если для параметра это возможно, то это получается исходное условие, а значит, проверяется только посредством отладочного подтверждения (а если слишком дорого, то вообще не проверяется). Но конечное решение зависит от многих других факторов, так что очень трудно дать какой-то общий совет. По умолчанию я стараюсь свести к неопределённому поведению и использованию только подтверждений. Иногда бывает целесообразно обеспечить оба варианта, как это делает стандартная библиотека с operator[] и at(). Хотя в ряде случаев это может быть ошибкой. Об иерархии std::exception Если в качестве стратегии обработки исправимых ошибок вы выбрали исключения, то рекомендуется создать новый класс и наследовать его от одного из классов исключений стандартной библиотеки. Я предлагаю наследовать только от одного из этих четырёх классов: std::bad_alloc: для сбоев выделения памяти. std::runtime_error: для общих runtime-ошибок. std::system_error (производное от std::runtime_error): для системных ошибок с кодами ошибок. std::logic_error: для программистских ошибок с определённым поведением. Обратите внимание, что в стандартной библиотеке разделяются логические (то есть программистские) и runtime-ошибки. Runtime-ошибки — более широкое определение, чем «системные». Оно описывает «ошибки, обнаруживаемые только при выполнении программы». Такая формулировка не слишком информативна. Лично я использую её для плохих параметров, которые не являются исключительно программистскими ошибками, а могут возникнуть и по вине пользователей. Но это можно определить лишь глубоко в стеке вызовов. Например, плохое форматирование комментариев в standardese приводит к исключению при парсинге, проистекающему из std::runtime_error. Позднее оно ловится на соответствующем уровне и фиксируется в логе. Но я не стал бы использовать этот класс иначе, как и std::logic_error. Подведём итоги Есть два пути обработки ошибок: как исправимые: используются исключения или возвращаемые значения (в зависимости от ситуации/религии); как неисправимые: ошибки журналируются, а программа прерывается. Подтверждения — это особый вид стратегии обработки неисправимых ошибок, только в режиме отладки. Есть три основных источника ошибок, каждый требует особого подхода: Пользовательские ошибки не должны обрабатываться как ошибки на верхних уровнях программы. Всё, что вводит пользователь, должно проверяться соответствующим образом. Это может обрабатываться как ошибки только на нижних уровнях, которые не взаимодействуют с пользователями напрямую. Применяется стратегия обработки исправимых ошибок. Системные ошибки могут обрабатываться в рамках любой из двух стратегий, в зависимости от типа и тяжести. Библиотеки должны работать как можно гибче. Программистские ошибки, то есть плохие параметры, могут быть запрещены исходными условиями. В этом случае функция должна использовать только проверку с помощью отладочных подтверждений. Если же речь идёт о полностью определённом поведении, то функции следует предписанным образом сообщать об ошибке. Я стараюсь по умолчанию следовать сценарию с неопределённым поведением и определяю для функции проверку параметров лишь тогда, когда это слишком трудно сделать на стороне вызывающего. Гибкие методики обработки ошибок в C++ Иногда что-то не работает. Пользователи вводят данные в недопустимом формате, файл не обнаруживается, сетевое соединение сбоит, в системе кончается память. Всё это ошибки, и их надо обрабатывать. Это относительно легко сделать в высокоуровневых функциях. Вы точно знаете, почему что-то пошло не так, и можете обработать это соответствующим образом. Но в случае с низкоуровневыми функциями всё не так просто. Они не знают, что пошло не так, они знают лишь о самом факте сбоя и должны сообщить об этом тому, кто их вызвал. В C++ есть два основных подхода: коды возврата ошибок и исключения. Сегодня широко распространено использование исключений. Но некоторые не могут / думают, что не могут / не хотят их использовать — по разным причинам. Я не буду принимать чью-либо сторону. Вместо этого я опишу методики, которые удовлетворят сторонников обоих подходов. Особенно методики пригодятся разработчикам библиотек. Проблема Я работаю над проектом foonathan/memory. Это решение предоставляет различные классы выделения памяти (allocator classes), так что в качестве примера рассмотрим структуру функции выделения. Для простоты возьмём malloc(). Она возвращает указатель на выделяемую память. Если выделить память не получается, то возвращается nullptr, то есть NULL, то есть ошибочное значение. У этого решения есть недостатки: вам нужно проверять каждый вызов malloc(). Если вы забудете это сделать, то выделите несуществующую память. Кроме того, по своей натуре коды ошибок транзитивны: если вызвать функцию, которая может вернуть код ошибки, и вы не можете его проигнорировать или обработать, то вы тоже должны вернуть код ошибки. Это приводит нас к ситуации, когда чередуются нормальные и ошибочные ветви кода. Исключения в таком случае выглядят более подходящим решением. Благодаря им вы сможете обрабатывать ошибки только тогда, когда вам это нужно, а в противном случае — достаточно тихо передать их обратно вызывающему. Это можно расценить как недостаток. Но в подобных ситуациях исключения имеют также очень большое преимущество: функция выделения памяти либо возвращает валидную память, либо вообще ничего не возвращает. Это функция «всё или ничего», возвращаемое значение всегда будет валидным. Это полезное следствие согласно принципу Скотта Майера «Make interfaces hard to use incorrectly and easy to use correctly». Учитывая вышесказанное, можно утверждать, что вам следует использовать исключения в качестве механизма обработки ошибок. Этого мнения придерживается большинство разработчиков на С++, включая и меня. Но проект, которым я занимаюсь, — это библиотека, предоставляющая средства выделения памяти, и предназначена она для приложений, работающих в реальном времени. Для большинства разработчиков подобных приложений (особенно для игроделов) само использование исключений — исключение. Каламбур детектед. Чтобы уважить эту группу разработчиков, моей библиотеке лучше обойтись без исключений. Но мне и многим другим они нравятся за элегантность и простоту обработки ошибок, так что ради других разработчиков моей библиотеке лучше использовать исключения. Так что же делать? Идеальное решение: возможность включать и отключать исключения по желанию. Но, учитывая природу исключений, нельзя просто менять их местами с кодами ошибок, поскольку у нас не будет внутреннего кода проверки на ошибки — весь внутренний код опирается на предположение о прозрачности исключений. И даже если бы внутри можно было использовать коды ошибок и преобразовывать их в исключения, это лишило бы нас большинства преимуществ последних. К счастью, я могу определить, что вы делаете, когда обнаруживаете ошибку нехватки памяти: чаще всего вы журналируете это событие и прерываете программу, поскольку она не может корректно работать без памяти. В таких ситуациях исключения — просто способ передачи контроля другой части кода, которая журналирует и прерывает программу. Но есть старый и эффективный способ передачи контроля: указатель функции (function pointer), то есть функция-обработчик (handler function). Если у вас включены исключения, то вы просто их бросаете. В противном случае вызываете функцию-обработчика и затем прерываете программу. Это предотвратит бесполезную работу функции-обработчика, та позволит программе продолжить выполняться в обычном режиме. Если не прервать, то произойдёт нарушение обязательного постусловия функции: всегда возвращать валидный указатель. Ведь на выполнении этого условия может быть построена работа другого кода, да и вообще это нормальное поведение. Я называю такой подход обработкой исключений и придерживаюсь его при работе с памятью. Решение 1: обработчик исключений Если вам нужно обработать ошибку в условиях, когда наиболее распространённым поведением будет «журналировать и прервать», то можно использовать обработчика исключений. Это такая функция-обработчик, которая вызывается вместо бросания объекта-исключения. Её довольно легко реализовать даже в уже существующем коде. Для этого нужно поместить управление обработкой в класс исключений и обернуть в макрос выражение throw. Сначала дополним класс и добавим функции для настройки и, возможно, запрашивания функции-обработчика. Я предлагаю делать это так же, как стандартная библиотека обрабатывает std::new_handler: class my_fatal_error { public: // тип обработчика, он должен брать те же параметры, что и конструктор, // чтобы у них была одинаковая информация using handler = void(*)( ... ); // меняет функцию-обработчика handler set_handler(handler h); // возвращает текущего обработчика handler get_handler(); ... // нормальное исключение }; Поскольку это входит в область видимости класса исключений, вам не нужно именовать каким-то особым образом. Отлично, нам же легче. Если исключения включены, то для удаления обработчика можно использовать условное компилирование (conditional compilation). Если хотите, то также напишите обычный подмешанный класс (mixin class), дающий требуемую функциональность. Конструктор исключений элегантен: он вызывает текущую функцию-обработчика, передавая ей требуемые аргументы из своих параметров. А затем комбинирует с последующим макросом throw: If```cpp #if EXCEPTIONS #define THROW(Ex) throw (Ex) #else #define THROW(Ex) (Ex), std::abort() #endif > Такой макрос throw также предоставляется [foonathan/compatiblity](https://github.com/foonathan/compatibility). Можно использовать его и так: ```cpp THROW(my_fatal_error(...)) Если у вас включена поддержка исключений, то будет создан и брошен объект-исключение, всё как обычно. Но если поддержка выключена, то объект-исключение всё равно будет создан, и — это важно — только после этого произойдёт вызов std::abort(). А поскольку конструктор вызывает функцию-обработчика, то он и работает, как требуется: вы получаете точку настройки для журналирования ошибки. Благодаря же вызову std::abort() после конструктора пользователь не может нарушить постусловие. Когда я работаю с памятью, то при включённых исключениях у меня также включён и обработчик, который вызывается при бросании исключения. Так что при этой методике вам ещё будет доступна определённая степень кастомизации, даже если вы отключите исключения. Конечно, замена неполноценная, мы только журналируем и прерываем работу программы, без дальнейшего продолжения. Но в ряде случаев, в том числе при исчерпании памяти, это вполне пригодное решение. А если я хочу продолжить работу после бросания исключения? Методика с обработчиком исключений не позволяет этого сделать в связи с постусловием кода. Как же тогда продолжить работу? Ответ прост — никак. По крайней мере, это нельзя сделать так же просто, как в других случаях. Нельзя просто так вернуть код ошибки вместо исключения, если функция на это не рассчитана. Есть только одно решение: сделать две функции. Одна возвращает код ошибки, а вторая бросает исключения. Клиенты, которым нужны исключения, будут использовать второй вариант, остальные — первый. Извините, что говорю такие очевидные вещи, но ради полноты изложения я должен был об этом сказать. Для примера снова возьмём функцию выделения памяти. В этом случае я использую такие функции: void* try_malloc(..., int &error_code) noexcept; void* malloc(...); При сбое выделения памяти первая версия возвращает nullptr и устанавливает error_code в коде ошибки. Вторая версия не возвращает nullptr, зато бросает исключение. Обратите внимание, что в рамках первой версии очень легко реализовать вторую: void* malloc(...) { auto error_code = 0; auto res = try_malloc(..., error_code); if (!res) throw malloc_error(error_code); return res; } Не делайте этого в обратной последовательности, иначе вам придётся ловить исключение, а это дорого. Также это не даст нам скомпилировать код без включённой поддержки исключений. Если сделаете, как показано, то можете просто стереть другую перегрузку (overload) с помощью условного компилирования. Но даже если у вас включена поддержка исключений, клиенту всё равно может понадобиться вторая версия. Например, когда нужно выделить наибольший возможный объём памяти, как в нашем примере. Будет проще и быстрее вызывать в цикле и проверять по условию, чем ловить исключение. Решение 2: предоставить две перегрузки Если недостаточно обработчика исключений, то нужно предоставить две перегрузки. Одна использует код возврата, а вторая бросает исключение. Если рассматриваемая функция не имеет возвращаемого значения, то можете её использовать для кода ошибки. В противном случае вам придётся возвращать недопустимое значение для сигнализирования об ошибке — как nullptr в вышеприведённом примере, — а также установить выходной параметр для кода ошибки, если хотите предоставить вызывающему дополнительную информацию. Пожалуйста, не используйте глобальную переменную errno или что-то типа GetLastError()! Если возвращаемое значение не содержит недопустимое значение для обозначения сбоя, то по мере возможности используйте std::optional или что-то похожее. Перегрузка исключения (exception overload) может — и должна — быть реализована в рамках версии с кодом ошибки, как это показано выше. Если компилируете без исключений, сотрите перегрузку с помощью условного компилирования. std::system_error Подобная система идеально подходит для работы с кодами ошибок в С++ 11. Она возвращает непортируемый (non-portable) код ошибки std::error_code, то есть возвращаемый функцией операционной системы. С помощью сложной системы библиотечных средств и категорий ошибок вы можете добавить собственные коды ошибок, или портируемые std::error_condition. Для начала почитайте об этом здесь. Если нужно, то можете использовать в функции кода ошибки std::error_code. А для функции исключения есть подходящий класс исключения: std::system_error. Он берёт std::error_code и применяется для передачи этих ошибок в виде исключений. Эту или подобную систему должны использовать все низкоуровневые функции, являющиеся закрытыми обёртками ОС-функций. Это хорошая — хотя и сложная — альтернатива службе кодов ошибок, предоставляемой операционной системой. Да, и мне ещё нужно добавить подобное в функции виртуальной памяти. На сегодняшний день они не предоставляют коды ошибок. std::expected Выше упоминалось о проблеме, когда у вас нет возвращаемого значения, содержащего недопустимое значение, которое можно использовать для сигнализирования об ошибке. Более того, выходной параметр — не лучший способ получения кода ошибки. А глобальные переменные вообще не вариант! В № 4109 предложено решение: std::expected. Это шаблон класса, который также хранит возвращаемое значение или код ошибки. В вышеприведённом примере он мог бы использоваться так: std::expected<void*, std::error_code> try_malloc(...); В случае успеха std::expected будет хранить не-null указатель памяти, а при сбое — std::error_code. Сейчас эта методика работает при любых возвращаемых значениях. Комбинация std::expected и функции исключения определённо допускает любые варианты использования. Заключение Если вы создаёте библиотеки, то иногда приходится обеспечивать максимальную гибкость использования. Под этим подразумевается и разнообразие средств обработки ошибок: иногда требуются коды возврата, иногда — исключения. Одна из возможных стратегий — улаживание этих противоречий с помощью обработчика исключений. Просто удостоверьтесь, что когда нужно, то вызывается callback, а не бросается исключение. Это замена для критических ошибок, которая в любом случае будет журналироваться перед прерыванием работы программы. Как таковой этот способ не универсален, вы не можете переключаться в одной программе между двумя версиями. Это лишь обходное решение при отключённой поддержке исключений. Более гибкий подход — просто предоставить две перегрузки, одну с исключениями, а вторую без. Это даст пользователям максимальную свободу, они смогут выбирать ту версию, что лучше подходит в их ситуации. Недостаток этого подхода: вам придётся больше потрудиться при создании библиотеки."], "hab": ["Совершенный код", "Проектирование и рефакторинг", "Анализ и проектирование систем", "C++", "Блог компании Mail.Ru Group"]}{"url": "https://habrahabr.ru/post/280498/", "title": ["Как накрутить счетчик Google Analytics или Google ненавидит Казахстан"], "text": ["Добрый день. Думаю многие читали историю про Катю, я решил написать в том же стиле, т.е. информация о том почему Google ненавидит Казахстан будет в спойлерах. И так. Предыстория. Однажды изучаю статистику в Google Search Console, я заметил что в разделе «Вид в поиске» появился еще один пункт, этот пункт вел на отчет о проиндексированных AMP страницах. Зайдя на эту страницу я увидел надпись Мне конечно же сразу захотелось узнать что это за ускоренные мобильные страницы. Пройдя по ссылке я увидел гайдлайн по созданию AMP страниц. Почитав гайдлайн я приступил к созданию amp страниц на своем сайте. История Прочитав документацию по созданию AMP, я сразу же приступил к реализации. Google ненавидит КазахстанПосле того как я подготовил все свои страницы, я стал ждать официального запуска AMP, он должен был быть 24 февраля, и он запустился 24 февраля, НО Казахстан не входил в список тех стран у которых будут отображаться AMP версии страниц в поиске. И нигде об этом не было написано. И нигде не написано когда будет доступен AMP в Казахстане. Наверное потому что... Google ненавидит Казахстан Через час у меня уже был готовы AMP версий моих страниц. Но возникла проблема, я не мог вставить Google Analytics, т.к. вставлять JS скрипты в AMP страницы нельзя, а компонента amp-analytics еще не существовало, но был компонент amp-pixel. Погуглив немного я наткнулся на вот этот вопрос на Stackoverflow. Тут предлагали вставлять pixel с следующим url который передавал нужные значение в Google Analytics <amp-pixel src=\"https://ssl.google-analytics.com/collect?v=1&tid=UA-12345678-1&t=pageview&cid=$RANDOM&dt=$TITLE&dl=$CANONICAL_URL&z=$RANDOM\"></amp-pixel> Это картинка передавала уникальный ID пользователя, название статьи и ссылку указанную в О передаваемых параметрах можно почитать тут, честно говоря я еще сам не читал Google ненавидит КазахстанТ.к. я работаю в редакции онлайн СМИ, для нас важны все методы привлечения трафика. Одним из основных каналов для других СМИ, это Google News. Но в Google News нет возможности выбрать новости с Казахстана. Черт, там есть даже новости с Эфиопии и Кении!!! А с Казахстана нету!!! У меня бомбит. Это все потому что... Google ненавидит Казахстан Добавив этот пиксель на свой сайт я заметил что при каждом обновлении страницы, он считал меня как нового пользователя, посмотрев еще раз на ссылку я понял что $RANDOM в amp-pixel каждый раз создавал новый ID для меня и GA считал меня уником. Первое что мне пришло в голову, это написания цикла который будет выводить кучу amp-pixel. Для начала я решил создать 100 amp-pixel чтобы не нагружать браузер. Вот сам цикл p.s. я использую laravel, поэтому разметка blade @for($i=0;$i<100;$i++) <amp-pixel src=\"https://www.google-analytics.com/collect?v=1&t=pageview&z={{rand(100000,500000)}}&dt=$TITLE&dl=$CANONICAL_URL&tid=UA-59188XXX-1&cid=$RANDOM\" > </amp-pixel> @endfor Я открыл страницу, дождался прогрузки и решил посмотреть на Google Analytics. И вот что я там увидел… , Google Analytics показывал что у меня на сайте 100 активных пользователей и все они читают ту статью. Я решил попробовать увеличить цикл до 1000 и у меня стало 1000 активных пользователей. Google ненавидит КазахстанИз прошлого спойлера вы знаете что нет выпуска «Казахстан» в Google News, но можно добавить свой сайт в новости, и они будут отображаться в секции Казахстан в Российском выпуске. В Google Новости для издателей можно добавить особый RSS «Выбор редакции» и он должен отображаться в правой части на главной news.google.ru, я благополучно добавил этот rss фид и стал ждать. Прождав неделю я решил написать в саппорт гугла и там я узнал что «Выбор редакции» не будет отображаться, т.к. СМИ казахстанский, а выпуск Российский (так какого х**, казахстанский СМИ вообще выходит в Российском выпуске???) Определенно Google ненавидит Казахстан Но показатель отказов был 100%, глубина 1 да и рендеринг amp-pixel сильно грузил процессов. Поэтому я сначала решил создать обычную версию страницы(Обычный HTML, вместо AMP) и вставить туда сотню изображении с этой ссылкой. Как оказалось это работало и в обычном HTML. Теперь надо было решать проблему глубины и отказов, поэтому за место рандомной генерации ID пользователя я решил создать массив. $users = range(1,2000); Дальше создаем 2 страницы с разными ссылками и названиями, где есть ссылка на другую страницу. Заходим на первую страницу ждем пока загрузятся картинки, и переходим на 2-ую страницу и так с десяток раз. Показательная гифка В итоге мы имеем приличную глубину, большое кол-во просмотров, низкий показатель отказов и т.д. Думаю что можно накрутить и другие показатели, так что экспериментируете :) P.S. Чуть не забыл, вы можете накрутить статистику кому угодно, главное знать идентификатор отслеживания."], "hab": ["Программирование", "Информационная безопасность"]}{"url": "https://habrahabr.ru/post/322208/", "title": ["Иммутабельные данные в C++"], "text": ["Привет, Хабр! Об иммутабельных данных немало говориться, но о реализации на С++ найти что-то сложно. И, потому, решил данный восполнить пробел в дебютной статье. Тем более, что в языке D есть, а в С++ – нет. Будет много кода и много букв. О стиле – служебные классы и метафункции используют имена в стиле STL и boost, пользовательские классы в стиле Qt, с которой я в основном и работаю. Введение Что из себя представляют иммутабельные данные? Иммутабельные данные – это наш старый знакомый const, только более строгий. В идеале иммутабельность означает контекстно-независиую неизменяемость ни при каких условиях. По сути иммутабельные данные должны: обеспечивать физическую и логическую константность; запрещать присваивание нового значения на этапе компиляции; все операции должны проводиться над копией, а не над оригиналом. Иммутабельные данные пришли из функционального программирования и нашли место в параллельном програмировании, т. к. гарантируют отсутсвие побочных эффектов. Как можно реализовать иммутабельные данные в С++? В С++ у нас есть (сильно упрощенно): значения – объекты фундаментальных типов, экземпляры классов (структур, объединений), перечислений; указатели; ссылки; массивы. Функции и void не имеет смысл делать иммутабельными. Ссылки тоже не будем делать иммутабельными, для этого есть const reference_wrapper. Что касается остальных вышеперечисленных типов, то для них можно сделать обертки (а точнее нестандартный защитный заместитель). Что будет в итоге? Цель сделать как-бы модификатор типа, сохранив естественную семантику для работы с объектами данного типа. Immutable<int> a(1), b(2); qDebug() << (a + b).value() << (a + 1).value() << (1 + a).value(); int x[] = { 1, 2, 3, 4, 5 }; Immutable<decltype(x)> arr(x); qDebug() << arr[0] Интерфейс Общий интерфейс прост – всю работу выполняет базовый класс, который выводится из характеристик (traits): template <typename Type> class Immutable : public immutable::immutable_impl<Type>::type { public: static_assert(!std::is_same<Type, std::nullptr_t>::value, \"nullptr_t cannot used for immutable\"); static_assert(!std::is_volatile<Type>::value, \"volatile data cannot used for immutable\"); using ImplType = typename immutable::immutable_impl<Type>; using BaseClass = typename ImplType::type; using BaseClass::BaseClass; using value_type = typename ImplType::value_type; constexpr Immutable& operator=(const Immutable &) = delete; }; Запрещая оператор присваивания, мы запрещаем перемещающий оператор присваивания, но не запрещаем перемещающий конструктор. immutable_impl что-то вроде switch, но по типам (не стал делать такой – слишком усложняет код, да и в простом случае он не особо нужен – ИМХО). namespace immutable { template <typename SrcType> struct immutable_impl { using Type = std::remove_reference_t<SrcType>; using type = std::conditional_t< std::is_array<Type>::value, array<Type>, std::conditional_t < std::is_pointer<Type>::value, pointer<Type>, std::conditional_t < is_smart_pointer<Type>::value, smart_pointer<Type>, immutable_value<Type> > > >; using value_type = typename type::value_type; }; } В качестве ограничений явно запретив все операции присваивания (макросы помогают): template <typename Type, typename RhsType> constexpr Immutable<Type>& operator Op=(Immutable<Type> &&, RhsType &&) = delete; А теперь давайте рассотрим как реализованы отдельные компоненты. Иммутабельные значения Под значениями (далее value) понимаются объекты фундаментальных типов, экземпляры классов (структур, объединений), перечислений. Для value у на есть класс, который определяет является ли тип классом, структурой или объединением: template <typename Type, bool = std::is_class<Type>::value || std::is_union<Type>::value> class immutable_value; Если да, то для реализации используется используется CRTP: template <typename Base> class immutable_value<Base, true> : private Base { public: using value_type = Base; constexpr explicit immutable_value(const Base &value) : Base(value) , m_value(value) { } constexpr explicit operator Base() const { return value(); } constexpr Base operator()() const { return value(); } constexpr Base value() const { return m_value; } private: const Base m_value; }; К сожалению, в С++ пока нет перегрузки оператора .. Хотя, это ожидается в С++ 17 (http://open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0252r0.pdf, http://open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0252r0.pdf, http://www.open-std.org/JTC1/SC22/wg21/docs/papers/2015/p0060r0.html), но вопрос еще открыт, ибо коммитет нашел нестыковки. Тогда бы можно было просто написать: constexpr Base operator.() const { return value(); } Но решение по этому вопросу ожидается в марте, поэтому для этих целей пока используем оператор (): constexpr Base operator()() const { return value(); } Обратите внимание, на конструктор:~~ constexpr explicit immutable_value(const Base &value) : Base(value) , m_value(value) { } там инициализируется как immutable_value, так и базовый класс. Это позволяет осмысленно манипулировать с immutable_value через operator (). Например: QPoint point(100, 500); Immutable<QPoint> test(point); test().setX(1000); // не поменяет исходный объект qDebug() << test().isNull() << test().x() << test().y(); Если же тип является встроенным, то реализация будет один-в-один, за исключением базового класса (можно было бы изъвернуться, чтобы соответствовать DRY, но как-то не хотелось усложнять, тем более, что immutable_value делался после остальных...): template <typename Type> class immutable_value<Type, false> { public: using value_type = Type; constexpr explicit immutable_value(const Type &value) : m_value(value) { } constexpr explicit operator Type() const { return value(); } constexpr Type operator()() const { return value(); } // Base operator . () const // { // return value(); // } constexpr Type value() const { return m_value; } private: const Type m_value; }; Иммутабельные массивы Пока вроде бы просто и неинтересно, но теперь примемся за массивы. Надо сделать что-то вроде std::array сохранив естественную семантику работы с массивом, в том числе для работы с STL (что может ослабить иммутабельность). Особенность релизации заключается в том, что при обращении по индексу к многомерному возвращается массив меньшей размерности, тоже иммутабельный. Тип массива рекурсивно инстанцируется: см. operator[], а конкретные типы для итераторов и т.д выводятся с помощью array_traits. namespace immutable { template <typename Tp> class array; template <typename ArrayType> struct array_traits; template <typename Tp, std::size_t Size> class array<Tp[Size]> { typedef Tp* pointer_type; typedef const Tp* const_pointer; public: using array_type = const Tp[Size]; using value_type = typename array_traits<array_type>::value_type; using size_type = typename array_traits<array_type>::size_type; using iterator = array_iterator<array_type>; using const_iterator = array_iterator<array_type>; using const_reverse_iterator = std::reverse_iterator<const_iterator>; constexpr explicit array(array_type &&array) : m_array(std::forward<array_type>(array)) { } constexpr explicit array(array_type &array) : m_array(array) { } ~array() = default; constexpr size_type size() const noexcept { return Size; } constexpr bool empty() const noexcept { return size() == 0; } constexpr const_pointer value() const noexcept { return data(); } constexpr value_type operator[](size_type n) const noexcept { return value_type(m_array[n]); } // рекурсивное инстанцирование для типа меньшей размерности constexpr value_type at(size_type n) const { return n < Size ? operator [](n) : out_of_range(); } const_iterator begin() const noexcept { return const_iterator(m_array.get()); } const_iterator end() const noexcept { return const_iterator(m_array.get() + Size); } const_reverse_iterator rbegin() const noexcept { return const_reverse_iterator(end()); } const_reverse_iterator rend() const noexcept { return const_reverse_iterator(begin()); } const_iterator cbegin() const noexcept { return const_iterator(data()); } const_iterator cend() const noexcept { return const_iterator(data() + Size); } const_reverse_iterator crbegin() const noexcept { return const_reverse_iterator(end()); } const_reverse_iterator crend() const noexcept { return const_reverse_iterator(begin()); } constexpr value_type front() const noexcept { return *begin(); } constexpr value_type back() const noexcept { return *(end() - 1); } private: constexpr pointer_type data() const noexcept { return m_array.get(); } [[noreturn]] constexpr value_type out_of_range() const { throw std::out_of_range(\"array: out of range\");} private: const std::reference_wrapper<array_type> m_array; }; } Для определения типа меньшей размерности используется класс характеристик: namespace immutable { template <typename ArrayType, std::size_t Size> struct array_traits<ArrayType[Size]> { using value_type = std::conditional_t<std::rank<ArrayType[Size]>::value == 1, ArrayType, array<ArrayType> // immutable::array >; using size_type = std::size_t; }; } который для многомерных массивов для при индексировании возвращает иммутабельный массив меньшей размерности. Операторы сравнения очень просты: Операторы сравненияtemplate<typename Tp, std::size_t Size> inline bool operator==(const array<Tp[Size]>& one, const array<Tp[Size]>& two) { return std::equal(one.begin(), one.end(), two.begin()); } template<typename Tp, std::size_t Size> inline bool operator!=(const array<Tp[Size]>& one, const array<Tp[Size]>& two) { return !(one == two); } template<typename Tp, std::size_t Size> inline bool operator<(const array<Tp[Size]>& a, const array<Tp[Size]>& b) { return std::lexicographical_compare(a.begin(), a.end(), b.begin(), b.end()); } template<typename Tp, std::size_t Size> inline bool operator>(const array<Tp[Size]>& one, const array<Tp[Size]>& two) { return two < one; } template<typename Tp, std::size_t Size> inline bool operator<=(const array<Tp[Size]>& one, const array<Tp[Size]>& two) { return !(one > two); } template<typename Tp, std::size_t Size> inline bool operator>=(const array<Tp[Size]>& one, const array<Tp[Size]>& two) { return !(one < two); } Иммутабельный итератор Для работы с иммутабельным массивом используется иммутабельный итератор array_iterator: namespace immutable { template <typename Tp> class array; template <typename Array> class array_iterator : public std::iterator<std::bidirectional_iterator_tag, Array> { public: using element_type = std::remove_extent_t<Array>; using value_type = std::conditional_t< std::rank<Array>::value == 1, element_type, array<element_type> >; using ptr_to_array_type = const element_type *; static_assert(std::is_array<Array>::value, \"Substitution error: template argument must be array\"); constexpr array_iterator(ptr_to_array_type ptr) : m_ptr(ptr) { } constexpr value_type operator *() const { return value_type(*m_ptr);} constexpr array_iterator operator++() { ++m_ptr; return *this; } constexpr array_iterator operator--() { --m_ptr; return *this; } constexpr bool operator == (const array_iterator &other) const { return m_ptr == other.m_ptr; } private: ptr_to_array_type m_ptr; }; template <typename Array> inline constexpr array_iterator<Array> operator++(array_iterator<Array> &it, int) { auto res = it; ++it; return res; } template <typename Array> inline constexpr array_iterator<Array> operator--(array_iterator<Array> &it, int) { auto res = it; --it; return res; } template <typename Array> inline constexpr bool operator != (const array_iterator<Array> &a, const array_iterator<Array> &b) { return !(a == b); } } Отделение массивов от указателей сделано сознательно, несмотря на их близкое родство. В итоге, получим что-то вроде: Пример кода с иммутабельным массивомint x[5] = { 1, 2, 3, 4, 5 }; int y[5] = { 1, 2, 3, 4, 5 }; immutable::array<decltype(x)> a(x); immutable::array<decltype(y)> b(y); qDebug() << (a == b); const char str[] = \"abcdef\"; immutable::array<decltype(str)> imstr(str); auto it = imstr.begin(); while(*it) qDebug() << *it++; Для многомерных массивов все тоже самое: Пример с многомерным иммутабельным массивомint y[2][3] = { { 1, 2, 3 }, { 4, 5, 6 } }; int z[2][3] = { { 1, 2, 3 }, { 4, 5, 6 } }; immutable::array<decltype(y)> b(y); immutable::array<decltype(z)> c(z); for(auto row = b.begin(); row != b.end(); ++row) { qDebug() << \"(*row)[0]\" << (*row)[0]; } for(int i = 0; i < 2; ++i) for(int j = 0; j < 2; ++j) qDebug() << b[i][j]; qDebug() << (b == c); for(auto row = b.begin(); row != b.end(); ++row) { for(auto col = (*row).begin(); col != (*row).end(); ++col) qDebug() << *col; } Иммутабельные указатели Попробуем слегка обезопасить указатели. В этом разделе рассмотрим обычные указатели (raw pointers), а далее (сильно далее) рассмотрим smart pointers. Для smart pointers будет использоваться SFINAE. По реализации immutable::pointer скажу сразу, что pointer не удаляет данные, не считает ссылки, а только обеспечивает неизменяемость объекта. (Если переданный указатель изменен или удален из-вне, то это нарушение контракта, которое средствами языка не отследить (стандартными средствами)). В конце-концов, защититься от умышленного вредительства или игры с адресами невозможно. Указатель должен быть корректно инициализирован. immutable::pointer может работать с указателями на указатели любой степени ссылочности (скажем так). Например: Пример работы с иммутабельными указателямиimmutable::pointer<QApplication*> app(&a); app->quit(); char c = 'A'; char *pc = &c; char **ppc = &pc; char ***pppc = &ppc; immutable::pointer<char***> x(pppc); qDebug() << ***x; Кроме вышеперечисленного, immutable::pointer не поддерживает работы со строками в стиле С: const char *cstr = \"test\"; immutable::pointer<decltype(str)> p(cstr); while(*p++) qDebug() << *p; Данный код будет работать не так как ожидается, т.к. immutable::pointer при инкременте возвращает новый immutable::pointer с другим адресом, а в условном выражении будет проверяться результат инкремента, т.е. значение второго символа строки. Вернемся к реализации. Класс pointer предоставляет общий интерфейс и, в зависимости от того что из себя представляет Tp (указатель на указатель или прото указатель) использует конкретную реализации pointer_impl. template <typename Tp> class pointer { public: static_assert( std::is_pointer<Tp>::value, \"Tp must be pointer\"); static_assert(!std::is_volatile<Tp>::value, \"Tp must be nonvolatile pointer\"); static_assert(!std::is_void<std::remove_pointer_t<Tp>>::value, \"Tp can't be void pointer\"); typedef Tp source_type; typedef pointer_impl<Tp> pointer_type; typedef typename pointer_type::value_type value_type; constexpr explicit pointer(Tp ptr) : m_ptr(ptr) { } constexpr pointer(std::nullptr_t) = delete; // Перегрузка защищает от 0 ~pointer() = default; constexpr const pointer_type value() const { return m_ptr; } /** * @brief operator = необязательное объявление, т.к const *const автоматически * запрещает присваивание. * При попытке присвоить, компиляторы дают несколько избыточных ошибок, * которые могут быть разбросаны по файлам и малоинформативны, * а явное описание \" = delete\" приводит к тому, что диагностируется * только одна конкретная ошибка */ pointer& operator=(const pointer&) = delete; constexpr /*immutable<value_type>*/ value_type operator*() const { return *value(); } constexpr const pointer_type operator->() const { return value(); } // добавим неоднозначности template <typename T> constexpr operator T() = delete; template <typename T> constexpr operator T() const = delete; /** * @brief operator [] не реализован сознательно, чтобы не смешивать массивы * и указатели. * * Использование типов-аргументов по-умолчанию помогают компилятору * дать более короткое и конкретное сообщение об ошибке * (использовании удаленной функции) * @return */ template <typename Ret = std::remove_pointer_t<Tp>, typename IndexType = ssize_t> constexpr Ret operator[](IndexType) const = delete; constexpr bool operator == (const pointer &other) const { return value() == other.value(); } constexpr bool operator < (const pointer &other) const { return value() < other.value(); } private: const pointer_type m_ptr; }; Суть следующая: был тип T , а для его хранения/представления используется (шаблонно-рекурсивно) реализация pointer_impl<T , true>, что можно изобразить так: pointer_impl<T***, true>{ pointer_impl<T**, true> { pointer_impl<T*, false> { const T *const } } } Итого, получается: const T const const *const. Для простого указателя (который не указывает на другой указатель) реализация следующая: template <typename Type> class pointer_impl<Type, false> { public: typedef std::remove_pointer_t<Type> source_type; typedef source_type *const pointer_type; typedef source_type value_type; constexpr pointer_impl(Type value) : m_value(value) { } constexpr value_type operator*() const noexcept { return *m_value; // * для обычных указателей } constexpr bool operator == (const pointer_impl &other) const noexcept { return m_value == other; } constexpr bool operator < (const pointer_impl &other) const noexcept { return m_value < other; } constexpr const pointer_type operator->() const noexcept { using class_type = std::remove_pointer_t<pointer_type>; static_assert(std::is_class<class_type>::value || std::is_union<class_type>::value , \"-> used only for class, union or struct\"); return m_value; } private: const pointer_type m_value; }; Для вложенных указателей (указатели на указатели): template <typename Type> class pointer_impl<Type, true> { public: typedef std::remove_pointer_t<Type> source_type; typedef pointer_impl<source_type> pointer_type; typedef pointer_impl<source_type> value_type; constexpr /* implicit */ pointer_impl(Type value) : m_value(*value) { // /\\ remove pointer } constexpr bool operator == (const pointer_impl &other) const { return m_value == other; // рекурсивное инстанцирование } constexpr bool operator < (const pointer_impl &other) const { return m_value < other; // рекурсивное инстанцирование } constexpr value_type operator*() const { return value_type(m_value); // рекурсивное инстанцирование } constexpr const pointer_type operator->() const { return m_value; } private: const pointer_type m_value; }; Что не надо делать!Для следующих видов указателей особого смысла не стоит делать специализации: указатель на массив (*)[]; указатель на функцию(*)(Args… [...]); указатель на переменную класса, Class:: весьма специфичная вещь, нужна при \"колдовстве\" с классом, нужно связывать с объектом; -указатель на метод класса (Class::)(Args… [...]) [const][volatile]. Иммутабельные smart pointers Как определить что перед нами smart pointer? Smart pointers реализуют операторы * и ->. Чтобы определить их наличие воспользуемся SFINAE (реализацию SFINAE рассмотрим позже): namespace immutable { // is_base_of<_Class, _Tp> template <typename Tp> class is_smart_pointer { DECLARE_SFINAE_TESTER(unref, T, t, t.operator*()); DECLARE_SFINAE_TESTER(raw, T, t, t.operator->()); public: static const bool value = std::is_class<Tp>::value && GET_SFINAE_RESULT(unref, Tp) && GET_SFINAE_RESULT(raw, Tp); }; } Скажу сразу, что через operator ->, увы, используя косвенное обращение, можно нарушить иммутабельность, особенно если в классе есть mutable данные. Кроме того константность возвращаемого значения может быть снята, как компилятором (при выводе типа), так и пользователем. Реализация – здесь все просто: namespace immutable { template <typename Type> class smart_pointer { public: constexpr explicit smart_pointer(Type &&ptr) noexcept : m_value(std::forward<Type>(ptr)) { } constexpr explicit smart_pointer(const Type &ptr) : m_value(ptr) { } constexpr const auto operator->() const { const auto res = value().operator->(); return immutable::pointer<decltype(res)>(res);// in C++17 immutable::pointer(res); } constexpr const auto operator*() const { return value().operator*(); } constexpr const Type value() const { return m_value; } private: const Type m_value; }; } SFINAE Что это такое и с чем его едят лишний раз объяснять не надо. С помощью SFINAE можно определить наличие в классе методов, типов-членов и т.д, даже наличие перегруженных функций (если задать в выражении testexpr вызов нужной функции с необходимыми параметрами). arg может быть пустым и не участвовать в testexpr. Здесь используется SFINAE с типами и SFINAE с выражениями: #define DECLARE_SFINAE_BASE(Name, ArgType, arg, testexpr) \\ typedef char SuccessType; \\ typedef struct { SuccessType a[2]; } FailureType; \\ template <typename ArgType> \\ static decltype(auto) test(ArgType &&arg) \\ -> decltype(testexpr, SuccessType()); \\ static FailureType test(...); #define DECLARE_SFINAE_TESTER(Name, ArgType, arg, testexpr) \\ struct Name { \\ DECLARE_SFINAE_BASE(Name, ArgType, arg, testexpr) \\ }; #define GET_SFINAE_RESULT(Name, Type) (sizeof(Name::test(std::declval<Type>())) == \\ sizeof(typename Name::SuccessType)) И еще: перегрузку можно разрешить (найти нужную перегруженную функцию) если сигнатуры совпадают, но отличаются квалификатором const [ volatile ] или volatile совместно с SFINAE в три фазы: 1) SFINAE — если есть, то ОК 2) SFINAE + QNonConstOverload, если не получилось, то 3) SFINAE + QConstOverload В исходниках Qt можно найти интересную и полезную вещь: Разрешение перегрузки с const template <typename... Args> struct QNonConstOverload { template <typename R, typename T> Q_DECL_CONSTEXPR auto operator()(R (T::*ptr)(Args...)) const Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } template <typename R, typename T> static Q_DECL_CONSTEXPR auto of(R (T::*ptr)(Args...)) Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } }; template <typename... Args> struct QConstOverload { template <typename R, typename T> Q_DECL_CONSTEXPR auto operator()(R (T::*ptr)(Args...) const) const Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } template <typename R, typename T> static Q_DECL_CONSTEXPR auto of(R (T::*ptr)(Args...) const) Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } }; template <typename... Args> struct QOverload : QConstOverload<Args...>, QNonConstOverload<Args...> { using QConstOverload<Args...>::of; using QConstOverload<Args...>::operator(); using QNonConstOverload<Args...>::of; using QNonConstOverload<Args...>::operator(); template <typename R> Q_DECL_CONSTEXPR auto operator()(R (*ptr)(Args...)) const Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } template <typename R> static Q_DECL_CONSTEXPR auto of(R (*ptr)(Args...)) Q_DECL_NOTHROW -> decltype(ptr) { return ptr; } }; Итог Попробуем что получилось: QPoint point(100, 500); Immutable<QPoint> test(point); test().setX(1000); // не поменяет исходный объект qDebug() << test().isNull() << test().x() << test().y(); int x[] = { 1, 2, 3, 4, 5 }; Immutable<decltype(x)> arr(x); qDebug() << arr[0]; Операторы Давате вспомним про операторы! Например, добавим поддержку оператора сложения: Сначала реализуем оператор сложения вида Immutable<Type> + Type: template <typename Type> inline constexpr Immutable<Type> operator+(const Immutable<Type> &a, Type &&b) { return Immutable<Type>(a.value() + b); } В С++17 вместо return Immutable<Type>(a.value() + b); можно записать return Immutable(a.value() + b); Т.к. оператор + коммутативен, то Type + Immutable<Type> можно реализовать в виде: template <typename Type> inline constexpr Immutable<Type> operator+(Type &&a, const Immutable<Type> &b) { return b + std::forward<Type>(a); } И снова, через первую форму реализуем Immutable<Type> + Immutable<Type>: template <typename Type> inline constexpr Immutable<Type> operator+(const Immutable<Type> &a, const Immutable<Type> &b) { return a + b.value(); } Теперь можем работать: Immutable<int> a(1), b(2); qDebug() << (a + b).value() << (a + 1).value() << (1 + a).value(); Аналогично можно определить остальные операции. Вот только не надо перегружать операторы получения адреса, &&, ||! Унарные +, -, !, ~ могут пригодиться… Эти операции наследуются: (), [], ->, ->, (унарный). Операторы сравнения должны возвращать значения булевского типа: Операторы сравненияtemplate <typename Type> inline constexpr bool operator==(const Immutable<Type> &a, const Immutable<Type> &b) { return a.value() == b.value(); } template <typename Type> inline constexpr bool operator!=(const Immutable<Type> &a, const Immutable<Type> &b) { return !(a == b); } template <typename Type> inline constexpr bool operator>(const Immutable<Type> &a, const Immutable<Type> &b) { return a.value() > b.value(); } template <typename Type> inline constexpr bool operator<(const Immutable<Type> &a, const Immutable<Type> &b) { return b < a; } template <typename Type> inline constexpr bool operator>=(const Immutable<Type> &a, const Immutable<Type> &b) { return !(a < b); } template <typename Type> inline constexpr bool operator<=(const Immutable<Type> &a, const Immutable<Type> &b) { return !(b < a); }"], "hab": ["Функциональное программирование", "Ненормальное программирование", "D", "C++"]}{"url": "https://habrahabr.ru/post/322256/", "title": ["Почему Kotlin отстой"], "text": ["Эта статья родилась по мотивам вот этой статьи в виде полу-шутки. В той статье большая часть \"проблем\" является либо синтетическими и крайне редко используемыми, либо притянутыми за уши из-за ожидания соответствия языка теоретической парадигме которой, по мнению автора, язык должен соответствовать. С другой стороны не упомянуты вещи, которые мне лично действительно усложняют жизнь. Я ни в коем случае не претендую на абсолютные знания Kotlin, поэтому в статье могут быть ошибки. Я буду благодарен, если Вы в комментариях мне укажете на способы решения тех проблем, которые у меня возникают. Убогий for Самую могучую конструкция для реализации любых циклических действий for в Kotlin превратили в самую бесполезную вещь, реализуемую самим же Kotlin всего одной строкой. inline fun <T> For(it : Iterator<T>, cb : (T) -> Unit) { while (it.hasNext()) cb(it.next()) } fun main(a : Array<String>) { val list = listOf(1, 3, 4, 12) println(\"for\"); for (it in list) println(it) println(\"FOR\"); For(list.iterator()) { println(it) } val arr = arrayOf(1, 3, 4, 12) println(\"a-for\"); for (it in arr) println(it) println(\"a-FOR\"); For(arr.iterator()) { println(it) } println(\"r-for\"); for (it in 0..10) println(it) println(\"r-FOR\"); For((0..10).iterator()) { println(it) } } Как видно по примеру выше даже такая примитивная реализация For не просто работает абсолютно одинаково с for, но и во всех случаях кроме работы с массивом еще и абсолютна идентична ему по генерируемому коду. Дописав еще несколько строк можно даже добиться того, что писанины самодельный аналог будет требовать меньше штатного. Вопрос: зачем было вообще вводить это ключевое слово в язык и реализовывать жалкую породию частного случая цикла? Убогий цикл и без того уже есть. Собственно, бог бы с ним с этим недоделанным for-ом, если бы была альтернатива. Но ее нет. К сожалению, жизнь на итераторах не заканчивается, а когда приходится писать какие-то сложные циклы, то приходится жестоко страдать с убогим while-ом. Истерично-бессмысленная война с null-абле Может быть из-за того, что я стар, а может быть из-за того, что уже лет 25 успешно пишу на С, где (sic!) есть такая вещь как void*, я не испытываю никакого экстаза от повторения вслух шаблонных: \"стрельба в ногу\" и \"ошибка на миллион\". В результате, я просто не понимаю с чем воюют. Какая разница, когда хлопнется программа, на проверке аргументов или на их использовании? В чем соль декларирования null-safety Kotlin-ом, если он ее даже теоретически обеспечить не может? Значение null есть в самом языке, оно есть в Java, без инфраструктуры которой Kotlin, скажем прямо, не представляет никакого интереса. Как можно защититься от того, что используется за пределами языка и никак им не контролируется? Да никак. Это не более чем модная тенденция, уродование исходных текстов и регулярный геморой. Впрочем, я далек от того чтобы учить других как именно им жить. Если кому-то хочется какого-то самоуспокоения, то кто я такой чтобы им в этом мешать? И я бы абсолютно спокойно игнорировал ажиотаж вокруг null, если бы не регулярный геморой с ним. Вернее с той абстракцией, которой Kotlin старательно усложняем мне жизнь. var value : Int? = null fun F() : Int { if ( value != null ) return 0 return value // Ошибка } Ошибка Smart cast to 'Int' is impossible, because 'value' is a mutable property that could have been changed by this time просто задалбывает. Хочется кого-то убить или что-то сломать. Где, как и кем эта проперть может быть модифицирована между двумя строчками??!!! Соседним тредом? Откуда взялась эта абсолютно бредовая уверенность компилятора в том, что каждая буква моей программы — это элемент многопоточной конкуренции? Даже в случае написания жестокого многопоточного кода пересечения тридов случаются на очень малом объеме текста программы, но из-за репрессивной заботы компилятора о такой возможности я имею гиморрой с клинописью постоянно. Кто придумал два восклицательных знака? Неуд! Два еще недостаточно взрывают мозг. Надо было пять. Или десять. И с обоих сторон. Так уж точно было бы понятно, где тут самый не кошерный и \"небезопасный\" код. var value : Int? = null fun F() : Int { if (value == null) return 0 return when (Random().nextInt()) { 3 -> value!! + 2 12 -> value!! + 1 5 -> value!! * 4 else -> 0 } } Самое пакостное, что жизнь никак не укладывается в красивые представления о \"безопасном\" коде тех, кому нужно каждый год продавать новую книгу о свежих тенденциях. К сожалению, null — это нормальное \"неизвестное\" состояние множества объектов, но при работе с ними постоянно приходится писать абсолютно ненужную клинопись. Смешно же во всей этой чепухе вокруг null то, что это не работает. Я уже почти смирился с писаниной бездарной клинописи в своем коде с надеждой, что \"зато когда-нибудь это спасет\". Ага, щаз. Java public class jHelper { public static jHelper jF() { return null; } public void M() {} } Kotlin fun F() { val a = jHelper.jF() a.M() //Упс! } Это замечательно компилируется без каких-либо ошибок или предупреждений, запускается и с грохотом схлопытвается cо стандартным NullPointerException т.к. тут Kotlin не проверяет ничего и нигде. И где обеща..., тьфу, декларируемая безопасность? В общем, в сухом остатке я имею следующее: регулярный гиморой с преодолением надуманных проблем в моем коде; постоянные приседания с !! при работе с nullable типами в моем коде; постоянный оверхед, генерируемый компилятором на проверках всех параметров функций и при установке любых значений в моем коде; нулевую безопасность для любых данных пришедших извне; Т.е. весь гиморрой только в той части, которую я знаю и контролирую, а все наружнее молча посыпется при первой же возможности. Зашибись, зачет! Зато все красиво и по феншую. Почему присваивание — это не выражение? Даже if это убогое, но выражение, а присваиванию эту возможность отрезали. Почему я не могу написать так? var value = 10 fun F() : Int { return value = 0 // Ошибка } или так: var v1 = 1 var v2 = 1 var v3 = 1 fun F() { v1 = v2 = v3 = 0 // Ошибка } Что в этом коде криминального? Хотя, я наверное, догадаюсь. Защищаем пользователя от if (v=20)?.. Но, врядли, т.к. это просто не соберется без автоматического приведения типов, которого у Kotlin, опять-же, нет. Сдаюсь. Кто знает ответ? Чем не угодил оператор \"?:\"? За что ампутировали оператор \"?:\"? Что усмотрели вредного в таких конструкциях? value != 0 ? \"Y\" : \"N\" С if все замечательно: if (value != 0) \"Y\" else \"N\" кроме полной альтернативности (где такое еще есть?) и того, что часто побочная писанина if () else места занимает больше, чем само выражение. За что убили автоматическое приведение типов? Да, тотальное приведение типов друг к другу — это чистое и незамутненное зло. Я обоими руками против того, чтобы плодить ребусы, к которым приводит взаимное преобразование чисел и строк. В принципе, я даже за то, чтобы различать целочисленное и плавучку. Но зачем было совсем все-то вырезать?! Почему нельзя использовать стандартные и общепринятые правила приведения типов, которые существуют в подавляющем большинстве языков? Ну ладно, пусть даже отрезали. Привет Pascal. Но зачем в документации-то врать про «there are no implicit widening conversions for numbers»? Где оно \"are no\", если такое замечательно собирается? val i = 10 val l = 12L val f = 12.1 val l1 = i+100/l-f Где ожидаемый хардкор?! val l1 = i.toDouble() + 100.toDouble() / l.toDouble() - f Т.е. авто-приведения типов нет… хотя… оно как-бы есть… но только для выражений… и еще для констант. А вот если передать в качестве параметра надо переменную или там присвоить в переменную без вычислений — тут уже ручная гребля в санях. Ведь это так принципиально, и нужно акцентировать все внимание на том, что вот из этого Int получается именно Long, а из этого Float именно Double. Я прямо чувствую, как количество ошибок в моей программе стремительно тает от такой заботы обо мне. Хотел бы я еще заикнуться про крайне желательное: val c : SomeClass? = null if ( c ) \"not-null\" if ( !c ) \"is-null\" но не буду т.к. опасаюсь за свою жизнь. Недо-typedef Давно просили прикрутить к Kotlin псевдонимы. Прикрутили. Я не знаю в каких случаях люди это планируют использовать но, на мой взгляд, толку от такой реализации примерно ноль. Назвали бы эту конструкцию макросом — у меня притензий бы не было, а так… обман какой-то. Давайте разберемся в каких ситуациях вообще нужны псевдонимы в каком-нибудь языке. Я могу предположить следующее их применение: Создание альтернативного имени для существующего класса. Задача довольно бестолковая но, возможно, кому-то пригодится. С этим существующие псевдонимы справляются полностью. Создание нового типа без создания нового класса. Эту задачу существующие псевдонимы решить не способны вообще т.к. они не являются самостоятельным типом. Различить два псевдонима, отличающихся только именем невозможно. Уменьшение писанины при использовании шаблонных типов. Эта задача самая полезная и часто используемая. Существующие псевдонимы могут решить только описательную ее часть (см п.1), т.е. их можно использовать для описания типа переменных, параметров, возвращаемого значения и создать объект такого (базового) типа. Певдоним для шаблонного типа нельзя использовать для приведения или проверки типа объекта. На практике мы имеем следующее: typealias aI = SuperPuperClassA typealias pSI = Pair<String,Int> typealias pIS = Pair<Int,String> typealias pOTHER = Pair<String,Int> typealias aS = List<String> class SuperPuperClassA { fun F() = pSI(\"\",10) } fun main(a : Array<String>) { val a = aI() val i1 = a.F() val i2 : Pair<*,*> = a.F() val i3 : Any = a.F() //Этот код собирается и условие выполняется if ( i1 is pSI ) println(\"ok\") if ( i1 is pOTHER ) println(\"ok\") //Этот код НЕ собирается if ( i1 is pIS ) println(\"not compile\") if ( i2 is pSI ) println(\"not compile\") if ( i2 is pIS ) println(\"not compile\") if ( i3 is pSI ) println(\"not compile\") if ( i3 is pIS ) println(\"not compile\") } Обратите внимание на то, что в обоих строках где код собирается условие выполнится. Т.к. псевдоним не является полноценным типом, то различить их невозможно. Собственно, Kotlin мог бы их различать хотя бы в случаях, как в этом примере (весь код с явными и известными типами), но, видимо, нет желания. Код, которые не собирается, имеет одну и ту же проблему: \"Cannot check for instance of erased type\". Проблема в недоразвитости (попросту отсутствии) шаблонов в рантайме JVM. Итого. Псевдонимы в существующей реализации выполняют роль текстовых макросов, которые попросту заменяют один текст на другой. Попытка выжать из них какое-то интеллектуальное поведение может привести только к огорчениям или ошибкам в программе. А, да, я говорил что псевдонимы можно описывать только глобальные, вне любого класса? Nested and local type aliases are not supported В результате их неудобно использовать и как макросы, для уменьшения писанины внутри одного класса, т.к. даже с модификатором private они \"светятся\" на весь текущий проект. Убогие шаблоны Шаблоны (generics) в Java вообще и в Kotlin в частности убоги и причина абсолютно одна и та же: JVM ничего не знает о шаблонах и все эти треугольные скобки в языке не более чем навесное синтаксическое украшательство. Бог с ней с Java т.к. меня лично ее проблемы не волнуют. Меня волнует ущербность шаблонов конкретно в Kotlin, который позиционируется как другой язык, а не препроцессор для Java и, в результате, кивать на ее недостатки по меньшей мере бессмысленно. То что шаблонные типы нельзя (бессмысленно) использовать для проверки типа или его приведения еще как-то можно пережить т.к. об этом компилятор хотя бы ошибку выдаст, но это не все проблемы. Как Вам такой ребус: /*00*/ class C<T>(val value : Any) { /*01*/ fun F() : T { /*02*/ try { /*03*/ val v = value as T //Предупреждение компилятора \"Unchecked cast: Any to T\" /*04*/ return v /*05*/ } catch(ex : RuntimeException) { /*06*/ println(\"Incompatible\") /*07*/ // Хак для иллюстрации того, что эксепшин будет съеден и не пойдет дальше /*08*/ return 0 as T /*09*/ } /*10*/ } /*11*/ } /*12*/ /*13*/ fun fTest() { /*14*/ val a = C<Int>( 12.789 ) /*15*/ println( \"rc: ${a.F()}\" ) /*16*/ /*17*/ val b = C<Int>( \"12.123\" ) /*18*/ println( \"rc: ${b.F()}\" ) /*19*/ } В этом коде, в классе \"С\" делается попытка проверить совместим ли тип объекта с типом шаблона. Внимание, вопрос: как отработает этот код? Варианты ответов: Не соберется вообще Соберется, выполнится и напечатает \"12\", \"12\" Соберется, выполнится и напечатает \"12\", \"Incompatible\" Соберется, выполнится и напечатает \"12.789\", \"12.123\" Хлопнется при запуске внутри функции \"C::F\" (на какой строке?) Хлопнется при запуске внутри функции \"fTest\" (на какой строке?) Правильный ответПравильный ответ: хлопнется при запуске внутри функции \"fTest\" на строке 18 rc: 12 Exception in thread \"main\" java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number at jm.test.ktest.KMainKt.fT(kMain.kt:18) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) Следующий конкурс: кто может объяснить почему это происходит? Почему не упало на первом вызове, где передается Double вместо Int? Почему не отработал блок try/catch? Как ошибка кастинга с ПРАВИЛЬНЫМИ типами смогла вообще доехать до кода используещего функцию \"C::F\"? Под капотомКратеньно, выводы делайте сами. Вот код, который генерирует Kotlin для проверки типа внутри \"C::F\": // val v = value as T GETFIELD jm/test/ktest/C.value : Ljava/lang/Object; CHECKCAST java/lang/Object ASTORE 1 Если очень сильно подумать (или заранее знать что оно неработоспособно), объяснить почему именно CHECKCAST Object можно. Сложнее объяснить зачем вообще этот код генерировать т.к. он абсолютная пустышка всегда, но это вопрос уже совсем к другой части компилятора. А вот код, который генерируется при вызове функции \"C::F\": LINENUMBER 18 L6 ALOAD 1 INVOKEVIRTUAL jm/test/ktest/C.F ()Ljava/lang/Object; CHECKCAST java/lang/Number Опять-же, если очень сильно думать (или знать заранее), то можно объяснить наличие правильных типов в этом месте, но лично для меня сам факт наличия проверки типов после вызова функции был неожиданностью. И да: Kotlin, оказывается, генерирует проверку типа снаружи при каждом использовании шаблонного результата для любого класса. В общем, несмотря на множество синтаксичесих прелестей шаблонов Kotlin, они могут подложить очень неожиданную и толстую свинью. Я все понимаю: шаблонов в Java нет и все подобное. Этот пункт, скорее всего, не появился бы вообще, если бы нормальную работу с шаблонами нельзя было бы реализовать в принципе никогда и нигде… Но вот у меня перед глазами яркий пример — VCL. Фирма Borland в богом забытом году умудрилась прикрутить не к чему-нибудь, а к С и Pascal настолько мощное RTTI, что альтернатив ему не существует до сих пор. А тут не машинный код, тут Java и обеспечить в ней полнофункциональное использование шаблонов в своем, Kotlin-овском коде можно. Но его нет. В результате, язык вроде бы и другой, а ситуация, из-за синтаксического разнообразия, еще хуже чем в самой Java. Напишем аналог шаблона из ребуса на Java. public class jTest<T> { Object value; jTest( Object v ) { value = v; } public T F() { return (T)value; } //Предупреждение компилятора \"Unchecked cast\" public static void Test() { jTest<Integer> a = new jTest<Integer>( 12.123 ); System.out.print( \"rcA: \" ); System.out.print( a.F() ); jTest<Integer> b = new jTest<Integer>( \"12.789\" ); System.out.print( \"\\nrcB: \" ); System.out.print( b.F() ); System.out.print( \"\\n\" ); } } И попробуем его вызвать из Kotlin и Java. fun fTJ_1() { val a = jTest<Int>( 12.123 ) println( \"rc: ${a.F()}\" ) val b = jTest<Int>( \"12.789\" ) println( \"rc: ${b.F()}\" ) } fun fTJ_2() { jTest.Test() } Я не буду утомлять разнообразием ребусов для всех возможных вариантов и сведу его к простейшему: как поведет себя программа, в которой: и шаблон и его использование реализовано на Kotlin; шаблон на Java, а его использование на Kotlin; и шаблон и реализацая на Java; и какие будут результаты выполнения программы в каждом случае? Варианты: Все примеры отработают одинаково. Все примеры отработают по разному. Все примеры, где реализация написана на Kotlin отработают одинаково, а с Java по другому. Правильный ответПравильный ответ: все три варианта поведут себя по разному Kotlin: rc: 12 Exception in thread \"main\" java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number Kotlin->Java: Exception in thread \"main\" java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer Java: rcA: 12.123 rcB: 12.789 Почему так а не иначе? А это будет домашнее задание. Нет синтаксиса для описания структур Если взять абсолютно любой сравнымый язык (хоть саму Java, хоть Scala, Groovy и множество прочих от Lua до, даже, С++) то в них во всех сделано так, чтобы было удобно описывать структуры данных в коде программы. Kotlin — это единственный известный мне язык, где синтаксиса для описания структур данных нет вообще. Есть (грубо говоря) всего три функции: listOf, mapOf и arrayOf. Если с массивами и спискамми синтаксис громоздок, но как-то структурируется зрительно: val iArr1 = arrayOf(1, 2, 3) val iArr2 = arrayOf( arrayOf(1, 2, 3), arrayOf(1, 2, 3), arrayOf(1, 2, 3) ) val iArr3 = arrayOf( arrayOf(arrayOf(1, 2, 3), arrayOf(1, 2, 3), arrayOf(1, 2, 3)), arrayOf(arrayOf(1, 2, 3), arrayOf(1, 2, 3), arrayOf(1, 2, 3)), arrayOf(arrayOf(1, 2, 3), arrayOf(1, 2, 3), arrayOf(1, 2, 3)) ) то с картами все значительно печальнее: val tree = mapOf( Pair(\"dir1\", mapOf(Pair(\"file1\", 0), Pair(\"file2\", 1))), Pair(\"dir2\", mapOf( Pair(\"dir21\", mapOf(Pair(\"file1\", 0), Pair(\"file2\", 1))), Pair(\"dir22\", mapOf(Pair(\"file1\", 0), Pair(\"file2\", 1))))) ) Я не знаю как именно другие пользователи описывают константные данные, но лично я испытываю жестокий дискомфорт при попытке использовать что-то сложнее одномерного списка. Понятно, что можно засунуть данные в любой подходящий формат (хотя бы тот же JSON и оттуда ее прочитать), однако... Засовывать каждый десяток строк в отдельный файл только для того, чтобы иметь возможность ими наглядно манипулировать — это как-то избыточно (хотя именно так и приходится делать). Усилия по написанию структуры кода прямо в программе и во внешнем файле, с последующим их чтением, просто несопоставимы. В случае изменения структуры данных приходится, помимо кода, править гору совершенно лишнего текста по обслуживанию его загрузки. В общем, концепция минимализма — это круто, но аццки неудобно. ПС: В виде отдельного гвоздя в голову я пожелаю кому-нибудь написать библиотеку для работы с матрицами. Зато научитесь понимать отличать Array<Array<Array<Array<Double>>>> и Array<Array<Array<Double>>> с первого взгляда и с любого расстояния."], "hab": ["Kotlin"]}{"url": "https://habrahabr.ru/post/321652/", "title": ["UNIX-подобные системы содержат кучу костылей. Крах «философии UNIX»"], "text": ["В первой части статьи перечислю кучу костылей UNIX, и вообще разных недостатков. Во второй — про «философию UNIX». Статья написана наскоро, «полировать» дальше не хочу, скажите спасибо, что написал. Поэтому многие факты привожу без ссылок. Костыли в UNIX начали возникать ещё с момента появления UNIX, а это было ещё раньше появления не только Windows, но даже вроде бы Microsoft DOS (вроде бы, мне лень проверять, проверяйте сами). Если лень читать, хотя бы просмотрите все пункты, что-нибудь интересное найдёте. Это далеко не полный список, это просто те косяки, который я захотел упомянуть. В самом начале make был программой, которую один человек написал для себя и нескольких своих знакомых. Тогда он, недолго думая, сделал так, что командами воспринимаются строки, которые начинаются с Tab. Т. е. Tab воспринимался отлично от пробела, что крайне некрасиво и нетипично ни для UNIX, ни за его пределами. Он так сделал, потому что не думал, что make будет ещё кто-то использовать кроме этой небольшой группы. Потом появилась мысль, что make — хорошая вещь и неплохо бы включить его в стандартный комплект UNIX. И тогда чтобы не сломать уже написанные мейкфайлы, т. е. написанные вот этими вот десятью людьми, он не стал ничего менять. Ну вот так и живём… Из-за тех десятерых страдаем мы все. Почти в самом начале в UNIX не было папки /usr. Все бинарники размещались в /bin и /sbin. Но потом вся инфа перестала помещаться на тот диск, который был в распоряжении авторов UNIX (Томпсон, Ритчи). Поэтому они достали ещё один диск, создали папку /usr, а в ней — ещё один bin и ещё один sbin. И смонтировали новый диск в /usr. Оттуда и пошло. Так появилась «вторая иерархия» /usr, а потом в какой-то момент ещё и «третья иерархия» /usr/local, а потом ещё и /opt. Как пишет рассказчик этой истории: «Не удивлюсь, если когда-нибудь ещё появится /opt/local». UPD от 2017-02-12: я нашёл ссылку, где я почерпнул эту историю. Читайте, там более точная версия произошедшего. sbin изначально означало «static bin», а не «superuser bin», как можно было бы подумать. И содержал sbin статические бинарники. Но потом sbin стал содержать динамические бинарники, его название потеряло смысл. Windows часто ругают за наличие реестра и сообщают при этом, что подход UNIX-подобных систем (куча конфигов) якобы лучше. А между прочим однажды в ext4 появилась особенность (является ли это багом, вопрос спорный), из-за которой при резком выключении компа Gnome потерял все свои конфиги в рабочей папке юзера. И разработчик этой ext4 сказал в обсуждении баг репорта, что Gnome'у надо было использовать что-то вроде реестра для хранения инфы. UPD от 2017-02-12: источники: раз и два. Имя отписавшегося maintainer'а ext4: Theodore Ts'o. Вот его слова: If you really care about making sure something is on disk, you have to use fsync or fdatasync. If you are about the performance overhead of fsync(), fdatasync() is much less heavyweight, if you can arrange to make sure that the size of the file doesn't change often. You can do that via a binary database, that is grown in chunks, and rarely truncated. I'll note that I use the GNOME desktop (which means the gnome panel, but I'm not a very major desktop user), and «find .[a-zA-Z]* -mtime 0» doesn't show a large number of files. I'm guessing it's certain badly written applications which are creating the «hundreds of dot files» that people are reporting become zero lengh, and if they are seeing it happen a lot, it must be because the dot files are getting updated very frequently. I don't know what the bad applications are, but the people who complained about large number of state files disappearing should check into which application were involved, and try to figure out how often they are getting modified. As I said, if large number of files are getting frequently modified, it's going to be bad for SSD's as well, there are multiple reasons to fix badly written applications, even if 2.6.30 will have a fix for the most common cases. (Although some server folks may mount with a flag to disable it, since it will cost performance.) И это не говоря уж о том, что критичные файлы UNIX (такие как /etc/passwd), который читаются при каждом (!) вызове, скажем, ls -l, записаны в виде простого текста. И эти файлы надо заново читать и заново парсить при каждом вызове ls -l! Было бы гораздо лучше использовать бинарный формат. Или БД. Или некий аналог реестра. Как минимум, для вот таких вот критичных для производительности ОС файлов. Two famous people, one from MIT and another from Berkeley (but working on Unix) once met to discuss operating system issues. The person from MIT was knowledgeable about ITS (the MIT AI Lab operating system) and had been reading the Unix sources. He was interested in how Unix solved the PC loser-ing problem. The PC loser-ing problem occurs when a user program invokes a system routine to perform a lengthy operation that might have significant state, such as IO buffers. If an interrupt occurs during the operation, the state of the user program must be saved. Because the invocation of the system routine is usually a single instruction, the PC of the user program does not adequately capture the state of the process. The system routine must either back out or press forward. The right thing is to back out and restore the user program PC to the instruction that invoked the system routine so that resumption of the user program after the interrupt, for example, re-enters the system routine. It is called «PC loser-ing» because the PC is being coerced into «loser mode,» where «loser» is the affectionate name for «user» at MIT. The MIT guy did not see any code that handled this case and asked the New Jersey guy how the problem was handled. The New Jersey guy said that the Unix folks were aware of the problem, but the solution was for the system routine to always finish, but sometimes an error code would be returned that signaled that the system routine had failed to complete its action. A correct user program, then, had to check the error code to determine whether to simply try the system routine again. The MIT guy did not like this solution because it was not the right thing. — The Rise of «Worse is Better» By Richard Gabriel Если кратко и своими словами, то в начале разработки UNIX авторы UNIX решили попросту выдавать ошибку из ядра пользовательской программе, если пользовательская программа прервана по сигналу, и на этот сигнал повешен обработчик. Иными словами, если вы перехватили Ctrl-C (т. е. поставили на него обработчик) в своей программе, а юзер за терминалом нажал этот самый Ctrl-C, то ОС выполнит обработчик, а потом вместо простого продолжения того сисвызова, который выполнялся в момент Ctrl-C, просто прервёт его, вернув из ядра в пользовательскую программу EINTR. В результате программисту, пишущему эту программу придётся эту EINTR предусмотреть. А это усложняет этот userspace код. Ценой упрощения кода ядра. Да, нужно было сделать по-другому. Усложнить код ядра и упростить userspace код, который придётся писать всем программистам. Но тому человеку из Беркли из цитаты выше было пофигу. Он фактически сказал: «Да мне пофиг, что все будут страдать, главное, чтоб код ядра попроще был». Дальше — больше. Позже в UNIX-системах всё же пофиксили упомянутую особенность, добавив так называемый SA_RESTART. То есть вместо того, чтобы просто всё пофиксить, они добавили специальный флаг. Так мало того, что они это сделали, этот SA_RESTART ещё и не всегда работает! В частности, в GNU/Linux select, poll, nanosleep и др. не продолжают свою работу после перехваченного прерывания даже в случае SA_RESTART! Вообще, конкретные обстоятельства, возникшие во время разработки оригинальной UNIX, сильно оказали на неё влияние. Скажем, читал где-то, что команда cp названа именно так, а не copy, потому что UNIX разрабатывали с использованием терминалов, которые очень медленно выдавали буквы. А потому набрать cp было быстрее, чем copy. UPD от 2017-02-12: найти именно ту ссылку, которую я видел когда-то давно, и в которой приводился пример с cp и copy, мне не удалось. Но есть, например, вот эта ссылка. Commands — Are These Real Words? The basic AIX commands (and all UNIX system commands) are, for the most part, very short, cryptic, two-letter command names. Imagine back years ago, when computers had only very slow teletype keyboards and paper “displays.” (Some of us aren’t imagining, we’re remembering!) Imagine also, people who didn’t like typing long commands because there was such a long delay between commands and the computer response. If there were any mistakes, the user had to retype the whole thing (especially aggravating for folks that type with only two fingers!). Also, some UNIX commands came from university students and researchers who weren’t bound by usability standards (no rules, merely peer pressure). They could write a very useful, clever command and name it anything—their own initials, for example (awk by Aho, Weinberger, and Kernighan), or an acronym (yacc, Yet Another Compiler-Compiler). Вообще, названия утилит UNIX — это отдельная история. Скажем, название grep идёт от командй g/re/p в текстовом редакторе ed. (Ну а cat — от concatenation, я надеюсь, это все и так знали :) Ну и для кучи: vmlinuz — gZipped LINUx with Virtual Memory support). printf внезапно является далеко не самым быстрым способом вывода информации на экран или в файл. Не знали, да? А дело в том, что printf, как и сама UNIX в целом, был придуман не для оптимизации времени, а для оптимизации памяти. printf каждый раз парсит в рантайме строку формата. Именно поэтому в веб сервере H2O был придуман специальный препроцессор, который переносит парсинг строки формата на этап компиляции. UPD от 2017-02-12: источник. Когда Кена Томпсона, автора UNIX (вместе с Деннисом Ритчи) спросили, что бы он поменял в UNIX, он сказал, что назвал бы функцию creat (sic!) как create. UPD от 2017-02-12: источников полно, например этот. No comments. Замечу, что позже этот же Кен Томпсон вместе с другими разработчиками оригинальной UNIX создал систему Plan 9, исправляющую многие недостатки UNIX. И в ней эта функция называется create :) Он смог :) Ещё одна цитата: A child which dies but is never waited for is not really gone in that it still consumes disk swap and system table space. This can make it impossible to create new processes. The bug can be noticed whenseveral & separators are given to the shell not followed by ancommand without an ampersand. Ordinarily things clean themselves upwhen an ordinary command is typed, but it is possible to get into asituation in which no commands are accepted, so no waits are done;the system is then hung.The fix, probably, is to have a new kind of fork which creates aprocess for which no wait is necessary (or possible); also to limit the number of active or inactive descendants allowed to a process. — Источник Это цитата из очень раннего манула UNIX. Уже тогда существование зомби-процессов признавалось багом. Но потом на этот баг попросту забили. Понятное дело, что гораздо позже эта проблема всё же была решена. Т. е. в современном GNU/Linux инструменты для убивания зомби-процессов всё же существуют. Но о них мало кто знает. Обычном kill'ом зомби не убиваются. Про существование зомби-процессов все говорят: «It's for design». Ещё немного про уже упомянутый язык C. Вообще язык C разрабатывался одновременно с UNIX, поэтому критикуя UNIX, нужно покритиковать и C тоже. То, что C очень плох, написано много, я не буду повторять все эти аргументы. Там, синтаксис типов плохой, препроцессор ужасен, легко выстрелить себе в ногу, всякие 4[\"string\"], всякие sizeof ('a') != sizeof (char) (в C, не в C++!), всякие i++ + ++i, всякие while (*p++ = *q++) ; (пример из Страуструпа, второе дополненное издание) и так далее и тому подобное. Скажу лишь вот что. В C до сих пор не научились удобно работать со строками. Неудобство работы со строками постоянно приводит к разнообразным проблемам безопасности. И эту проблему до сих пор не решили! Вот относительно свежий документ от комитета C. В нём обсуждается весьма сомнительный способ решения проблемы со строками. И делается вывод, что этот способ плох. Год публикации: 2015. То есть даже к 2015-му году окончательного решения ещё нет! И это не говоря об отсутствии простой, удобной и мультиплатформенной системы сборки (а не этого монстра autotools, который ещё и не поддерживает винду, и другого монстра cmake, который поддерживает винду, но всё равно монстр), стандартного менеджера пакетов, удобного как npm (js) или carge (rust), нормальной portability library, с помощью которой можно было кроссплатформенно хотя бы прочитать содержимое папки и хотя бы даже главного сайта C, который был бы главной точкой входа для всех новичков и содержал бы в себе не только документацию, но и краткую инструкцию по установке инструментов C на любую платформу, по созданию простого проекта на C, а также содержал бы удобный поиск по пакетам C (которые должны быть размещены в стандартном репозитории) и, главное, был бы точкой сбора user community. Я даже зарегал домен c-language.org в надежде, что когда-нибудь я создам там такой сайт. Эх, мечты, мечты. (У меня ещё cpp-language.org заныкан, бугога :)) Но всего этого нет. Хоть это и есть у всех популярных языков, кроме C и C++. И даже у Haskell всё это есть. И у Rust. У Rust, у этого выскочки, который, кстати говоря, метит в ту же нишу, что и C. Есть единый конфиг, который одновременно является конфигом проекта, конфигом сборки и конфигом для менеджера пакетов (собственно, cargo — это менеджер проектов и система сборки одновременно). Есть возможность указания в качестве зависимости для данного пакета другого пакета, размещённого где-то в *GIT*, в том числе указание в качестве зависимости напрямую программы на *GITHUB*. Генерация из коробки документации из сорцов, записанной в комментах на *MARKDOWN*. И пакетный менеджер, использующий для версий *SEMVER*. Итак, *GIT*, *GITHUB*, *MARKDOWN*, *SEMVER*, короче говоря *BUZZWORDS*, *BUZZWORDS* и ещё раз *HIPSTERS' BUZZWORDS*. И всё сразу из коробки. Прямо вот заходишь на их главный сайт, и вот на тебе на блюдечке с голубой каёмочкой. И работает всё одинаково на всех платформах. Несмотря на то, что Rust — это вроде как язык системного программирования, а не какой-нибудь там javascript. Несмотря на то, что в Rust можно байты гонять. И арифметика указателей там есть. Так почему же у них, у этих выскочек-растовцев, эти хипстерские баззворды есть, а у нас, сишников, их нет? Обыдно. Я помню, один знакомый спрашивает у меня, где посмотреть список пакетов для C/C++. Пришлось сказать ему, что такого единого места нет. Он: «Программисты на C/C++ должны страдать?» Мне нечего было ему ответить. Ах да, забыл ещё одну вещь. Посмотрите, пожалуйста, на прототип функции signal в том виде, в котором он дан в стандарте C: void (*signal(int sig, void (*func)(int)))(int); и попытайтесь его понять. Терминал в UNIX — жуткое legacy. Имена файлов в файловых системах UNIX (ext2 и пр.) есть просто поток байтов без кодировки. В какой кодировке они будут интерпретированы, зависит от локали. То есть если создать файл на ОС в одной локали, а потом пытаться посмотреть его имя в ОС в другой локали, будет плохо. В виндовом NTFS такой проблемы нет. UNIX shell хуже PHP! Да, да, а вы что, не знали? Сейчас модно ругать PHP. Но ведь UNIX shell ещё хуже :) Особенно плохим он становиться, если пытаться на нём программировать, ведь полноценным языком программирования он не является. Но даже для своей ниши (скриптинг типичных задач по администрированию) он годится плохо. Виной тому примитивность shell, непродуманность, legacy, куча частных случаев, костылей, бардак с кавычками, бекслешами, специальными символами и повёрнутость shell'а (как и всего UNIX) на простом тексте. Начнём с затравки. Как рекурсивно найти в папке foo все файлы с именем \\? Правильный ответ таков: find foo -name '\\\\'. Ну или так: find foo -name \\\\\\\\. Последний вариант вызовет особенно много вопросов. Попробуйте объяснить человеку, плохо разбираемущемуся в UNIX shell, почему здесь нужно именно четыре бекслеша, а не два и не восемь (грамотеи, подскажите, как правильно написать это предложение, пишите в личку). А написать здесь нужно четыре бекслеша, потому что UNIX shell делает backslash expanding, и find тоже его делает. Как touch'нуть все файлы в папке foo (и во вложенных)? На первый взгляд, один из способ таков: find foo | while read A; do touch $A; done. Ну, на первый взгляд. На самом деле здесь можно придумать аж 5 нюансов, которые могут испортить нам малину (и привести к проблемам с безопасностью): Имя файла может содержать бекслеш, поэтому нужно писать не read A, а read -r A. Имя файла может содержать пробел, поэтому нужно писать не touch $A, а touch \"$A\". Имя файла может не только содержать пробел, но и начинаться с пробела, поэтому нужно писать не read -r A, а IFS=\"\" read -r A. Имя файла может содержать перевод строки, поэтому вместо find foo нужно использовать find foo -print0, а вместо IFS=\"\" read -r A нужно использовать IFS=\"\" read -rd \"\" A (тут я не совсем уверен). Имя файла может начинаться с дефиса, поэтому вместо touch \"$A\" нужно писать touch -- \"$A\". Итоговый вариант выглядит так: find foo -print0 | while IFS=\"\" read -rd \"\" A; do touch -- \"$A\"; done. Круто, да? И здесь мы, кстати, не учли, что POSIX не гарантирует (я не совсем в этом уверен), что touch поддерживает опцию --. Если учитывать ещё и это, то придётся для каждого файла проверять, что он начинается с дефиса (или что не начинается со слеша) и добавлять в начало ./. Теперь вы поняли, почему скрипты configure, генерируемые autoconf'ом такие большие и трудночитаемые? Потому что этому configure нужно учитывать всю эту муть, включая совместимость с разными shell'ами. (В данном примере для демонстрации я использовал решение с пайпом и циклом. Можно было использовать решение с -exec или xargs, но это было бы не так эффектно). (Ладно, хорошо, мы знаем, что имя файла начинается с foo, поэтому оно не может начинаться с пробела или дефиса). В переменной A лежит имя файла, нужно удалить его на хосте a@a. Как это сделать? Может быть так: ssh a@a rm -- \"$A\" (как вы уже заметили, мы тут уже учли, что имя файла может содержать пробелы и начинаться с дефиса)? Ни в коем случае! ssh — это вам не chroot, не setsid, не nohup, не sudo и не какая-нибудь ещё команда, которая получает exec-команду (т. е. команду для непосредственной передачи сисвызовам семейства execve). ssh (как и su) принимает shell-команду, т. е. команду для обработки shell'ом (термины exec-команда и shell-команда — мои). ssh соединяет все аргументы в строку, передаёт строку на удалённую сторону и там выполняет shell'ом. Окей, может быть так: ssh a@a 'rm -- \"$A\"'? Нет, эта команда попытается найти переменную A на удалённой стороне. А её там нет, потому что переменные через ssh не передаются. Может, так: ssh a@a \"rm -- '$A'\"? Нет, это не сработает, если имя файла содержит одинарную кавычку. В общем, не буду вас мучать, правильный ответ таков: ssh a@a \"rm -- $(printf '%q\\n' \"$A\")\". Согласитесь, удобно? Как зайти на хост a@a, с него — на b@b, с него — на c@c, с него — на d@d, а с него удалить файл /foo? Ну, это легко: ssh a@a \"ssh b@b \\\"ssh c@c \\\\\\\"ssh d@d \\\\\\\\\\\\\\\"rm /foo\\\\\\\\\\\\\\\"\\\\\\\"\\\"\" Слишком много бекслешей, да? Ну, не нравится так, давайте чередовать одинарные и двойные кавычки, будет не так скучно: ssh a@a 'ssh b@b \"ssh c@c '\\''ssh d@d \\\"rm /foo\\\"'\\''\"' А между прочим, если бы вместо shell'а был Lisp, и там функция ssh передавала бы на удалённую сторону не строку (вот она, повёрнутось UNIX на тексте!), а уже распарсенный AST (abstract syntax tree), то такого ада бекслешей не было бы: (ssh \"a@a\" '(ssh \"b@b\" '(ssh \"c@c\" '(ssh \"d@d\" '(rm \"foo\"))))) «А? Что? Lisp? Что за Lisp?» Интересно, да? На, читайте. И другие статьи Грэма. На русском тоже можно найти. Совместим предыдущие два пункта. Имя файла лежит в переменной A. Нужно зайти на a@a, с него — на b@b, далее на c@c, d@d и удалить файл, лежащий в переменной A. Это я оставляю вам в качестве упражнения :) (Сам я не знаю, как это сделать :) Ну, может, придумаю, если подумаю). echo вроде как предназначен, чтобы печатать на экран строки. Вот только использовать его для этой цели, если строчка чуть сложнее, чем «Hello, world!», нельзя. Единственно верный способ вывести произвольную строку (скажем, из переменной A) таков: printf '%s\\n' \"$A\". Допустим, нужно направить stdout и stderr команды cmd в /dev/null. Загадка: какие из этих шести команд выполняют поставленную задачу, а какие — нет? cmd > /dev/null 2>&1 cmd 2>&1 > /dev/null { cmd > /dev/null; } 2>&1 { cmd 2>&1; } > /dev/null ( cmd > /dev/null ) 2>&1 ( cmd 2>&1 ) > /dev/null Оказывается, правильный ответ — 1-я, 4-я и 6-я выполняют, 2-я, 3-я и 5-я — не выполняют. Опять-таки, выяснение причин этого оставляется в качестве упражения :) Вообще, этот пост появился в ответ на вот этот пост. Там говорилось, мол, в винде специальная дата используется как метка драйвера от Microsoft. Вместо ввода специального аттрибута или проверки производителя. Особенностей такого рода в UNIX полно. Является ли файл скрытым, выясняется на основе наличия точки в начале файла вместо специального аттрибута. Когда я сам впервые об этом узнал (да, да, в те далёкие времена, когда я впервые поставил Ubuntu), я был шокирован. Я подумал, вот идиоты. А сейчас привык. Но если вдуматься, это жуткий костыль. Далее, shell выясняет, является ли он login shell'ом на основе дефиса, переданного первым символом в argv[0] (?!). Это abuses (ну или misuses, неправильно использует, не знаю, как по-русски сказать) argv[0]. argv[0] не для этого предназначен. Вместо какого-нибудь другого способа. Любой другой способ был бы красивее. Как угодно, любым другим аргументом, переменной окружения. В BSD sockets юзер вынужден сам менять порядок байт у номера порта. А всё потому, что когда-то давно кто-то допустил в коде ядра UNIX ошибку, не предусмотрев смену порядка байт. И в качестве временного хака исправил user space код вместо кода ядра. Так и живём. Оттуда это и в Windows перешло (вместе с файлом /etc/hosts, он же C:\\windows\\system32\\drivers\\etc\\hosts). UPD от 2017-02-12: источник. «Философия UNIX». Есть мнение, что якобы UNIX прекрасна и идеальна. Что все её основные идеи («всё есть файл», «всё есть текст» и т. д.) прекрасны и составляют так называемую прекрасную «философию UNIX». Так вот, как вы уже начали догадываться, это не совсем так. Давайте разберём эту «философию UNIX» по пунктам. Сразу скажу: я не хочу сказать, что все пункты нужно отменить, просто я указываю на их неуниверсальность. «Всё есть текст». Как мы с вами уже выяснили на примере /etc/passwd, повсеместное использование простого текста может привести к проблемам с производительностью. И вообще, авторы UNIX фактически придумали для каждого системного конфига (passwd, fstab и так далее) свой формат. Со своими правилами экранирования специальных символов. Да, а вы что думали? /etc/fstab использует пробелы и переносы строк как разделители. Но что если имена папок содержат, скажем, пробелы? На этот случай формат fstab'а предусматривает специальное экранирование имён папок. Так что любой скрипт, читающий fstab, оказывается, должен это экранирование интерпретировать. Например, с помощью специально предназначенной для этого утилиты fstab-decode (запускать от рута). Не знали, да? Идите исправляйте свои скрипты :) В результате для каждого системного конфига нужен свой парсер. И было бы гораздо проще, если бы для системных конфигов использовался вместо этого какой-нибудь JSON или XML. А может быть даже некий бинарный формат. Особенно для тех конфигов, которые постоянно читаются разными программами. И для которых, как следствие, нужна хорошая скорость чтения (а у бинарных форматов она выше). Я не закончил по поводу «всё есть текст». Стандартные утилиты выдают вывод в виде простого текста. Для каждой утилиты фактически нужен свой парсер. Часто приходится парсить вывод той или иной утилиты при помощи sed, grep, awk и т. д. У каждой утилиты свои опции для того, чтобы установить, какие именно столбцы нужно выдавать, по каким столбцам нужно сортировать вывод и т. д. Было бы лучше, если бы утилиты выдавали вывод в виде XML, JSON, некоего бинарного формата или ещё чего-нибудь. А для удобного вывода этой информации на экран и для дальнейшей работы с ней можно было бы пайпить результат в дополнительные утилиты, которые убирают те или иные столбцы, сортируют по тому или иному столбцу, выбирают нужные строки и т. д. И либо выводят результат в виде красивой таблички на экран, либо передают его куда-то дальше. И всё это универсальным способом, не зависящим от исходной утилиты, которая сгенерировала вывод. И без необходимости парсить что-либо регексами. Да, UNIX shell плохо работает с JSON и XML. Но ведь у UNIX shell полно других недостатков. Нужно выкинуть его вовсе и заменить на некий другой язык, который помимо всего прочего может удобно работать со всякими JSON. Вы только представьте! Вот допустим, нужно удалить все файлы в текущей папке с размером, большим 1 килобайта. Да, я знаю, что такое надо делать find'ом. Но давайте предположим, что это нужно сделать непременно ls'ом (и без xargs). Как это сделать? Вот так: LC_ALL=C ls -l | while read -r MODE LINKS USER GROUP SIZE M D Y FILE; do if [ \"$SIZE\" -gt 1024 ]; then rm -- \"$FILE\"; fi; done. (LC_ALL здесь нужен был, чтобы быть уверенным, что дата будет занимать именно три слова в выводе ls). Мало того, что это решение выглядит некрасиво, оно ещё страдает рядом недостатков. Во-первых, оно не будет работать, если имя файла содержит перевод строки или начинается с пробела. Далее, нам нужно явно перечислить названия всех столбцов ls, ну или как минимум помнить, на каком месте находятся интересующие нас (т. е. SIZE и FILE). Если мы ошибёмся в порядке столбцов, то ошибка выяснится лишь на этапе выполнения. Когда мы удалим не те файлы :) А как бы выглядело решение в идеальном мире, который я предлагаю? Как-то так: ls | grep 'size > 1kb' | rm. Кратко, а главное смысл виден из кода, и невозможно ошибиться. Смотрите. ls в моём мире всегда выдаёт всю инфу. Специальня опция -l для этого не нужна. Если нужно убрать все столбцы и оставить только имя файла, то это делается специальной утилитой, в которую нужно направить вывод ls. Итак, ls выдаёт список файлов. В некоем структуированном виде, скажем, JSON. Это представление «знает» названия столбцов и их типы, т. е. что это, строка, число или что-то ещё. Далее этот вывод направляется в grep, который в моём мире выбирает нужные строки из этого JSON. JSON «знает» названия полей, поэтому grep «понимает», что здесь означает «size». Более того, JSON содержит инфу о типе поля size. Он содержит инфу о том, что это число, и даже что это не просто число, а размер файла. Поэтому можно сравнить его с 1kb. Далее grep направляет вывод в rm. rm «видит», что он получил файлы. Да, да, JSON ещё и хранит инфу о типе этих строк, о том, что это — файлы. И rm их удаляет. А ещё JSON отвечает за правильное экранирование специальных символов. Поэтому файлы со спецсимволами «просто работают». Круто? Идею я взял отсюда (там ещё есть ссылка на более подробный английский оригинал), посмотрите. Ещё замечу, что в Windows Powershell реализовано как раз что-то похожее на эту идею. UNIX shell. Ещё одна базовая идея UNIX. Причём о мелких недостатках UNIX shell я уже поговорил в первой части статьи. Сейчас будут крупные. В чём «крутость» UNIX shell? В том, что на момент своего появления (это было очень давно) UNIX shell был гораздо мощнее командных интерпретаторов, встроенных в другие ОС. И позволял писать более мощные скрипты. Да и вообще, на момент своего появления UNIX shell был, видимо, самым мощным из скриптовых языков вообще. Потому что нормальных скриптовых языков, т. е. таких, которые бы позволяли полноценное программирование, а не только скриптинг, тогда, видимо, вообще не существовало. Это потом уже в один прекрасный день один программист по имени Larry Wall заметил, что UNIX shell всё-таки недостаёт до нормального языка программирования. И он захотел соединить краткость UNIX shell'а с возможностью полноценного программирования из C. И создал Perl. Да, Perl и другие последующие скриптовые языки программирования фактически заменили UNIX shell. Это константирует даже Роб Пайк, один из авторов (как я считаю) той самой «философии UNIX» (про него мы ещё поговорим). Вот здесь на вопрос об «одной утилите для одной вещи» он сказал: «Those days are dead and gone and the eulogy was delivered by Perl». Причём я считаю, что эта его фраза относилась к типичному использованию UNIX shell, т. е. к ситуации связывания большого количества маленьких утилит в shell-скрипте. Нет, говорит Пайк, просто используйте Perl. Я не закончил про UNIX shell. Рассмотрим ещё раз пример кода на shell, который я уже приводил: find foo -print0 | while IFS=\"\" read -rd \"\" A; do touch -- \"$A\"; done. Здесь в цикле вызывается touch (да, я знаю, что этот код можно переписать на xargs, причём так, чтобы touch вызывался только один раз; но давайте пока забьём на это, хорошо?). В цикле вызывается touch! То есть для каждого файла будет запущен новый процесс! Это нереально неэффективно. Код на любом другом языке программирования будет работать быстрее этого. Просто на момент появления UNIX shell он был одним из немногих языков, которые позволяют написать это действие в одну строчку. Короче говоря, вместо UNIX shell нужно использовать любой другой скриптовый язык программирования. Который подходит не только для скриптинга, но и для реального программирования. Который не запускает новый процесс каждый раз, когда нужно «touch'нуть» файл. Возможно, понадобится «доложить» в этот скриптовый язык средства для простого выполнения вещей, которые есть в shell, скажем, для создания пайпов. Простота. Здесь я говорю не конкретно про shell и про связывание кучи простых утилит из shell'а (про это был предыдущий пункт), а про простоту вообще. Использование простых инструментов. Скажем, редактирование картинки sed'ом. Да, да. Конвертим jpg в ppm при помощи командной строки. Затем при помощи графического редактора, grep, sed и такой-то матери редактируем картинку. А потом обратно в jpg. Да, так можно. Но часто photoshop'ом или gimp'ом всё-таки лучше. Хоть это и большие, интегрированные программы. Не в стиле UNIX. На этом я закончу эти пункты. Да, хватит. Есть идеи в UNIX, которые мне реально нравятся. Скажем, «программа должна делать одну вещь и делать её хорошо». Но не в контексте shell. Вы уже поняли, что я не люблю shell. (Ещё раз повторю, я считаю, что в приведённом выше интервью Пайка он воспринял принцип «программа должна делать одну вещь и делать её хорошо» именно в контексте shell и потому отверг его). Нет, я говорю про этот принцип в своей сути. Скажем, консольный почтовый клиент не должен иметь встроенный текстовый редактор, он должен просто запустить некий внешний редактор. Или вот принцип, по которому нужно писать консольное ядро для программы и потом графическую оболочку для этого ядра. Теперь общая картина. Однажды появился UNIX. На момент появления он был прорывом. И он был во многом лучше своих конкурентов. UNIX имел много идей. И, как и любая ОС, UNIX требовал от программистов соблюдения некоторых принципов при написании прикладных программ. Идеи, лежащие в основе UNIX, стали называться «философией UNIX». Одним из тех людей, которые сформулировали философию UNIX, был уже упомянутый Роб Пайк. Он это сделал в своей презентации «UNIX Style, or cat -v Considered Harmful». После презентации он вместе с Керниганом опубликовал статью по мотивам презентации. В ней авторы рассказали о том, что, скажем, предназначение cat — это только конкатенация и ничего больше (ну то есть «склеивание» файлов, мы с вами помним, как расшифровывается cat, так ведь?). Возможно, что это Пайк как раз и придумал «философию UNIX». В честь этой презентации был назван сайт cat-v.org, почитайте его, очень интересный сайт. Но потом, через много лет, этот же Пайк сделал ещё две презентации, в которых, как я считаю, отменил свою философию обратно. Поняли, фанатики, да? Ваш кумир отказался от своей же философии. Можете расходиться по домам. В первой презентации «Systems Software Research is Irrelevant» Пайк сетует на то, что никто больше не пишет новых ОС. А даже если и пишут, то просто ещё один UNIX (который подразумевается в этой презентации уже чем-то неинтересным): «New operating systems today tend to be just ways of reimplementing Unix. If they have a novel architecture — and some do — the first thing to build is the Unix emulation layer. How can operating systems research be relevant when the resulting operating systems are all indistinguishable?» Вторую презентацию Пайк прямо называет: «The Good, the Bad, and the Ugly: The Unix Legacy». Пайк говорит, что простой текст не универсален, он хорош, но работает не всегда: «What makes the system good at what it's good at is also what makes it bad at what it's bad at. Its strengths are also its weaknesses. A simple example: flat text files. Amazing expressive power, huge convenience, but serious problems in pushing past a prototype level of performance or packaging. Compare the famous spell pipeline with an interactive spell-checker». Далее: «C hasn't changed much since the 1970s… And — let's face it — it's ugly». Дальше Пайк признаёт ограниченность пайпов, соединяющих простые утилиты, ограниченность регексов. UNIX был гениальным на момент своего появления. Особенно, если учесть, какие инструменты были в распоряжении у авторов UNIX. У них не было уже готового UNIX, чтобы на нём можно было разрабатывать UNIX. У них не было IDE. И программировали они вообще на ассемблере изначально. У них, видимо, был только ассемблер и текстовый редактор. Люди, стоящие у истоков UNIX, в определённый момент начали писать новую ОС: Plan 9. В том числе упомянутые Томпсон, Ритчи и Пайк. Учитывая многие ошибки UNIX. Но и Plan 9 никто не возводит в абсолют. В «Systems Software Research is Irrelevant» Пайк упоминает Plan 9, но несмотря на это всё равно призывает писать новые ОС. James Hague, ветеран программирования (занимается программированием с восьмидесятых) пишет: «What I was trying to get across is that if you romanticize Unix, if you view it as a thing of perfection, then you lose your ability to imagine better alternatives and become blind to potentially dramatic shifts in thinking» (ссылка). Прочитайте эту статью и его же статью «Free Your Technical Aesthetic from the 1970s», на которую он ссылается. (Вообще, если вам понравилась моя статья, то и его блог тоже, наверное, понравится, погуляйте там по ссылкам). Итак, я не хочу сказать, что UNIX — плохая система. Просто обращаю ваше внимание на то, что у неё есть полно недостатков, как и у других систем. И «философию UNIX» я не отменяю, просто обращаю внимание, что она не абсолют. Мой текст обращён скорее к фанатикам UNIX и GNU/Linux. Провокационный тон просто чтобы привлечь ваше внимание. UPD от 2017-02-14: комментаторы указывают, что сравнивать UNIX shell с PHP некорректно. Конечно, некорректно! Потому что UNIX shell не претендует на то, чтобы быть полноценным языком программирования, он предназначен для скриптинга системы. Вот только я в одно время этого не знал. И вдобавок считал UNIX shell прекрасным. Вот для людей в таком же положении я всё это и говорю. Ещё как минимум один комментатор говорит, что сравнивать UNIX shell нужно с cmd. Я бы сказал, что сравнивать надо с Windows Powershell. Последний, как я уже говорил, в чём-то превосходит UNIX shell. UPD от 2017-02-14: мне понравился вот этот коммент от sshikov: Но я скажу за автора — к сожалению, прямо сегодня можно найти сколько угодно восторженных статей типа «А вот есть такая замечательная фигня, как bash, щас я вам про нее расскажу...» — где unix way откровенно перехваливается неофитами. Это не помешает иногда компенсировать долей скепсиса. Да, в этом-то и всё дело! Достало, что хвалят UNIX way. Что считают UNIX красивым и ещё и других учат. А использовать-то UNIX можно. UPD от 2017-02-14: как минимум один комментатор сказал, что пересел с Windows на UNIX-подобные ОС и счастилив. Что поначалу он плевался от UNIX, но потом решил, что программировать под UNIX гораздо проще, чем под Windows. Так вот, я тоже сперва использовал и программировал на Windows. Потом пересел на UNIX. И сперва, конечно, было очень непривычно. Потом прочувствовал «философию UNIX», ощутил всю её мощь. Программировать под UNIX стало легко. Но позже пришло ещё одно озарение. Что UNIX неидеальна, а «философия UNIX» неабсолютна. Что программирование на «голом UNIX», с использованием C и Shell сильно уступает, скажем, Web-программированию. И далеко не только потому, что в Web-программировании используются языки, в которых трудно выстрелить себе в ногу, в отличие от C (тут языку C предъявить нечего, он намеренно является низкоуровневым). Но ещё и из-за всех этих quirks мейкфайлов, шела, языка C. Отсутствия удобных инструментов, систем сборки, менеджеров пакетов. Всё это, в принципе, можно было бы исправить. Вот я написал эту статью, чтобы открыть на это глаза тем, кто об этом не знает. У Windows тоже полно недостатков (я разве где-то говорил, что Windows лучше UNIX?). Но в чём-то Windows лучше UNIX (как минимум в некоторых особенностях Powershell). Сейчас я продолжаю использовать и программировать под UNIX. UNIX меня устраивает, мне достаточно удобно, хотя теперь уже я вижу многие его недостатки. Я не призываю бросать UNIX. Используйте UNIX дальше, просто не считайте его идеалом. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10070776. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10071096. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10071714. UPD от 2017-02-16: понравился этот коммент: habrahabr.ru/post/321652/#comment_10066240. UPD от 2017-02-16: многие комментаторы рассказывают, как же полезны и удобны UNIX системы. Что они есть уже десятки лет, на них работает весь интернет. Что они стабильны и прекрасно справляются с возложенными на них задачами. И даже удалённо переустановить GNU/Linux можно :) А я и не спорю. Я не призываю отказываться от UNIX. Я просто хочу, чтобы вы видели недостатки UNIX. UNIX работает, используйте его. Процитирую James Hague, на которого я уже ссылался: Enough time has passed since the silly days of crazed Linux advocacy that I'm comfortable pointing out the three reasons Unix makes sense: 1. It works. 2. It's reliable. 3. It stays constant. But don't--do not--ever, make the mistake of those benefits being a reason to use Unix as a basis for your technical or design aesthetic. Yes, there are some textbook cases where pipelining commands together is impressive, but that's a minor point. Yes, having a small tool for a specific job sometimes works, but it just as often doesn't. Одно время я тоже, как и многие из вас, повёлся на эту «философию UNIX». Думал, что она прекрасна. А потом понял, что это не так. И вот этим своим открытием я хочу с вами поделиться. Мои мысли не новы. Они уже есть в приведённых мною ссылках. Я просто хочу сообщить эти мысли аудитории Хабра. Мой пост написан наскоро, ночью. Читайте скорее не его, а ссылки, которые я привожу. В первую очередь две презентации Пайка, в которых он «отменяет философию UNIX» и два поста от James Hague. Мой пост фактически написан, чтобы привлечь внимание к этим ссылкам. Как минимум один из комментаторов сказал, что многие из названных мной «недостатков» UNIX недостатками не являются. Например, слишком короткие имена команд. Ну да. Это не недостаток. Но это пример необдуманного решения. Сиюминутного решения, принятого под влиянием обстоятельств, имевших важность тогда. Как и с тем примером с /usr или make. Я показываю, что UNIX была непродумана. Да и вообще, вглядитесь в историю UNIX! Сотрудникам Bell Labs не понравилась сложность проекта Multics. Они сказали: «Да ну этот Multics, давайте по-быстрому напишем свою ОС, запростецкую». И написали. Понимаете? ОС получилась довольно хорошей. Но не идеальной. UNIX — это хак. Успешный хак, который выполнил свою миссию и продолжает её выполнять. В комментариях была мысль, что заголовок поста не соответствует содержанию, и что я критикую не самую суть, философию UNIX, а просто привожу некий список недостатков. Возможно даже не всего класса UNIX-подобных систем, а конкретных реализаций. Так вот, это не так. Да, статья начинается с перечисления мелких недостатков. Этим я обращаю внимание на то, что в UNIX полно костылей, как и в других системах. В том числе очень старых, оставшихся во всех UNIX системах и попавших во все стандарты. Но я критикую и саму философию UNIX. Основные принципы (но не все!). Язык C, UNIX shell, идею конвееров, «всё есть текст». Замечу, что компилятор C и make, хоть и являются по идее отдельными программами, всегда рассматриваются как неотъемлемая часть экосистемы UNIX. И входят в POSIX. Некоторые комментаторы пишут: «А я сижу в IDE и не использую этот ваш make». Ну окей, хорошо, мой пост предназначен скорее как раз для тех фанатиков, которые считают, что всякие IDE — это не труъ и что программировать нужно непременно используя голый C, make и shell. И я не говорю, что философия UNIX (даже в тех местах, которые мне не нравятся) всегда не верна. Часто конвееры и shell-скрипты — это именно то, что нужно. Но не всегда. Некоторые комментаторы указывают, что голый shell, make и прочее часто скрыты от глаз юзера всякими обёртками, всякими IDE, сложными системами сборки, GUI-интерфейсами и пр. Ну да. Так ведь это и есть признак кривости системы :) Когда что-то уродское покрывают слоем красоты. А ещё абстракции протекают. А потому использовать, скажем, autotools ещё сложнее, чем голый make. Потому что чтобы использовать autotools, нужно знать ещё и m4, make и shell. Да, да, всю эту цепочку языков, используемых при генерации окончательного мейкфайла. Один комментатор приводит следующие принципы UNIX: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. С первыми двумя я согласен при условии, что они понимаются в отрыве от UNIX shell и конвееров. Их можно перенести даже на новомодные микросервисы, общающиеся с помощью REST. С третьим я не согласен (как я понимаю, подразумевается именно придумываение простого кастомного текстового формата для каждого случая вместо единого формата наподобие JSON). Часто текст — это именно то, что нужно. Но пихать его везде как universal interface глупо. На эту роль скорее претендует JSON или XML. Или, может, какой-нибудь формат для структуированных данных, который ещё не изобрели. Многие указали на искусственность некоторых примеров на shell. Ну да, я знаю, что их можно было бы переписать на find -exec или xargs. Ну что вы хотите, наскоро написанная статья. Можно было привести примеры получше, просто мне не хотелось. Это не отменяет того, что в shell'е постоянно возникают проблемы со специальными символами. Которые нужно по-особому обходить. И вообще у shell'а полно quirks, которые нужно постоянно держать в голове. И он запускает новые программы на каждый чих. Я вам ещё покушать принёс. Вот вам цитата от безусловно ещё одного вашего кумира Линуса Торвальдса: iTWire: Systemd seems to depart to a large extent from the original idea of simplicity that was a hallmark of UNIX systems. Would you agree? And is this a good or a bad thing? Linus Torvalds: So I think many of the «original ideals» of UNIX are these days more of a mindset issue than necessarily reflecting reality of the situation. There's still value in understanding the traditional UNIX «do one thing and do it well» model where many workflows can be done as a pipeline of simple tools each adding their own value, but let's face it, it's not how complex systems really work, and it's not how major applications have been working or been designed for a long time. It's a useful simplification, and it's still true at *some* level, but I think it's also clear that it doesn't really describe most of reality. It might describe some particular case, though, and I do think it's a useful teaching tool. People obviously still do those traditional pipelines of processes and file descriptors that UNIX is perhaps associated with, but there's a *lot* of cases where you have big complex unified systems. And systemd is in no way the piece that breaks with old UNIX legacy. Graphical applications seldom worked that way (there are certainly _echoes_ of it in things like «LyX», but I think it's the exception rather than the rule), and then there's obviously the traditional counter-example of GNU emacs, where it really was not about the «simple UNIX model», but a whole new big infrastructure thing. Like systemd. Now, I'm still old-fashioned enough that I like my log-files in text, not binary, so I think sometimes systemd hasn't necessarily had the best of taste, but hey, details… UPD от 2017-02-18: ещё по поводу надуманных примеров на shell. Вы говорите, примеры надуманные, что можно сделать find -exec или xargs. Да, можно. Но как минимум сам факт того, что нужно постоянно держать в голове, что, мол, цикл нельзя и нужен -exec и xargs — это уже костыль. Проистекающий из принципа «всё есть текст», ну или из слишком тупой реализации этого принципа в UNIX shell. Итак, сейчас я приведу такую задачу, в которой любое решение будет уродским, даже с использованием find -exec и xargs. Вернёмся к моему примеру с touch'ем. «Как touch'нуть все файлы в папке foo (и во вложенных)?» Допустим, что нужно не touch'нуть их, а grep'нуть из них все строки со словом bar и положить результат туда же. Т. е. для каждого файла file сделать grep bar file > tmp; mv tmp file. Как быть? Если делать решение с циклом, то мы упираемся в те пять хаков, которые нужно сделать, чтобы не выстрелить себе в ногу. Результат будет таким, со всеми пятью хаками: find foo -print0 | while IFS=\"\" read -rd \"\" A; do grep -- bar \"$A\" > tmp mv -- tmp \"$A\" done Ладно, хорошо, мы знаем, что имя файла начинается на foo, а потому не может начинаться с дефиса и пробела. Но оно может заканчиваться на пробел, а потому тот трюк с IFS всё равно нужен. Так что единственный хак, от которого можно избавиться, зная, что имя начинается с foo — это написание --. Но даже от этого хака я бы не советовал избавляться, т. к. постоянное использование -- даёт понять читающему: «Да, я подумал об этом». Это как условия Йоды. Окей, можно ли этот пример написать проще с использованием xargs или find -exec? Если бы каждый файл нужно было всего лишь touch'нуть, то да, можно было бы написать существенно проще. Но если нужно выполнить два действия: grep и переименование, то существенного упрощения мы уже не получим. Два действия означают, что нам уже нужно запихивать эти два действия в вызов shell'а, в sh -c. Как будет выглядеть результат? Может быть, так? find foo -exec sh -c \"grep -- bar '{}' > tmp; mv -- tmp '{}'\" ';' Нет, неправильно! Это не будет работать, если имя содержит одинарную кавычку. Правильный вариант таков: find foo -exec sh -c 'grep -- bar \"$1\" > tmp; mv -- tmp \"$1\"' dummy '{}' ';' Видите? Опять хак. Нам пришлось передать имя файла через $1. И по-прежнему нужно помнить, что нам нужны двойные кавычки вокруг $1. То же самое было бы с xargs. Опять нужен sh -c и опять нужно передавать аргументы через $1. Всё это сделать можно, если надо, но сам факт того, что нужно постоянно держать это в голове и обходить грабли, говорит о том, что здесь что-то не то. Теперь по поводу другого примера. Где нужно удалить все файлы определённого размера. Да, всё это можно сделать одним вызовом find. Там есть опции и для проверки размера, и для удаления. Да. Вот только я вижу здесь хак. Хак в том, что find имеет фактически в себе целый sublanguage, подъязык. Язык вот этих вот опций. Почитайте хорошенько ман find'а. Вы узнаете, что, оказывается, порядок опций find'а имеет значение. Что каждая опция имеет truth value, т. е. булевское значение. Что можно по-хитрому комбинировать эти опции. Что в зависимости от порядка опции, от их truth value find принимает решение, в какой момент нужно остановить обработку опций для данного файла и нужно ли descend в данный каталог (т. е. нужно ли искать внутри этого этого каталога). Я помню, как однажды жутко оплошался, не зная этих тонкостей. Я набрал find -delete -name '*~' вместо find -name '*~' -delete или что-то такое. Ну подумаешь, думал я, опции не в том порядке. Смысл же тот же. И find удалил всё. Снёс мои важные файлы. Потом я восстановил из бекапа, так что всё ок. Это потом уже я понял, что -name имеет truth value true в случае, если файл соответствует маске. И если -name вернул true, то обработка опций продолжается. Что тут плохого? Плохо то, что find имеет свой sublanguage. Что это ещё один язык в дополнение к shell. (А sed, кстати говоря — это ещё один язык, а awk — это ещё один язык и так далее, авторы UNIX'а любили создавать по языку на каждый чих.) Нужно было вместо этого сделать так, чтобы find только умел искать файлы. А всю остальную функциональность нужно вынести из него. Проверки на размер файла должны быть снаружи. А если find'у нужно принять решение, нужно ли descend в данный каталог, то он должен вызывать внешний callback. Да, в UNIX shell так вряд ли получится. На то он и UNIX shell."], "hab": ["Разработка под Linux", "Open source", "C"]}{"url": "https://habrahabr.ru/post/189046/", "title": ["GitHub Flow: рабочий процесс Гитхаба", "перевод"], "text": ["Краткое предисловие переводчика.Захватывающе интересная статья одного из разработчиков «GitHub Inc.» о принятом в компании рабочем процессе потребовала употребить пару специальных терминов при переводе. То понятие, для которого на английском языке достаточно одного слóва «workflow», на русский приходится переводить словосочетанием — «рабочий процесс». Ничего лучше не знаю ни сам я, ни при помощи гуглоперевода — так что и мне, и читателям придётся с этим мириться, хотя бы и поневоле. Другое понятие, «deploy», на русский часто переводят словом «развёртывание», но в моём переводе я решил вспомнить оборот из советского делопроизводства — «внедрение инноваций на производстве» — и стану говорить именно о «внедрении» новых фич. Дело в том, что описанный ниже рабочий процесс не имеет «выпусков» (releases), что делает несколько неудобными и речи о каком-либо «развёртывании» их. К сожалению, некоторые переводчики бывают склонны грубо убивать сочную метафору «иньекции» (или даже «впрыскивания», если угодно), содержающуюся в термине «code injection», так что и его также переводят словосочетанием «внедрение кода». Эта путаница огорчает меня, но ничего не могу поделать. Просто имейте в виду, что здесь «внедрением кода» я стану назвать внедрение его именно в производство (на продакшен), а не в чей-нибудь чужой код. Я стремился употреблять словосочетание «в Гитхабе» в значении «в компании GitHub Inc.», а «на Гитхабе» — в значении «на сайте GitHub.com». Правда, иногда разделять их сложновато. Проблемы git-flow Повсюду путешествую, преподавая Git людям — и почти на каждом уроке и семинаре, недавно мною проведённом, меня спрашивали, что я думаю о git-flow. Я всегда отвечал, что думаю, что этот подход великолепен — он взял систему (Git), для которой могут существовать мириады возможных рабочих процессов, и задокументировал один проверенный и гибкий процесс, который для многих разработчиков годится при довольно простом употреблении. Подход этот также становится чем-то вроде стандарта, так что разработчики могут переходить от проекта к проекту и из компании в компанию, оставаясь знакомыми с этим стандартизированным рабочим процессом. Однако и у git-flow есть проблемы. Я не раз слыхал мнения людей, выражавших неприязнь к тому, что ветви фич отходят от develop вместо master, или к манере обращения с хотфиксами, но эти проблемы сравнительно невелики. Для меня одной из более крупных проблем git-flow стала его сложность — бóльшая, чем на самом деле требуется большинству разработчиков и рабочих групп. Его сложность ужé привела к появлению скрипта-помощника для поддержания рабочего процесса. Само по себе это круто, но проблема в том, что помощник работает не из GUI Git, а из командной строки, и получается, что те самые люди, которым необходимо действительно хорошо выучить сложный рабочий процесс, потому что им вручную придётся пройти все шаги его — для этих-то людей система и недостаточно удобна для того, чтобы использовать её из командной строки. Вот что становится крупною проблемою. Все эти проблемы можно без труда преодолеть, следуя гораздо более простому рабочему процессу. Мы не пользуемся git-flow в Гитхабе. Наш рабочий процесс основан (и всегда был основан) на более простом подходе к Git. Простота его имеет несколько достоинств. Во-первых, людям проще понять его, так что они быстрее начинают использовать его, реже (или вовсе никогда не) допускают ошибки, требующие отката. Кроме того, не требуется скрипт-обёртка, помогающий следовать процессу, так что употребление GUI (и т. п.) не создаёт проблем. Рабочий процесс Гитхаба Итак, почему мы в Гитхабе не используем git-flow? Главная проблема в том, что у нас принято беспрестанное внедрение изменений. Рабочий процесс git-flow создавался в основном в помощь «выпускам» нового кода. А у нас нет «выпусков», потому что код поступает на продакшен (основной рабочий сервер) ежедневно — иногда по нескольку раз в день. Мы можем подавать для этого команды боту в той же чат-комнате, в которой отображаются итоги CI (интеграционного тестирования). Мы стремимся сделать процесс тестирования кода и его внедрения как можно проще, чтобы каждому сотруднику он был удобен в работе. У такого частого внедрения новинок есть ряд достоинств. Если оно случается каждые несколько часов, то почти невозможно возникнуть большому количеству крупных багов. Небольшие недочёты случаются, но они могут быть исправлены (а исправления, в свою очередь, внедрены) очень быстро. Обычно пришлось бы делать «хотфикс» или как-то ещё отступать от нормального процесса, но для нас это становится просто частью нормального процесса: в рабочем процессе Гитхаба нет разницы между хотфиксом и небольшою фичею. Другим достоинством беспрерывного внедрения изменений становится возможность быстро отреагивать на проблемы любого рода. Мы можем откликаться на сообщения о проблемах с безопасностью или исполнять небольшие (но интересные) просьбы о новых фичах — но тот же самый процесс работает и при внесении изменений, связанных с разработкою фичи нормального (или даже крупного) размера. Процесс один и тот же и он очень прост. Как мы это делаем Итак, каков рабочий процесс Гитхаба? Содержимое ветви master всегда работоспособно (deployable).   Начиная работу над чем-то новым, ответвляйте от ветви master новую ветвь, имя которой соответствует её предназначению (например, «new-oauth2-scopes»).   Закоммитив в эту ветвь локально, отправляйте регулярно свою работу и в одноимённую ветвь на сервере.   Когда вам понадобится отзыв, или помощь, или когда вы сочтёте ветвь готовою ко слиянию, отправьте запрос на слияние.   После того, как кто-то другой просмотрел и одобрил фичу, вы можете слить вашу ветвь в ветвь master.   После того, как ветвь master пополнилась новым кодом, вы можете немедленно внедрить его на продакшен и вам следует сделать это. Вот и весь рабочий процесс. Он очень прост и результативен, он работает для довольно крупных рабочих групп — в Гитхабе сейчас работает 35 человек, из которых, может быть, пятнадцать или двадцать одновременно работают над одним и тем же проектом (github.com). Думаю, что большинство команд разработчиков (групп, одновременно работающих с логикою одного и того же кода, что может порождать конфликты) имеют такой же размер — или меньше такого. Особенно группы, достаточно прогрессивные для того, чтобы заниматься быстрым и последовательным внедрением. Итак, давайте по порядку рассмотрим каждый шаг. Содержимое ветви master всегда работоспособно (deployable) В общем-то это единственное жёсткое правило во всей системе. Есть только одна ветвь, имеющая всегда некоторое особенное значение, и её мы назвали master. Для нас это значит, что код этой ветви либо внедрён на продакшен, либо, в худшем случае, окажется внедрён в течение нескольких часов. Эта ветвь очень редко подвергается откручиванию на несколько коммитов назад (для отмены работы): если возникает проблема, то отменяются изменения из коммитов или совершенно новые коммиты исправляют проблему, но сама ветвь почти никогда назад не откручивается. Ветвь master стабильна. Внедрение её кода на продакшен или создание новых ветвей на её основе — это всегда, всегда безопасно. Если от вас в master поступает неоттестированный код или он ломает сборку, то вы нарушили «общественный договор» команды разработчиков и у вас на душé должны кошки поскрести по этому поводу. Каждая ветвь подвергается у нас тестированию, а итоги поступают в чат-комнату — так что, если вы не тестировали её локально, то можете запушить ветвь (даже с единственным коммитом) на сервер и подождать, пока Jenkins не сообщит, все ли тесты успешно пройдены. Ответвляйте от ветви master новые ветви, имена которых соответствуют предназначению Когда хотите поработать над чем-то новым, ответвляйте от стабильной ветви master новую ветвь, имя которой соответствует предназначению. (Например, в коде Гитхаба прямо сейчас есть ветви «user-content-cache-key», «submodules-init-task», «redis2-transition».) Такое наименование имеет несколько достоинств. Например, достаточно подать команду fetch, чтобы увидеть, над какими темами работают остальные. Кроме того, оставив ветвь на какое-то время и возвратившись к ней позднее, по имени проще припомнить, о чём она была. И это приятно, потому что, когда на Гитхабе заходим на страницу со списком ветвей, то легко видеть, над какими ветвями недавно поработали (и, приблизительно, каков был объём работы). Это почти как список будущих фич с грубою оценкою нынешнего состояния их. Если не пользуетесь этой страницею, то знайте — у ней классные возможности: она вам показывает только те ветви, в которых была проделана работа, уникальная по отношению к выбранной вами в настоящий момент ветви, да ещё и сортирует таким способом, чтобы ветви с наиболее недавнею работою были сверху. Если захочется полюбопытствовать, то я могу нажать на кнопку «Compare» и поглядеть на точный объединённый diff и на список коммитов, уникальных для этой ветви. Сейчас, когда я это пишу, у нас в репозитории 44 ветви с невоссоединённым кодом, но также видно, что из них только в девять или в десять поступал код за последнюю неделю. Постоянно отправляйте код именованных ветвей на сервер Другое крупное отличие от git-flow: мы беспрерывно делаем push ветвей на сервер. С точки зрения внедрения приходится по-настоящему беспокоиться только о ветви master, так что push никого не озадачит и ничего не поломает: всё, что не master — это просто код, над которым идёт работа. Этим создаётся страховочная копия на случай утраты ноутбука или выхода жёсткого диска из строя. Этим поддерживается, что ещё важнее, постоянный обмен сведениями между разработчиками. Простой командою «git fetch» можно получить список тех TODO, над которыми все сейчас работают. $ git fetch remote: Counting objects: 3032, done. remote: Compressing objects: 100% (947/947), done. remote: Total 2672 (delta 1993), reused 2328 (delta 1689) Receiving objects: 100% (2672/2672), 16.45 MiB | 1.04 MiB/s, done. Resolving deltas: 100% (1993/1993), completed with 213 local objects. From github.com:github/github * [new branch] charlock-linguist -> origin/charlock-linguist * [new branch] enterprise-non-config -> origin/enterprise-non-config * [new branch] fi-signup -> origin/fi-signup 2647a42..4d6d2c2 git-http-server -> origin/git-http-server * [new branch] knyle-style-commits -> origin/knyle-style-commits 157d2b0..d33e00d master -> origin/master * [new branch] menu-behavior-act-i -> origin/menu-behavior-act-i ea1c5e2..dfd315a no-inline-js-config -> origin/no-inline-js-config * [new branch] svg-tests -> origin/svg-tests 87bb870..9da23f3 view-modes -> origin/view-modes * [new branch] wild-renaming -> origin/wild-renaming Также это позволяет всем видеть (на гитхабовской странице списка ветвей), над чем работают все остальные — можно проанализировать код и решить, не желаешь ли в чём-нибудь помочь разработчику. В любое время создавайте запрос на слияние GitHub снабжён поразительною системою обзора кода, которая называется запросами на слияние; боюсь, недостаточно разработчиков вполне знают о ней. Многие пользуются ею в обыкновенной работе над открытым исходным кодом: форкнул проект, обновил код, отправил запрос на слияние к хозяину проекта. Однако эта система также может употребляться как средство внутрикорпоративной проверки кода, и так её используем мы. Её мы, собственно, скорее используем как средство просмотра и обсуждения ветвей, чем как запрос на слияние. GitHub поддерживает отсылку запроса на слияние из одной ветви в другую в одном и том же проекте (открытом или приватном), так что в запросе можно сказать «мне нужна подсказка или обзор этого кода», а не только «прошу принять этот код». На этой иллюстрации вы можете видеть, как Джош просит Брайана взглянуть на код, и тот является с советом по поводу одной из строк кода. Ниже можно видеть, как Джош соглашается с соображениями Брайана и пополняет код, чтобы отреагировать на них. Можно, наконец, видеть и то, что код находится ещё на стадии испытаний: это ещё не ветвь, подготовленная ко внедрению, мы используем запросы на слияние для рассмотрения кода задолго до того, как захотим действительно слить его в master и отправить на внедрение. Если ваша работа над фичею или ветвью застревает и нужна помощь или совет, или если вы — разработчик, а на вашу работу надо бы посмотреть и дизайнеру (или наоборот), или даже если кода у вас мало (или вовсе нет), но есть какая-то композиция скриншотов и общих идей, то вы открываете запрос на слияние. Система Гитхаба позволяет добавлять людей к обсуждению @-упоминанием их, так что если обзор или отклик нужен от конкретного человека, то можно в запросе упомянуть его (вы видели выше, как Джош сделал это). И это круто, потому что в запросах на слияние можно комментировать отдельные строки объединённого диффа, или отдельные коммиты, или весь запрос в целом — и копии реплик сложатся в единое обсуждение. Также можно продолжать пополнение ветви кодом, так что если кто-нибудь укажет на ошибку или на позабытую возможность в коде, то можно поместить исправление в ту же ветвь, и GitHub покажет новые коммиты в обсуждении, так что и вот так можно трудиться над ветвью. Если ветвь существует слишком долго и вы ощущаете, что код в ней рассогласовывается с кодом ветви master, то можно код из master влить в вашу ветвь и продолжить работу. В обсуждении запроса на слияние или в списке коммитов без труда видно, когда ветвь последний раз обновляли кодом, взятым из master. Когда работа над ветвью целиком и полностью окончена и вы ощущаете её готовою ко внедрению, тогда можете переходить к следующему шагу. Слияние только после обзора запроса Мы не работаем непосредственно в ветви master, но и работу из именованной ветви мы не подвергаем слиянию сразу после того, как сочтём её оконченною — сперва мы стараемся получить одобрение от других сотрудников компании. Оно обычно имеет форму «+1», или эмоджи, или комментария «:shipit:», но кого-то ещё нам надо привести поглядеть на ветвь. Когда одобрение получено, и ветвь прошла CI, мы можем слить её в master и на внедрение; в этот момент запрос на слияние будет закрыт автоматически. Внедрение непосредственно после обзора Наконец, ваш труд окончен, а плоды его — на ветви master. Это означает, что, даже если прямо сейчас вы не станете внедрять их, то они всё равно станут основою для ветвей других сотрудников, и что следующее внедрение (которое, скорее всего, случится через несколько часов) запустит новинку в дело. И так как бывает очень неприятно обнаружить, что кто-то ещё запустил ваш код и пострадал от этого (если код что-то поломал), то людям свойственно самим заботливо проверять стабильность итогов совершённого ими слияния, самостоятельно внедрять результаты. Наш campfire-бот, по имени hubot, может внедрять код по указанию от любого из сотрудников. Достаточно подать в чате команду hubot deploy github to production, и код поступит на продакшен, где начнётся перезапуск (с нулевым даунтаймом) всех необходимых процессов. Можете сами судить о том, как часто это случается на Гитхабе: Как видите, шесть разных человек (среди которых один саппорт и один дизайнер) внедряли код более двух дюжин раз за сутки. Всё вышеописанное я совершал для ветвей с одним коммитом, содержащим однострочное изменение. Процесс прост, бесхитростен, масштабируем и силён. То же самое можно делать и с ветвью фичи, содержащей полсотни коммитов, потребовавших двухнедельной работы, и с одним коммитом, изготовленным минут за десять. Процесс настолько прост и настолько не тяготит, что его необходимость не раздражает даже в однокоммитном случае, так что люди редко пропускают или обходят отдельные шаги его — разве что речь идёт об изменении настолько малом и незначительном, что это никакого значения не имеет. Наш рабочий процесс обладает и силою, и невероятною простотою. Думаю, многие согласятся, что GitHub — очень стабильная платформа, что на её проблемы мы реагируем быстро (если они вообще возникают), что новые фичи внедряются в быстром темпе. Там нет таких компромиссов в отношении качества или стабильности, которые могли бы увеличить скорость и простоту рабочего процесса или уменьшить число его шагов. Заключение Git сам по себе довольно сложен для понимания. Если его ещё и употреблять в рабочем процессе более сложном, чем это необходимо, то дело кончится ежедневным чрезмерным усилием рассудка. Я всегда буду отстаивать употребление простейшей из возможных систем, пригодных для работы вашей группы, и до тех пор, пока система эта не перестанет работать; только тогда добавляйте сложность, когда никак не удаётся избежать этого. Для тех рабочих групп, которым необходимо подготавливать официальные выпуски кода через продолжительные интервалы (от нескольких недель до нескольких месяцев между выпусками), и создавать хотфиксы, и ветви поддержки прежних версий, и совершать другие дела, необходимость которых вызывается такими нечастыми выпусками кода, имеет смысл git-flow, и я весьма рекомендовал бы его употребление. Для групп, труд которых строится вокруг доставки кода, которые ежедневно обновляют продакшен, беспрерывно тестируют и внедряют фичи, я рекомендовал бы более простой рабочий процесс — такой, как GitHub Flow."], "hab": ["GitHub", "Git"]}{"url": "https://habrahabr.ru/post/278533/", "title": ["(La)TeX на Хабрахабре"], "text": ["Привет, хабр. Вопрос о нативном отображении формул на хабре есть достаточно давно (сам я не так давно писал в техподдержку с этим вопросом, получил ответ, что в планах есть, но пока всё очень неопределённо), и сегодня (а, точнее, уже вчера) был поднят в комментариях к \"Магия тензорной алгебры: Перезагрузка\". А если заглянуть в хаб по LaTeX, то сразу же 2 последние статьи — на тему оформления формул в статьях хабра. Самое интересное, что в принципе подключить поддержку TeX / LaTeX к любому сайту — дело пары минут и пары строк кода: достаточно подключить MathJax, ставшую уже почти что стандартной в задаче отображения формул в браузере. Традиционно для формул на хабре используются картинки, однако всё вышесказанное наводит на вполне определённую мысль, что есть и другой вид костылей. С одной стороны, менее удобный, с другой — более. … Вот он: v=document.createElement('script');v.type='text/x-mathjax-config';v.textContent=\"MathJax.Hub.Config({tex2jax:{inlineMath:[['$tex','$']],displayMath:[['$$tex','$$']]},asciimath2jax:{delimiters:[['$asc','$']]}});\";s=document.createElement('script');s.src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&locale=ru';document.head.appendChild(v);document.head.appendChild(s); Ctrl+C, F12 и Ctrl+V. При желании можно использовать как букмарклет, добавив в начале `javascript:` Аудитория хабра — не те люди, которых можно испугать консолью браузера или букмарклетом, правда ведь? Инлайновый TeX вставляется в $tex ...$, отдельным абзацем — в $$tex ... $$, и помимо этого есть поддержка AsciiMath в $asc ... $. Ну и живые примеры конечно же: $$tex \\frac{1}{\\pi} = \\frac{2 \\sqrt 2}{9801} \\sum_{k=0}^{\\infty} \\frac{(4k)!(1103 + 26390k)}{(k!)^4 396^{4k}} $$ Или вот инлайновая формула: $tex e^{ \\frac{d}{dt} } = 1 + \\frac{1}{1!}\\; \\frac{d}{dt} + \\frac{1}{2!}\\;\\frac{d^2}{dt^2} + \\frac{1}{3!}\\;\\frac{d^3}{dt^3} + \\cdots $. Минусы всего этого не менее очевидны, и, по правде говоря, заставляют сомневаться в жизнеспособности идеи, но, как минимум, как эксперимент — интересно, имхо. Что думаете?"], "hab": ["Математика", "LaTeX"]}{"url": "https://habrahabr.ru/post/318954/", "title": ["Обработка препроцессорных директив в Objective-C"], "text": ["Язык программирования с препроцессорными директивами сложен для обработки, поскольку в этом случае необходимо вычислять значения директив, вырезать некомпилируемые фрагменты кода, а затем производить парсинг очищенного кода. Обработка директив может осуществляться во время парсинга обычного кода. Данная статья подробно описывает оба подхода применительно к языку Objective-C, а также раскрывает их достоинства и недостатки. Эти подходы существуют не только в теории, но уже реализованы и используются на практике в таких веб-сервисах, как Swiftify и Codebeat. Swiftify — веб-сервис для преобразования исходников на Objective-C в Swift. На данный момент сервис поддерживает обработку как одиночных файлов, так и целых проектов. Таким образом, он может сэкономить время разработчикам, желающим освоить новый язык от Apple. Codebeat — автоматизированная система для подсчета метрик кода и проведения анализа различных языков программирования, в том числе и Objective-C. Содержание Введение Одноэтапная обработка Связывание скрытых токенов с нетерминальными узлами Связывание скрытых токенов с терминальными узлами Игнорируемые макросы Двухэтапная обработка Препроцессорный лексер Препроцессорный парсер Препроцессор Лексер Парсер Другие способы обработки Заключение Введение Обработка директив препроцессора осуществляется во время парсинга кода. Базовые понятия парсинга мы описывать не будем, однако здесь будут использоваться термины из статьи по теории и парсингу исходного кода с помощью ANTLR и Roslyn. В качестве генератора парсера в обоих сервисах используется ANTLR, а сами грамматики Objective-C выложены в официальный репозиторий грамматик ANTLR (Objective-C grammar). Нами было выделено два способа обработки препроцессорных директив: одноэтапная обработка; двухэтапная обработка. Одноэтапная обработка Одноэтапная обработка подразумевает одновременный парсинг директив и токенов основного языка. В ANTLR существует механизм каналов, позволяющий изолировать токены различных типов: например, токенов основного языка и скрытых токенов (комментариев и пробелов). Токены директив также могут быть помещены в отдельный именованный канал. Обычно токены директив начинаются со знака решетки (# или шарп) и заканчиваются символами разрыва строк (\\r\\n). Таким образом, для захвата подобных токенов целесообразно иметь другой режим распознавания лексем. ANTLR поддерживает такие режимы, они описываются так: mode DIRECTIVE_MODE;. Фрагмент лексера с секцией mode для препроцессорных директив выглядит следующим образом: SHARP: '#' -> channel(DIRECTIVE_CHANNEL), mode(DIRECTIVE_MODE); mode DIRECTIVE_MODE; DIRECTIVE_IMPORT: 'import' [ \\t]+ -> channel(DIRECTIVE_CHANNEL), mode(DIRECTIVE_TEXT_MODE); DIRECTIVE_INCLUDE: 'include' [ \\t]+ -> channel(DIRECTIVE_CHANNEL), mode(DIRECTIVE_TEXT_MODE); DIRECTIVE_PRAGMA: 'pragma' -> channel(DIRECTIVE_CHANNEL), mode(DIRECTIVE_TEXT_MODE); Часть препроцессорных директив Objective-C преобразуется в определенный код на языке Swift (например, с использованием синтаксиса let): какие-то остаются в неизмененном виде, а остальные преобразуются в комментарии. Таблица ниже содержит примеры: Objective-C Swift #define SERVICE_UUID @ \"c381de0d-32bb-8224-c540-e8ba9a620152\" let SERVICE_UUID = \"c381de0d-32bb-8224-c540-e8ba9a620152\" #define ApplicationDelegate ((AppDelegate *)[UIApplication sharedApplication].delegate) let ApplicationDelegate = (UIApplication.shared.delegate as? AppDelegate) #define DEGREES_TO_RADIANS(degrees) (M_PI * (degrees) / 180) func DEGREES_TO_RADIANS(degrees: Double) -> Double { return (.pi * degrees)/180; } #if defined(__IPHONE_OS_VERSION_MIN_REQUIRED) #if __IPHONE_OS_VERSION_MIN_REQUIRED #pragma mark - Directive between comments. // MARK: - Directive between comments. Комментарии также нужно помещать в правильную позицию в результирующем Swift коде. Однако, как уже упоминалось, в дереве разбора отсутствуют сами скрытые токены. Что если включать скрытые токены в дерево разбора?Действительно, скрытые токены можно включать в грамматику, но из-за этого она станет слишком сложной и избыточной, т.к. токены COMMENT и DIRECTIVE будут содержаться в каждом правиле между значимыми токенами: declaration: property COMMENT* COLON COMMENT* expr COMMENT* prio?; Поэтому о таком подходе можно сразу забыть. Возникает вопрос: как же все же извлекать такие токены при обходе дерева разбора? Как оказалось, существует несколько вариантов решения такой задачи, при котором скрытые токены связываются с нетерминальными или же терминальными (конечными) узлами дерева разбора. Связывание скрытых токенов с нетерминальными узлами Данный способ заимствован из относительно старой статьи 2012 года по ANTLR 3. В этом случае все скрытые токены разбиваются на множества следующих типов: предшествующие токены (precending); последующие токены (following); токены-сироты (orphans). Чтобы лучше понять что означают эти типы рассмотрим простое правило, в котором фигурные скобки — терминальные символы, а в качестве statement может быть любое выражение, содержащее точку с запятой на конце, например присваивание a = b;. root : '{' statement* '}' ; В таком случае все комментарии из следующего фрагмента кода попадут в список precending, т.е. первый токен в файле или токены перед нетерминальными узлами дерева разбора. /*First comment*/ '{' /*Precending1*/ a = b; /*Precending2*/ b = c; '}' Если комментарий является последним в файле, или же комментарий вставлен после всех statement (после него идет терминальная скобка), то он попадает в список following. '{' a = b; b = c; /*Following*/ '}' /*Last comment*/ Все остальные комментарии попадают в список orphans (все они по сути обособлены токенами, в данном случае фигурными скобками): '{' /*Orphan*/ '}' Благодаря такому разбиению, все скрытые токены можно обрабатывать в общем методе Visit. Данный способ и сейчас используется в Swiftify, однако он достаточно сложный и строить достоверное (fidelity) дерево разбора с помощью него проблематично. Достоверность дерева заключается в том, что оно может быть преобразовано обратно в код символ в символ, включая пробелы, комментарии и директивы препроцессора. В будущем мы планируем перейти на использование способа для обработки препроцессорных директив и других скрытых токенов, описание которого вы увидите ниже. Связывание скрытых токенов с терминальными узлами В данном случае скрытые токены связываются с определенным значимыми токенами. При этом скрытые токены могут быть лидирующими (LeadingTrivia) и замыкающими (TrailingTrivia). Этот способ сейчас используется в Roslyn парсере (для C# и Visual Basic), а скрытые токены в нем называются тривиями (Trivia). Во множество замыкающих токенов попадают все тривии на той же самой строчке от значимого токена до следующего значимого токена. Все остальные скрытые токены попадают в множество лидирующих и связываются со следующим значимым токеном. Первый значимый токен содержит в себе начальные тривии файла. Скрытые токены, замыкающие файл, связываются с последним специальным end-of-file токеном нулевой длины. Более детально о типах дерева разбора и тривиях написано в официальной документации по Roslyn. В ANTLR для токена с индексом i существует метод, который возвращает все токены из определенного канала слева или справа: getHiddenTokensToLeft(int tokenIndex, int channel), getHiddenTokensToRight(int tokenIndex, int channel). Таким образом, можно сделать так, чтобы парсер на основе ANTLR формировал достоверное дерево разбора, схожое с деревом разбора Roslyn. Игнорируемые макросы Так как при одноэтапной обработке макросы не заменяются на фрагменты кода Objective-C, их можно игнорировать или помещать в отдельный изолированный канал. Это позволяет избежать проблем при парсинге обычного кода Objective-C и необходимости включать макросы в узлы грамматики (по аналогии с комментариями). Это касается и макросов по умолчанию, таких как NS_ASSUME_NONNULL_BEGIN, NS_AVAILABLE_IOS(3_0) и других: NS_ASSUME_NONNULL_BEGIN : 'NS_ASSUME_NONNULL_BEGIN' ~[\\r\\n]* -> channel(IGNORED_MACROS); IOS_SUFFIX : [_A-Z]+ '_IOS(' ~')'+ ')' -> channel(IGNORED_MACROS); Двухэтапная обработка Алгоритм двухэтапной обработки может быть представлен в виде следующей последовательности шагов: Токенизация и разбор кода препроцессорных директив. Обычные фрагменты кода на этом шаге распознаются как простой текст. Вычисление условных директив (#if, #elif, #else) и определение компилируемых блоков кода. Вычисление и подстановка значений #define директив в соответствующие места в компилируемых блоках кода. Замена директив из исходника на символы пробела (для сохранения корректных позиций токенов в исходном коде). Токенизация и парсинг результирующего текста с удаленными директивами. Третий шаг может быть пропущен, и макросы могут быть включены непосредственно в грамматику, по крайней мере частично. Однако данный метод все равно сложнее реализовать, чем одноэтапную обработку: в этом случае после первого шага необходимо заменять код препроцессорных директив на пробелы, если существует потребность в сохранении правильных позиций токенов обычного исходного кода. Тем не менее данный алгоритм обработки препроцессорных директив в свое время также был реализован и сейчас используется в Codebeat. Грамматики выложены на GitHub вместе с визитором, обрабатывающим препроцессорные директивы. Дополнительным плюсом такого метода является представление грамматик в более структурированной форме. Для двухэтапной обработки используются следующие компоненты: препроцессорный лексер; препроцессорный парсер; препроцессор; лексер; парсер. Напомним, что лексер группирует символы исходного кода в значимые последовательности, которые называются лексемами или токенами. А парсер строит из потока токенов связную древовидную структуру, которая называется деревом разбора. Визитор (Visitor) — паттерн проектирования, позволяющий выносить логику обработки каждого узла дерева в отдельный метод. Препроцессорный лексер Лексер, отделяющий токены препроцессорных директив и обычного Objective-C кода. Для токенов обычного кода используется DEFAULT_MODE, а для кода директив — DIRECTIVE_MODE. Ниже приведены токены из DEFAULT_MODE. SHARP: '#' -> mode(DIRECTIVE_MODE); COMMENT: '/*' .*? '*/' -> type(CODE); LINE_COMMENT: '//' ~[\\r\\n]* -> type(CODE); SLASH: '/' -> type(CODE); CHARACTER_LITERAL: '\\'' (EscapeSequence | ~('\\''|'\\\\')) '\\'' -> type(CODE); QUOTE_STRING: '\\'' (EscapeSequence | ~('\\''|'\\\\'))* '\\'' -> type(CODE); STRING: StringFragment -> type(CODE); CODE: ~[#'\"/]+; При взгляде на этот фрагмент кода может возникнуть вопрос о необходимости дополнительных токенов (COMMENT, QUOTE_STRING и прочих), тогда как для кода Objective-C используется всего один токен — CODE. Дело в том, что символ # может быть спрятан внутрь обычных строк и комментариев. Поэтому такие токены необходимо выделять отдельно. Но это не является проблемой, поскольку их тип все равно изменяется на CODE, а в препроцессорном парсере для отделения токенов существуют следующие правила: text : code | SHARP directive (NEW_LINE | EOF) ; code : CODE+ ; Препроцессорный парсер Парсер, отделяющий токены кода Objective-C и обрабатывающий токены препроцессорных директив. Полученное дерево разбора затем передается препроцессору. Препроцессор Визитор, вычисляющий значения препроцессорных директив. Каждый метод обхода узла возвращает строку. Если вычисленное значение директивы принимает значение true, то возвращается последующий фрагмент кода Objective-C. В противном случае код Objective-C заменяется на пробелы. Как уже говорилось ранее, это необходимо для того, чтобы сохранить правильные позиции токенов основного кода. Для облегчения понимания приведем в качестве примера следующий фрагмент кода Objective-C: BOOL trueFlag = #if DEBUG YES #else arc4random_uniform(100) > 95 ? YES : NO #endif ; Этот фрагмент будет преобразован в следующий код на Objective-C при заданном условном символе DEBUG при использовании двухэтапной обработки. BOOL trueFlag = YES ; Стоит обратить внимание, что все директивы и некомпилируемый код превратились в пробелы. Директивы также могут быть вложенными друг в друга: #if __IPHONE_OS_VERSION_MIN_REQUIRED >= 60000 #define MBLabelAlignmentCenter NSTextAlignmentCenter #else #define MBLabelAlignmentCenter UITextAlignmentCenter #endif Лексер Лексер обычного Objective-C без токенов, распознающих препроцессорные директивы. Если директив в исходном файле нет, то на вход поступает тот же самый оригинальный файл. Парсер Парсер обычного Objective-C кода. Грамматика данного парсера совпадает с грамматикой парсера из одноэтапной обработки. Другие способы обработки Существуют и другие способы обработки препроцессорных директив, например можно использовать безлексерный парсер. Теоретически в таком парсере можно будет совмещать достоинства как одноэтапной, так и двухэтапной обработки, а именно: парсер будет вычислять значения директив и определять некомпилируемые блоки кода, причем за один проход. Однако такие парсеры также имеют и недостатки: их сложнее понимать и отлаживать. Так как ANTLR очень сильно завязан на процесс токенизации, то подобные решения не рассматривались. Хотя возможность создания безлексерых грамматик сейчас уже существует и будет дорабатываться в будущем (см. обсуждение). Заключение В настоящей статье были рассмотрены подходы по обработке препроцессорных директив, которые могут использоваться при парсинге C-подобных языков. Эти подходы уже реализованы для обработки кода Objective-C и используются в коммерческих сервисах, таких как Swiftify и Codebeat. Парсер с двухэтапной обработкой протестирован на 20 проектах, в которых количество безошибочно обработанных файлов составляет более 95% от общего числа. Кроме того, одноэтапная обработка также реализована для парсинга C# и выложена в Open Source: C# grammar. В Swiftify используется одноэтапная обработка препроцессорных директив, так как наша задача — не выполнить работу препроцессора, а транслировать препроцессорные директивы в соответствующие языковые конструкции Swift, несмотря на потенциально возможные ошибки парсинга. Например, директивы #define в Objective-C обычно используются для объявления глобальных констант и макросов. В Swift для этих же целей используются константы (let) и функции (func)."], "hab": ["Компиляторы", "Алгоритмы", "Swift", "Open source", "Objective C"]}{"url": "https://habrahabr.ru/post/273387/", "title": ["Сравнение аудитории Хабрахабра, Гиктаймса и Мегамозга"], "text": ["Привет, Хабр! Год назад я писал статью о том, кто и как подписан на Хабрахабр в соцсети Вконтакте. Буквально в первых же комментариях к тому посту было выражено пожелание увидеть разницу между подписчиками Geektimes и собственно Хабра. Прошел всего год и я, поборов свою лень, это желание исполняю. На самом деле у моей медлительности были и объективные причины – в январе запустился Мегамозг, и стало очевидным, что сравнение надо делать по всем трем сайтам. А для этого необходимо было подождать хотя бы полгода с момента окончательного разделения Хабра. В этой статье не будет очередных статистических выкладок о том, в какой день недели пост на Хабре получает наилучший рейтинг, а в какой собирает мало комментов — про это уже все сказано задолго до меня. Зато под катом мы попытаемся понять, как отличаются аудитории «хабровых» пабликов по различным параметрам (от пола до отношения к вредным привычкам), и есть ли связь между поведением пользователей в VK и на самих сайтах. Вместо вступления Для начала, обратимся к предметной области. Что из себя представляют три некогда единых сайта? Если вспомнить пояснения создателей, то вкратце и очень упрощенно, специфика каждого сайта такова: Хабрахабр (далее – ХХ) –для собственно IT-шников Гиктаймс (ГТ) – для гиков Мегамозг (ММ) – для ИТ-управленцев Как и чем отличаются аудитории этих сайтов? На этот вопрос подробно могут ответить, пожалуй, только сотрудники TM. А мы посмотрим на то, как отличаются аудитории одноименных пабликов в ВК. Вкратце о методике сбора данных. С помощью VK Api были собраны данные по всем подписчикам пабликов Хабрахабр, Geektimes и Мегамозг. Данные собирались на конец октября. Примерно на эту же дату с помощью самописного парсера (доступа к Хабр Api, увы, нет) были скачены все (ну или практически все) доступные статьи с этих же сайтов. В некоторых местах я ссылаюсь на статистическую значимость или незначимость различий. Она проверялась с помощью хи-квадрат критерия. Уровень значимости <0,05 (в том числе для коэффициентов корреляций). UPD: Кроме того, все же повторю и здесь свою цитату из прошлой статьи: «Также, обращаю внимание, что исследуемая выборка — аудитория паблика из соцсети «Вконтакте». А это значит, что данные пользователей в ней периодически могут изменяться, они могут быть неверны или неточны. Поэтому когда я буду говорить «читатели Хабра состоят на 146% из 91-летних мужчин с Острова Мэн», это не истина в последней инстанции. Просто такова информация, указанная пользователями в профилях.» И выводы, сделанные на основе данных подписчиков Хабра в VK, конечно же не обязательно будут справедливы для всех хабражителей на самих сайтах. Во-первых, необходимо понять, как перекликаются аудитории пабликов. Для торжественности момента приведем диаграмму Венна с соблюдением масштаба: Таблица пересечения аудиторий пабликов Хабрахабр Гиктаймс Мегамозг Хабрахабр 517 553 - - Гиктаймс 31 309 45 603 - Мегамозг 11 162 7 034 13 470 Общее пересечение (пользователи, подписанные сразу на все три паблика) – 6 481 Видим вполне логичную картину. Поскольку ГТ и ММ являются «отпрысками» самого Хабра, они пока что не могут тягаться с ним ни по размеру аудитории в целом, ни даже по относительному количеству «уникальных» подписчиков. Под «уникальными» подписчиками здесь понимаются пользователи, подписанные только на этот паблик и ни на один из двух других. На рисунке они выделены цветными областями, в то время как «неуникальные» — серыми. Для того чтобы наиболее четко выделить отличия аудиторий пабликов, анализировать мы будем именно «уникальных подписчиков», то есть серые области на рисунке – отбрасываем. Пример, почему это необходимо делать, приведен чуть ниже. Итак, приступим. Пол Не будем оригинальными и первым же делом посмотрим на различия по полу: Интерактивный вариант (где возможно, я буду приводить ссылки на интерактивные диаграммы, ибо они более наглядны и приятны глазу). Больше всего девушек в процентном соотношении среди подписчиков Мегамозга – практически треть. Меньше всего – в Гиктаймс (среди гиков реже встречаются представительницы «слабого» пола?), а Хабр занимает золотую середину. Причем различия эти статистически значимы. Обратите внимание, как отличается распределение для уникальных и неуникальных пользователей: большинство подписчиков ГТ и ММ – одновременно подписчики ХХ. Большинство подписчиков ХХ – мужчины. Из-за этого начинает искажаться и распределение признака (в данном случае пола) в других аудиториях. Именно поэтому мы анализируем только уникальных подписчиков. В целом, ничего неожиданного мы не увидели: среди «технарей» традиционно больше мужчин. Мегамозг, пожалуй наименее «технарский» проект из всех, что предопределяет относительно высокий процент девушек. С полом определились, на очереди возраст. Возраст Посмотрим на распределение относительного количества подписчиков по годам рождения (значения до 1975 года колеблятся около 0, так что эту часть графика отбросим для наглядности): Интерактивный вариант У Хабра и GT довольно плавные кривые. Линию Мегамозга «колбасит» больше всех – вероятно, это происходит из-за относительно малого количества респондентов. Но даже несмотря на это, очевидно, что «пик» у Хабра приходится на более солидный возраст, нежели чем у его «дочерних» сайтов, пускай и всего на пару-тройку лет. Наверное, такие различия довольно логичны. Хотя лично я ожидал, что у Мегамозга будет более возрастная публика. Но, как известно, мои ожидания — это мои проблемы. При этом отличия между ХХ и ГТ, ХХ и ММ – статистически значимы, а между ГТ и ММ – нет (что, в общем-то и так видно из рисунка). Любопытен так же всплеск активности в диапазоне 2000-2001 годов, наблюдаемый прежде всего у Хабра, ему я объяснения не нашел. Сильного всплеска численности аудитории «Вконтакте» этого года рождения не наблюдается. Так что будем надеяться, что у молодежи просто растет интерес к IT. Или же это как-то связано с «дефолтными» возрастами при регистрации в соцсети. География В этот раз (в отличие от прошлого исследования) ограничимся странами «большой четверки» Хабра – Россией, Украиной, Белоруссией, Казахстаном. Страны дальнего зарубежья отбросим, потому что даже если страна в профиле пользователя указана правдиво (сами помните, что порой указывают в графе «страна» хабравчане), то подавляющее большинство пользователей из таких стран – эмигранты с постсоветсткого пространства. Остаются страны бывшего СССР. Их мы тоже учитывать не будем, потому что они не дают сколько-нибудь значимого (а иногда и вовсе никакого не дают) числа уникальных подписчиков для Мегамозга. В конце концов, около 92% подписчиков приходятся именно на четыре вышеназванные страны, так что многого мы не упустим. И вот так выглядит разбивка «нормированного» числа подписчиков по ним: Интерактивный вариант Если вы помните, в прошлом году самой захабренной страной стала Белоруссия. Она и сейчас своего не упускает, но только относительно Хабрахабра. В то время как дочерние проекты интересны, прежде всего, пользователям из России. Замыкает четверку Казахстан, кроме случая с Мегамозгом, где третье место вырвано в упорной борьбе у Украины. Но по ММ вообще наблюдается самое равномерное распределение. Наиболее резкий спад интереса к дочерним пабликам наблюдается у украинских пользователей. Либо на Украине меньше интересуются тематиками этих ресурсов, либо за прошедший год пользователи из этой страны стали реже подписываться на паблики VK. Проверка первой гипотезы выходит за рамки нашего исследования, а вот вторую легко опровергнуть — достаточно взглянуть на темпы роста подписчиков Хабрахабра за прошедший год (с момента прошлого исследования) в разбивке по странам: Интерактивный вариант Как мы видим, все страны «большой четверки» показали одинаковый рост, за исключением Казахстана, который здесь в однозначных лидерах. Вузы Статистики по вузам в этот раз не будет, извините. И вот почему: как вы помните, мы смотрим только уникальных пользователей. Но деление по вузам разбивает подписчиков на слишком малые группы. Настолько малые, что даже для ГТ (не говоря уж про ММ) зачастую не остается уникальных пользователей. Из-за этого вуз может присутствовать в списке вузов подписчика Хабра, но будет отсутствовать в списке для ГТ. Что будет создавать ложное впечатление, будто студентам/выпускникам этого вуза Geektimes неинтересен вовсе. Понятный пример. Есть такой вуз, а вернее факультет вуза — ФСПО ИТМО. Из него 30 человек подписаны на Хабр и 5 человек на Geektimes. При этом все подписанные на ГТ подписаны на ХХ. Как результат – количество уникальных подписчиков ГТ — 0. Что с таким вузом делать? Игнорировать? Включать в статистику с особой пометкой? Анализировать по неуникальным пользователям? В общем, слишком много вопросов, а ценность сравнения сомнительна. Так что если кого-то интересует статистика по конкретному вузу – обращайтесь, выгружу. Вредные привычки В отношении к курению и алкоголю подписчики высказывают удивительное равнодушие, даже неинтересно: Интерактивный вариант Интерактивный вариант Правда, можно заметить, что мегамозговцы к вредным привычкам относятся чуть более лояльно. Видимо, работа более нервная. Но на самом деле это все не значимые отличия. Политические взгляды А вот различия в политических взглядах оказались значимы: Интерактивный вариант Самыми неравнодушными, либеральными (но и консервативными!) оказались подписчики Мегамозга. А наименее и наиболее умеренными – «гики» и хабравчане соответственно. Семейное положение Еще более интересны и различия в делах любовных. «Вконтакте» предоставляет несколько вариантов отношений, в которых состоит пользователь. Мы их немного скомпонуем, чтобы было нагляднее и удобнее: Таблица соответствия статусов семейного положения Статус для анализа Статус из ВК Есть партнер Есть партнер В браке Помолвлен Влюблен (да, можно быть влюбленным безответно, но не будьте занудами) Нет партнера Нет партнера В активном поиске В активном поиске - Все сложно Статус «все сложно» исключим – его сложно трактовать, да и выбрало его всего 3,2% подписчиков. Вдобавок разделим респондентов по половому признаку. И получим занимательную картину: Интерактивный вариант Во-первых, во всех пабликах девушки более успешны в поиске второй половинки, нежели парни (причем статистически значимо). А теперь посмотрим на количество подписчиков без второй половинки. Суммарно статусы «свободен» и «в поиске» дают примерно одинаковые результаты для всех пабликов. Но при этом хабравчане почти вдвое «смелее» своих коллег и активно ищут вторую половинку. Любой комментарий на этот счет выглядит плоской шуткой, даже если это было сказано всерьез. Так что оставим без комментариев. Ну а девушкам-подписчицам Мегамозга, судя по всему, и так хорошо, даже если они и одиноки. Связь между ВК и сайтами (лайки, рейтинги, вот это все) Следующим шагом хотелось бы увязать поведение пользователей в ВК и на самих сайтах. Сразу оговорюсь, что мы будем рассматривать только данные за 2015-й год. Во-первых, потому что именно в начале этого года произошло окончательное разделение на три различных сайта. А во-вторых, я не уверен, что создатели Хабра хотели бы, чтобы публиковалось сравнение показателей, например, количества просмотров. Особенно в разрезе лет. У записей в VK мы будем рассматривать три основных числовых показателя: Количество лайков Количество репостов Количество комментариев У постов на сайтах показателей чуть больше: • Рейтинг • Просмотры • Комментарии • Избранное Но, конечно же, кроме вышеперечисленных существует еще ряд факторов, которые могут влиять на показатели постов. Часть из них описывалась в других статьях по тематике (день, в который опубликован пост, например), часть требует более глубокого анализа, который выходит за рамки данной статьи, поэтому мы не будем их пытаться учесть. Ведь у нас нет задачи построить регрессионную модель, мы просто хотим посмотреть на связь показателей между собой. Но как минимум еще один фактор мы должны учесть, а именно – дата публикации. Ведь с течением времени количество подписчиков может расти, а это, в свою очередь, может влиять на количество репостов и лайков (больше подписчиков – больше лайков). Тогда мы не можем просто сравнить запись, созданную 1 января 2015-го с записью от сегодняшнего числа — нам необходимо будет так же учитывать насколько больше лайков ставят сегодня. Для начала определимся с изменением числа подписчиков за 2015-й год. В этом нам поможет старый-добрый веб-архив, с помощью которого мы сможем найти несколько значений числа подписчиков каждого паблика для нескольких разных дат. Отобразим эти точки на графике: Мы видим, что быстрее всех в относительном выражении растет аудитория Мегамозга (недалеко от него Гиктаймс), а медленнее всего – Хабр. Это вполне логично, учитывая возраст пабликов – молодые паблики растут быстрее. Но главная хорошая для нас новость заключается в том, что изменение числа подписчиков практически идеально описывается линейной функцией. Не придется сильно мучиться в дальнейшем, если захотим учесть влияние этого фактора. Простейшей регрессией мы можем предсказать численность аудитории любого из пабликов на любую дату в исследуемом периоде. Но придется ли этот фактор учитывать? Похоже, что нет: Лайки достаточно равномерно «размазаны» по всему году. Получается, что как ни увеличивается аудитория паблика, щедрее на лайки и репосты она не становится. Кстати, обратите внимание на «зазубрины» снизу на распределении HH. Это те самые выходные, про которые столько раз говорилось в обзорах статей Хабра – видимо потому что статей выходит мало и хабражители становятся щедрее на рейтинг. Эта закономерность в какой-то степени перекочевала и в соцсеть. Но только для Хабра — на остальные паблики, как видно из графиков, она не распространяется. Это подтверждается и коэффициентами корреляции для величин «количество записей в день» и «среднее количество лайков». Хабрахабр -0.455 Гиктаймс -0.237 Мегамозг -0.169 Теперь, когда мы прояснили вопросы с наиболее очевидными зависимостями, хочется посмотреть, как обстоят дела с другими показателями. Для этого построим корреляционные матрицы для каждого паблика. Но будем помнить, что корреляция говорит о тесноте связи, но в общем случае не позволяет установить причину и следствие. Для наглядности отобразим матрицы в следующем виде: Как мы видим, ситуация примерно одинакова для всех пабликов. Серьезные отличия есть только в зависимости показателя «избранное» от лайков и репостов. У Хабра связь достаточно явная, у остальных значительно слабее. Следует также отметить практически линейную связь лайков и репостов, хотя это было довольно ожидаемо. От дня года (и, как следствие, от количества подписчиков) ничего не зависит. Зато наблюдается довольно сильная корреляция между просмотрами статьи и ее рейтингом/количеством добавления в избранное. Что вполне логично – плохую статью вряд ли будут много просматривать, а хорошей статье, написанной для малой аудитории, не набрать сильно много плюсов. Лайки и репосты из ВК слабо связаны с рейтингом, проставляемом на сайтах (зато у Хабра и ГТ они не сильно, но коррелируют с количеством просмотров статей). Это собственно, один из главных выводов сравнения. Получается, что аудитория хабропабликов во Вконтакте и аудитория на сайтах не слишком сходятся в оценке постов. Занимательно, что количество комментариев на сайтах и количество комментариев в ВК очень слабо зависят друг от друга, хотя и призваны служить одной и той же цели – обсуждению статьи. Еще одно подтверждение разного поведения юзеров в VK и на самих порталах. Вместо заключения Можно долго спорить, было ли разделение Хабра оправдано и с какой целью оно делалось, но уже сейчас, спустя чуть менее года, начинают проявляться различия между аудиториями трех разных сайтов (или, по крайней мере, их пабликов). Подводя итог, можно сказать, что постепенно и Гиктаймс и Мегамозг начинают жить своей собственной жизнью, набирая свою отчасти уникальную аудиторию. Хоть пока и несравнимую по количеству с аудиторией своего «папы». Как разделение сказалось на жизни самого Хабра — другой вопрос, выходящий за рамки данного поста. На этой философской ноте и закруглимся. До новых встреч, если таковым суждено быть. И помните, что статистика – лишь третий вид лжи. P.S. Я извиняюсь, что запостил так же в хабе VK Api, а никакого кода не привел (он тривиален). Но насколько я видел, здесь порой бывают такие статьи. Думаю, это вполне подходящий паблик для поста, посвященному обработке данных, добытых из VK."], "hab": ["Вконтакте API", "Data Mining"]}{"url": "https://habrahabr.ru/post/254831/", "title": ["Генерация кода во время исполнения или «Пишем свой JIT-компилятор»"], "text": ["Современные компиляторы очень хорошо умеют оптимизировать код. Они удаляют никогда не выполняющиеся условные переходы, вычисляют константные выражения, избавляются от бессмысленных арифметических действий (умножение на 1, сложение с 0). Они оперируют данными, известными на момент компиляции. В момент выполнения информации об обрабатываемых данных гораздо больше. На её основании можно выполнить дополнительные оптимизации и ускорить работу программы. Оптимизированный для частного случая алгоритм всегда работает быстрее универсального (по крайней мере, не медленнее). Что если для каждого набора входных данных генерировать оптимальный для обработки этих данных алгоритм? Очевидно, часть времени выполнения уйдёт на оптимизацию, но если оптимизированный код выполняется часто, затраты окупятся с лихвой. Как же технически это сделать? Довольно просто — в программу включается мини-компилятор, генерирующий необходимый код. Идея не нова, технология называется “компиляция времени исполнения” или JIT-компиляция. Ключевую роль JIT-компиляция играет в виртуальных машинах и интерпретаторах языков программирования. Часто используемые участки кода (или байт-кода) преобразуются в машинные команды, что позволяет сильно повысить производительность. Java, Python, C#, JavaScript, Flash ActionScript — неполный (совсем неполный) список языков, в которых это используется. Я предлагаю решить конкретную задачу с использованием этой технологии и посмотреть, что получится. Задача Задачей возьмём реализацию интерпретатора математических выражений. На входе у нас строка вида и число — значение x. На выходе должны получить число — значение выражения при этом значении x. Для простоты будем обрабатывать только четыре оператора — ‘+’, ‘-‘, ‘*’, ‘/’. На данном этапе ещё не понятно, как вообще можно скомпилировать это выражение, ведь строка — не самый удобный для работы способ представления. Для анализа и вычислений лучше подойдёт дерево разбора, в нашем случае — бинарное дерево. Для исходного выражения оно будет выглядеть так: Каждый узел дерева представляет собой оператор, его дети — операнды. В листьях дерева оказываются константы и переменные. Самым простым алгоритмом построения такого дерева является рекурсивный спуск — на каждом этапе мы находим оператор с наименьшим приоритетом, разбиваем выражение на две части — до этого оператора и после, и рекурсивно вызываем для этих частей операцию построения. Пошаговая иллюстрация работы алгоритма: Реализацию алгоритма я тут приводить не буду по простой причине — он весьма объёмен, да и статья не про это. Дерево состоит из узлов, каждый из которых будет представлен структурой TreeNode     typedef struct TreeNode TreeNode;     struct TreeNode     {         TreeNodeType type; //тип узла         TreeNode* left;    //ссылка на левого потомка         TreeNode* right;   //ссылка для правого потомка         float value;       //значение (для узла-константы)     }; Вот все возможные типы узлов: typedef enum {     OperatorPlus,  //Оператор плюс     OperatorMinus, //Оператор минус     OperatorMul,   //Оператор умножить     OperatorDiv,   //Оператор разделить     OperandConst,  //Операнд - константа     OperandVar,    //Операнд - переменная     OperandNegVar, //Операнд - переменная, взятая с минусом (для обработки унарного минуса) } TreeNodeType; Значение выражения при заданном x вычисляется очень просто — с помощью обхода в глубину. Это тоже реализуется с помощью рекурсии.float calcTreeFromRoot(TreeNode* root, float x) {       if(root->type == OperandVar)         return x;     if(root->type == OperandNegVar)         return -x;     if(root->type == OperandConst)         return root->value;       switch(root->type)     {         case OperatorPlus:             return calcTreeFromRoot(root->left, x) + calcTreeFromRoot(root->right, x);         case OperatorMinus:             return calcTreeFromRoot(root->left, x) - calcTreeFromRoot(root->right, x);         case OperatorMul:             return calcTreeFromRoot(root->left, x) * calcTreeFromRoot(root->right, x);         case OperatorDiv:             return calcTreeFromRoot(root->left, x) / calcTreeFromRoot(root->right, x);     } } Что на текущий момент? Для выражения генерируется дерево При необходимости вычислить значение выражения вызывается рекурсивная функция calcTreeFromRoot Как бы выглядел код, если бы мы знали выражение ещё на этапе сборки? float calcExpression(float x) {     return x * x + 5 * x - 2; } Вычисление значения выражения в таком случае сводится к вызову одной очень простой функции — в разы быстрее, чем рекурсивный обход дерева. Время компиляции! Хочу заметить, что я буду генерировать код для платформы x86 ориентируясь на процессоры архитектуры, схожей с IA32. Кроме того, я приму размер float равным 4 байтам, размер int равным 4 байтам. Теория Итак, наша задача получить обычную функцию языка C, которую можно будет вызывать при необходимости. Для начала, определим её прототип: typedef float _cdecl (*Func)(float); Теперь тип данных Func представляет собой указатель на функцию, возвращающую значение типа float и принимающую один аргумент, тоже типа float. _cdecl указывает на то, что используется соглашение о вызове C-declaration. Это стандартное соглашение о вызове для языка C: — аргументы передаются через стек справа на лево (при вызове на вершине стека должен оказаться первый аргумент) — целочисленные значения возвращаются через регистр EAX — значения с плавающей точкой возвращаются через регистр st0 — за сохранность регистров eax, edx, ecx отвечает вызывающая функция — остальные регистры должна восстанавливать вызываемая функция Вызов функции выглядит примерно так: push ebp //Сохраняем указатель стекового фрейма push arg3 //Кладём в стек аргументы в порядке справа на лево (первый аргумент в конце) push arg2 push arg1 call func //Сам вызов add esp, 0xC //Восстанавливаем указатель на стек (3 аргумента по 4 байта как раз займут 0xC байт) pop ebp //Восстанавливаем указатель стекового фрейма Состояние стека в момент вызова: Как мы будем генерировать код, вычисляющий выражение? Время вспомнить, что есть такая штука как стековый калькулятор. Как он работает? На вход подаются числа и операторы. Алгоритм работы элементарен: — Встретили константу или переменную — положили в стек — Встретили оператор — сняли со стека 2 операнда, произвели операцию, положили результат в стек Но кормить стековый калькулятор надо определённым образом — он работает с выражениями, представленными в обратной польской нотации. Её особенность заключается в том, что в записи сперва идут операнды, потом оператор. Таким образом наше исходное выражение предстанет в следующем виде: Программа для стекового калькулятора соответствующая выражению: push x push x mul push 5 push x mul add push 2 sub Очень напоминает программу на ассемблере, не так ли? Получается, достаточно перевести выражение в обратную польскую нотацию и создавать код следуя элементарным правилам. Перевод выполнить очень просто — у нас уже есть построенное дерево разбора, надо только выполнить обход в глубину и при выходе из вершины генерировать соответствующее ей действие — получится последовательность команд в нужном нам порядке. Псевдокод этого действа:static void generateCodeR(TreeNode* root, ByteArray* code) {     if(root->left &amp;&amp; root->right)     {         generateCodeR(root->right, code); //Сначала должен сгенерироваться         generateCodeR(root->left, code); //код вычисления дочерних элементов     }     //Генерируем код для родительского элемента     if(root->type == OperandVar)     {         code += \"push x\"; //Кладём в стек значение аргумента (переменной)     }     else if(root->type == OperandNegVar)     {         code += \"push -x\"; //Кладём в стек значение аргумента со сменой знака     }     else if(root->type == OperandConst)     {         code += \"push \" + root->value; //Кладём в стек константу     }     else     {         code += \"pop\"; //Снимаем со стека         code += \"pop\"; //два значения         switch(root->type)         {             case OperatorPlus:                 code += \"add\"; //Выполняем сложение                 break;               case OperatorMinus:                 code += \"sub\"; //Выполняем вычитание                 break;               case OperatorMul:                 code += \"mul\"; //Выполняем умножение                 break;               case OperatorDiv:                 code += \"div\"; //Выполняем деление                 break;         }         code += \"push result\" //Сохраняем в стек результат математической операции     } } Реализация Для начала определимся со стеком: с ним всё просто — регистр esp содержит указатель на вершину. Чтобы положить что-нибудь туда, достаточно выполнить команду push {значение} или вычесть из ESP число 4 и записать по полученному адресу нужное значение sub esp, 4  mov [esp], {значение} Снятие со стека выполняет команда pop или прибавление к esp число 4. Раньше я упомянул соглашение о вызове. Благодаря ему мы можем точно знать, где будет находиться единственный аргумент функции. По адресу esp (на вершине) будет находиться адрес возврата, а по адресу esp — 4 как раз будет находиться значение аргумента. Так как обращение к нему будет происходить сравнительно часто можно поместить его в регистр eax: mov eax, [esp - 4]; Теперь, немного про работу с числами с плавающей запятой. Мы будем использовать набор инструкций x87 FPU. FPU имеет 8 регистров, образующих стек. Каждый из них вмещает 80 бит. Обращение к вершине этого стека происходит через регистр st0. Соответственно, регистр st1 адресует следующее за вершиной значение в этом стеке, st2 — следующее и так далее до st7. Стек FPU: Чтобы загрузить на вершину значение используется команда fld. Операндом этой команды может быть только хранящееся в памяти значение. fld [esp] //Загрузить в st0 значение, хранящееся по адресу, содержащемуся в esp Команды для выполнения арифметических операций очень просты: fadd, fsub, fmul, fdiv. У них может быть много разных комбинаций аргументов, но мы будем использовать их только так: fadd dword ptr [esp] fsub dword ptr [esp] fmul dword ptr [esp] fdiv dword ptr [esp] Во всех этих случаях загружается значение из [esp], выполняется необходимая операция, результат сохраняется в st0. Снять значение со стека тоже просто: fstp [esp] //Удалить значение с верхушки FPU стека и сохранить его в ячейку памяти по адресу, лежащему в esp Вспомним, что в выражении может встретиться переменная x с унарным минусом. Для её обработки надо сменить знак. Для этого гам пригодиться команда FCHS — она инвертирует бит знака регистра st0 Для каждой из этих команд определим по функции, генерирующей необходимые опкоды: void genPUSH_imm32(ByteArray* code, int32_t* pValue); void genADD_ESP_4(ByteArray* code); void genMOV_EAX_PTR_ESP_4(ByteArray* code);   void genFSTP(ByteArray* code, void* dstAddress); void genFLD_DWORD_PTR_ESP(ByteArray* code); void genFADD_DWORD_PTR_ESP(ByteArray* code); void genFSUB_DWORD_PTR_ESP(ByteArray* code); void genFMUL_DWORD_PTR_ESP(ByteArray* code); void genFCHS(ByteArray* code); Чтобы код-вычислитель нормально отработал и вернул значение надо добавить перед ним и после него соответствующие инструкции. Всё это дело вместе собирает функция-обёртка generateCode: void generateCode(Tree* tree, ByteArray* resultCode) {     ByteArray* code = resultCode;       genMOV_EAX_ESP_4(code); //Помещаем в eax значение аргумента       generateCodeR(tree->root, code); //Генерируем код-вычислитель       genFLD_DWORD_PTR_ESP(code);     genADD_ESP_4(code); //Снимаем лишнее двойное слово со стека     genRET(code); //Выходим из функции } Конечный вид функции генерации кода, вычисляющего значение выражения:void generateCodeR(TreeNode* root, ByteArray* resultCode) {     ByteArray* code = resultCode;     if(root->left &amp;&amp; root->right)     {         generateCodeR(root->right, code);         generateCodeR(root->left, code);     }       if(root->type == OperandVar)     {         genPUSH_EAX(code); //В eax лежт аргумент функции     }     else if(root->type == OperandNegVar)     {         genPUSH_EAX(code);          //Загружаем в стек         genFLD_DWORD_PTR_ESP(code); //Меняем         genFCHS(code);              //знак         genFSTP_DWORD_PTR_ESP(code);//Возвращаем в стек     }     else if(root->type == OperandConst)     {         genPUSH_imm32(code, (int32_t*)&amp;root->value);     }     else     {         genFLD_DWORD_PTR_ESP(code);//Загружаем в FPU левый операнд..         genADD_ESP_4(code);        //… и снимаем его со стека         switch(root->type)         {             case OperatorPlus:                 genFADD_DWORD_PTR_ESP(code); //Выполняем сложение (результат сохраниться в st0)                 break;               case OperatorMinus:                 genFSUB_DWORD_PTR_ESP(code); //Выполняем вычитание (результат сохраниться в st0)                 break;               case OperatorMul:                 genFMUL_DWORD_PTR_ESP(code); //Выполняем умножение (результат сохраниться в st0)                 break;               case OperatorDiv:                 genFDIV_DWORD_PTR_ESP(code); //Выполняем деление (результат сохраниться в st0)                 break;         }           genFSTP_DWORD_PTR_ESP(code);//Сохраняем результат в стек (st0 -&gt; [esp])     } } Кстати, о буфере для хранения кода. Для этих целей я создал тип-контейнер ByteArray: typedef struct {     int size; //Размер выделенной памяти     int dataSize; //Фактический размер хранящихся данных     char* data; //Указатель на данные } ByteArray;   ByteArray* byteArrayCreate(int initialSize); void byteArrayFree(ByteArray* array); void byteArrayAppendData(ByteArray* array, const char* data, int dataSize); Он позволяет добавлять в конец данные и не задумываться о выделении памяти — эдакий динамический массив. Если сгенерировать код с помощью generateCode() и передать ему управление, скорее всего программа рухнет. Причина проста — отсутствие разрешения на исполнение. Я пишу под Windows, поэтому мне тут поможет WinAPI функция VirtualProtect, позволяющая задать права для области памяти (вернее для страниц памяти). В MSD она описана так: BOOL WINAPI VirtualProtect(   _In_   LPVOID lpAddress, //Адрес начала региона в памяти   _In_   SIZE_T dwSize, //Размер региона в памяти   _In_   DWORD flNewProtect, //Новые параметры доступа для страниц в регионе   _Out_  PDWORD lpflOldProtect //Указатель на переменную, куда сохранить старые параметры доступа ); Она используется в основной функции-компиляторе: CompiledFunc compileTree(Tree* tree) {     CompiledFunc result;     DWORD oldP;     ByteArray* code;       code = byteArrayCreate(2); //Начальный размер контейнера - 2 байта       generateCode(tree, code); //Генерируем код       VirtualProtect(code->data, code->dataSize, PAGE_EXECUTE_READWRITE, &oldP); //Даём права на исполнение       result.code = code;     result.run = (Func)result.code->data;     return result; } CompiledFunc — структура, для удобного хранения кода и указателя на функцию: typedef struct {     ByteArray* code; //Контейнер с кодом     Func run; //Указатель на функцию } CompiledFunc; Компилятор написан и использовать его очень просто: Tree* tree; CompiledFunc f; float result;   tree = buildTreeForExpression(\"x+5\"); f = compileTree(tree); result = f.run(5); //result = 10 Осталось только провести тестирование на скорость. Тестирование При тестировании я буду сравнивать время работы скомпилированного кода и рекурсивного алгоритма вычисления на основе дерева. Замерять время будем с помощью функции clock() из стандартной библиотеки. Чтобы вычислить время работы участка кода достаточно сохранить результат её вызова до и после профилируемой части. Если найти разность этих значений поделить на константу CLOCKS_PER_SEC можно получить время работы кода в секундах. Конечно, это не очень точный способ замеров, но мне точнее и не требовалось. Во избежание сильного влияния погрешностей будем замерять время многократного вызова функций. Код для тестирования: double measureTimeJIT(Tree* tree, int iters) {     int i;     double result;     clock_t start, end;     CompiledFunc f;       start = clock();     f = compileTree(tree);     for (i = 0; i &lt; iters; i++)     {         f.run((float) (i % 1000));     }     end = clock();       result = (end - start) / (double)CLOCKS_PER_SEC;       compiledFuncFree(f);       return result; }   double measureTimeNormal(Tree* tree, int iters) {     int i;     clock_t start, end;     double result;       start = clock();     for (i = 0; i &lt; iters; i++)     {         calcTree(tree, (float) (i % 1000));     }     end = clock();       result = (end - start) / (double)CLOCKS_PER_SEC;       return result; } Код самого тестера можно посмотреть в репозитории. Всё что он делает — последовательно увеличивает размер входных данных в регулируемых пределах, вызывает указанные выше функции тестирования и записывает результаты в файл. По этим данным я построил график зависимости времени исполнения от размера входных данных (длины выражения). В качестве числа итераций для замера я взял 1 миллион. Просто так. Длина строки последовательно возрастала от 0 до 2000. Синий график — зависимость времени исполнения компилированного кода от размера задачи. Красный график — зависимость времени исполнения рекурсивного алгоритма от размера задачи. Чёрный график — отношение y координаты соответственных точек первого и второго графика. Показывает во сколько раз JIT быстрее в зависимости от размера задачи. Видно, что при размере входных данных = 2000 JIT выигрывает примерно в 9.4 раза. Мне кажется это весьма неплохо. Почему с JIT получается быстрее? Скомпилированный код абсолютно линейный — ни одного условного перехода. Благодаря этому за время исполнения не происходит ни одного сброса конвейера процессора, что крайне благотворно влияет на производительность. Что можно было сделать лучше? Самой большой проблемой нашего компилятора является то, что он не использует все возможности FPU. В FPU 8 регистров, мы используем от силы два. Если бы удалось перенести часть нашего стека в стек FPU, уверен, скорость вычислений бы сильно увеличилась (кончено, это надо тестировать). Ещё один недостаток — компилятор весьма глуп. Он не вычисляет константных выражений, не избегает лишних вычислений. Очень легко составить выражение так, чтобы он тратил максимум ресурсов. Если добавить элементарный упроститель выражений, компилятор станет близким к идеальному. Проблемой самой реализации можно считать невозможность простого переноса на другую платформу (в функции генерации кода используются конкретные команды) и операционную систему (VirtualProtect, всё же). Вполне очевидным и довольно простым решением будет абстрагирование зависящих от платформы и ОС частей и вынесение их реализаций в отдельные модули. Спасибо за прочтение, очень жду комментариев и советов. Репозиторий: bitbucket.org/Informhunter/jithabr На статью меня вдохновила книга “Совершенный код”, а именно, её глава “Динамическая генерация кода для обработки изображений”."], "hab": ["Программирование", "Компиляторы", "C"]}{"url": "https://habrahabr.ru/post/175685/", "title": ["GitHub Pages переезжают на github.io"], "text": ["Начиная с сегодняшнего дня все сайты GitHub Pages переходят на новый домен: github.io. Это мера безопасности нацеленна на предотвращение CSRF атак на главный сервер — github.com. Если ваш сайт настроен, как «yoursite.com» вместо «yoursite.github.com» — изменения вас никак не затронут. Если ваш сайт раньше располагался на домене «username.github.com», последующие запросы будут редиректиться на новый домен: «username.github.io». C этого момента все сайты, размещенные на субдоменах github.com могут и должны расцениваться как официальные продукты GitHub. Технические детали Изменения в сайтах и пользовательских доменах: Все пользователи, организации, и сайты проектов настроенные на работу с github.io, вместо github.com. Все сайты *.github.com редиректятся c кодом 301 на *.github.io. Сайты, с пользовательскими доменами изменения не затронули. IP адреса не изменились, существующие A-записи указывают на старые IP. Изменения в GitHub репозиториях: Пользовательские сайты можно называть используя новое, или старое соглашение: username/username.github.[io/com]. Существующие репозитории сайтов, названные по старому соглашению так и будут продолжать свою работу. Если существует два репозитория, названные по старому и новому соглашению — *.github.io победит. Уязвимости безопасности. Существует две основные категории потенциальных уязвимостей, которые привели к этим переменам. Изменение сессий и CSRF. Так как пользовательские сайты могут подключать JavaScript, который хранится на субдоменах github.com, становится возможным записывать(но не считывать) куки с домена github.com, что позволяет злоумышленнику получить доступ к github.com и осуществить атаку Фишинговые атаки, при которых злоумышленник создает похожий сайт и запрашивает у пользователя конфиденциальную информацию. У нас нет доказательств, что такие атаки были, однако мы постарались их заранее не допустить. Источник: github.com/blog/1452-new-github-pages-domain-github-io UPD: В комментариях s01o указал на пост Егора Хомакова, там рассказывается как всетаки можно провести атаку. Как оказалось — возможно это в WebKit браузерах, так как они особым образом работают с куками. Так это устроенно в нормальных браузерах: Cookie:_gh_sess=ORIGINAL; _gh_sess=HACKED; Надо понимать, что _gh_sess — это две разные куки, не смотря на название и отсылаются они в одно и то же время. В WebKit дело же обстоит по другому, куки отсылаются не по домену (первой записью должно быть Domain=github.com) ровно тоже самое и с httpOnly(очевидно, что они также должны быть в начале). В действительности они упорядочиваются по времени создания (В этом месте я должен быть не прав, но это то, как есть на самом деле)"], "hab": ["Системы управления версиями", "Git"]}{"url": "https://habrahabr.ru/post/60030/", "title": ["Git Workflow"], "text": ["1 Вступление В топике освещаются не столько подробности работы с git, сколько его отличия от схемы разработки других систем контроля версий, и общий подход (выработанный по большей части личным опытом и Git Community Book) к работе. Современная разработка программного обеспечения стала чем-то большим, чем просто набором исходного кода программы в текстовом редакторе; она обросла целым комплексом дополнительного инструментария, вроде багтреккеров, систем для управления проектами и систем контроля версий (СКВ). Последние, пожалуй, играют особенную роль в проекте, поскольку определяют сам ход работ (или workflow). 2 Централизованные системы контроля версий Классическим примером подобных программ в мире открытого софта являются CVS и ее потомок — Subversion (лозунг проекта — «CVS the right way»); проприетарные аналоги: Perforce или Clearcase. Эти системы строятся вокруг централизованной модели разработки, в которой существует единственный удаленный репозитарий, в который вносят изменения все разработчики проекта. Ветвление (branching) проекта возможно, но не желательно и приносит, как правило, только дополнительные сложности в проект. Стандартный ход разработки выглядит примерно следующим образом: выкачивание из репозитария последней версии; разработка новой функциональности или исправление ошибок; повторное обращение к репозитарию для разрешения возможных конфликтов с работой других разработчиков; закачивание очередной ревизии на сервер. Соответствующие команды: svn checkout (забрать последнюю версию), svn resolve (показать, что конфликт в исходном коде разрешен) и svn commit (создать в репозитарии очередную ревизию). Подобный линейный подход к разработке прост и очевиден, но здорово ограничивает программиста. А что, если на любой из стадий цикла требуется отвлечься на другой функционал? Или срочно исправить какой-либо баг в предыдущей работе? Существуют разные выходы из ситуации. Можно проявить твердость, и закончить текущую работу, чтобы потом обратиться к следующим этапам; или, как вариант, нагружать текущий коммит большим количеством изменений. В первом случае не обеспечивается достаточная гибкость; во втором — усложняется поиск ошибки в коммите, нарушается логическая цельность действия. Откатиться к начальному состоянию невозможно — значит потерять уже проведенную работу. Ну или, если совсем усложнить, завести отдельную копию проекта, в ней исправить ошибку (внести функционал), закоммитить, затем стереть копию, и вернуться к прежней работе… Сложно? Сложно. Не выход, иными словам. Кроме того, иногда требуется работать без доступа к центральному репозитарию (удаленная работа, поездка и т.д. и т.п.). Что делать? Лишаться всякой гибкости разработки и заливать монстроидальный коммит весом в неделю? 3 Распределенный подход Решением подобных проблем явилась альтернативная схема разработки, предлагаемая так называемыми распределенными системами контроля версий (Distributed Version Control System). Среди открытых разработок на данную тему можно вспомнить git, Mercurial и Bazaar. Первый проект особенно интересен, он используется в некоторых из сложнейших современных программных систем(Linux Kernel, Qt, Wine, Gnome, Samba и многие другие), крайне быстро работает с любым объемом кода и сейчас набирает популярность в открытом мире. Какое-то время на распространении этой программы негативно сказывался недостаток документации; но сейчас этот недостаток можно считать устраненным. Итак, в чем заключается глобальное отличие git (и DVCS вообще) от централизованных аналогов? Во-первых, как следует из самого названия, не существует главного (в том смысле, который его понимают разработчики, привыкшие к SVN) репозитария. У каждого разработчика имеется собственный полноценный репозитарий, с которым и ведется работа; периодически проводится синхронизация работы с (чисто условно!) центральным репозитарием. Во вторых, операции ветвления и слияния веток (merging) ставятся во главу угла при работе программиста, и поэтому они очень легковесны. Кстати говоря, привычных ревизий также не существует; но об этом — чуть позже. 4 workflow в одну морду Итак, рассмотрим простейший случай: личный проект, в котором участвует единственное одно лицо — вы. Для создания нового репозитария достаточно просто зайти в папку проекта и набрать: git init Был создан пустой репозитарий — папка .git в корне проекта, в которой и будет собираться вся информация о дальнейшей работе (и никаких некрасивых .svn, разбросанных по дереву проекта!). Предположим, уже существует несколько файлов, и их требуется проиндексировать командой git add: git add . Внесем изменения в репозитарий: git commit -m \"Первоначальный коммит\" Готово! Имеется готовый репозитарий с единственной веткой. Допустим, потребовалось разработать какой-то новый функционал. Для этого создадим новую ветку: git branch new-feature И переключимся на нее (обратите внимание на отличие в терминологии по сравнению с SVN): git checkout new-feature Вносим необходимые изменения, после чего смотрим на них, индексируем и коммитимся: git status git add . git commit -m \"new feature added\" Теперь у нас есть две ветки, одна из которых (master) является условно (технически же ничем не отличается) основной. Переключаемся на нее и включаем изменения: git checkout master git merge new-feature Легко и быстро, не находите? Веток может быть неограниченное количество, из них можно создавать патчи, определять diff с любым из совершенных коммитов. Теперь предположим, что во время работы выясняется: нашелся небольшой баг, требующий срочного внимания. Есть два варианта действий в таком случае. Первый состоит из создания новой ветки, переключения в нее, слияния с основой… Второй — команда git stash. Она сохраняет все изменения по сравнению с последним коммитом во временной ветке и сбрасывает состояние кода до исходного: git stash Исправляем баг и накладываем поверх произведенные до того действия (проводим слияние с веткой stash): git stash apply Вот и все. Очень удобно. На самом деле таких «заначек» (stash) может быть сколько угодно; они просто нумеруются. При такой работе появляется необычная гибкость; но среди всех этих веточек теряется понятие ревизии, характерное для линейных моделей разработки. Вместо этого каждый из коммитов (строго говоря, каждый из объектов в репозитарии) однозначно определяется хэшем. Естественно, это несколько неудобно для восприятия, поэтому разумно использовать механизм тэгов для того, чтобы выделять ключевые коммиты: git tag просто именует последний коммит; git tag -a также дает имя коммиту, и добавляет возможность оставить какие-либо комментарии (аннотацию). По этим тегам можно будет в дальнейшем обращаться к истории разработки. Плюсы такой системы очевидны! Вы получаете возможность колдовать с кодом как душе угодно, а не как диктует система контроля версий: разрабатывать параллельно несколько «фишек» в собственных веточках, исправлять баги, чтобы затем все это дело сливать в единую кашу главной ветки. Замечательно быстро создаются, удаляются или копируются куда угодно папочки .git с репозитарием, не в пример SVN. Гораздо удобней такую легковесную систему использовать для хранения версий документов, файлов настроек и т.д, и т.п. К примеру, настройки и плагины для Емакса я храню в директории ~/site-lisp, и держу в том же месте репозитарий; и у меня есть две ветки: work и home; иногда бывает удобно похожим образом управлять настройками в /etc. Естественно, что каждый из моих личных проектов тоже находит под управлением git. 5 Общественные репозитарии Общественный репозитарий — способ обмениваться кодом в проектах, где участвует больше двух человек. Лично я использую сайт github.com, настолько удобный, что многие начинают из-за него пользоваться git. Итак, создаем у себя копию удаленного репозитария: git clone git://github.com/username/project.git master Команда создала у вас репозитарий, и внесла туда копию ветки master проекта project. Теперь можно начинать работу. Создадим новую ветку, внесем в нее изменения, закоммитимся: git branch new-feature edit README git add . git commit -m \"Added a super feature\" Перейдем в основную ветку, заберем последние изменения в проекте, и попробуем добавить новую фишку в проект: git checkout master git pull git merge new-feature Если не было неразрешенных конфликтов, то коммит слияния готов. Команда git pull использует так называемую удаленную ветку (remote branch), создаваемую при клонировании удаленного репозитария. Из нее она извлекает последние изменения и проводит слияние с активной веткой. Теперь остается только занести изменения в центральный (условно) репозитарий: git push Нельзя не оценить всю гибкость, предоставляемую таким средством. Можно вести несколько веток, отсылать только определенную, жонглировать коммитами как угодно. В принципе, никто не мешает разработать альтернативную модель разработки. Например, использовать иерархическую систему репозитариев, когда «младшие» разработчики делают коммиты в промежуточные репозитарии, где те проходят проверку у «старших» программистов и только потом попадают в главную ветку центрального репозитария проекта. При работе в парах возможно использовать симметричную схему работы. Каждый разработчик ведет по два репозитария: рабочий и общественный. Первый используется в работе непосредственно, второй же, доступный извне, только для обмена уже законченным кодом. 6 Заключение Я старался не акцентировать внимание на всех тонкостях использования главных команд, тем более что почти наверняка не знаю их все, да активно использовать git начал сравнительно недавно; но хотел продемонстрировать именно простоту и гибкость этого замечательного средства разработки. Также не стремился объяснить особенности внутренней механики проекта; хотя бы даже потому, что изящность и красота реализации заслуживает как минимум еще одного хабратопика. Естественно, приветствуются указания на фактические ошибки, подсказки насчет использования системы и общие замечания. :) UPD: разжился кармой и перенес в блог Git UPD2: продолжение заметки, несколько неформатное для Хабра :)"], "hab": ["Git"]}{"url": "https://habrahabr.ru/post/322700/", "title": ["Охота на мифический MVC. Пользовательский интерфейс: как делить и куда помещать логику"], "text": ["Детектив по материалам IT. Часть вторая Чтобы не было разочарований, \"считаю долгом предупредить, что кот древнее и неприкосновенное животное\" что в этой части будет продолжено исследование “исторического хлама” и написанное здесь практически бесполезно при работе с конкретными фреймворками. Ну а тем, кто не испугался, я попробую показать, как же изначально выглядело деление Вид/Контроллер и какие интересные решения можно найти в этой области на сегодняшний день. Ссылки на первоисточники приведены в начале первой части. Итак, переходим к Пользовательскому Интерфейсу. Не смотря на то что Вид определяется как модуль, отображающий Модель – \"а view is a (visual) representation of its model\", на практике к Виду, как правило, просто относят все графические элементы GUI, то есть Видом считается все то, что мы видим на экране ЭВМ. Понятно, что тут содержится некое противоречие, поскольку такие графические компоненты как меню, кнопки, тулбары служат не для отображения информации о системе, а прежде всего для управления системой. Клавиатура и мышь всегда были средством управления программой и находились в «ведомости» Контроллера (как бы его не трактовали). Поэтому кажется нелогичным и странным, что кнопки, сделанные из пластмассы, считаются элементами управления и относятся к Контроллеру, а кнопки, нарисованные на экране, и по сути выполняющие те же самые функции (производить входящие события), почему то относят к Виду. Кроме того, поскольку разумно считается что в графических элементах не хорошо «прописывать логику», то отсюда почему-то делается вывод, что Вид должен быть тонким и тупым (dumb View) и соответственно логику работы интерфейса можно обнаружить в самых неожиданных местах, вплоть до доменной модели (даже Фаулер пишет о «загрязнении» Модели настройками интерфейса GUI Architectures). Интерфейс. Деление на Вид и Контроллер И вновь для того, чтобы прояснить ситуацию предлагаю обратиться к «архитектурным принципам» и первоисточникам. Когда речь идет о декомпозиции то одно из основных «архитектурных» правил заключается в том, что делить на модули нужно прежде всего исходя из тех задач, которые решает система. Каждый модуль должен отвечать за решение какой-то определенной задачи (желательно одной) и выполнять соответствующую ей функцию. Соответственно, для того, чтобы понять как следует делить пользовательский интерфейс на модули, в первую очередь надо проанализировать что он делает и какие задачи решает. Ведь интерфейс это функция, а не графические элементы. И функция эта заключается в том, чтобы обеспечивать взаимо-действие пользователя с системой. Что означает: выводить и удобно отображать пользователю информацию о системе вводить данные и команды пользователя в систему (передавать их системе) То есть, в общем случае пользовательский интерфейс является двунаправленным и решает две задачи, одна из которых связана с выводом и представлением информации, а вторая – с вводом команд и данных. Вот эти-то задачи и определяют то, как нужно делить интерфейс на модули. Вновь смотрим картинку из первоисточника (Реенскауг): Как видно пользовательский интерфейс довольно естественно и логично делится на два относительно независимых функциональных модуля. Причем такое функциональное деление, основанное на решаемых задачах, универсально. Не важно звуковой это интерфейс, графический или сенсорный в нем должен быть модуль, отвечающий за Ввод управляющих команд и данных (отсюда и название Controller — управление), и модуль отвечающий Вывод и представление информации о системе и о том что в ней происходит (View — Вид, представление). Не смотря на простоту и очевидность этого принципа он часто нарушается. Сравните: «Вид это то, что мы видим на экране ЭВМ, графические элементы» Нет решаемой задачи, за определение взят вторичный признак. Функциональное определение: «Вид отвечает за вывод информации о системе и ее представление для пользователя»Четко указана решаемая задача, при этом абсолютно не важны детали ее реализации, то каким именно образом будет выводиться информация – графически ли, с помощью аудио или еще как-то. За счет чего и достигается универсальность. «Контроллер обрабатывает действия пользователя, \"interprets the mouse and keyboard inputs\"» Указано действие, но не определена цель этого действия, то есть решаемая задача. Это равносильно тому, как сказать, что работа программиста заключается в том, чтобы нажимать клавиши на компьютере. В результате под «обработкой действий пользователя» может пониматься все что угодно, вплоть до бизнес логики. Плохо также то, что в определение вынесены детали реализации – мышь и клавиатура. Как только технология поменялась – мышь и клавиатура сменились сенсорными экранами, определение устарело) Функциональное определение: «Контроллер обеспечивает удобный ввод команд и данных и их передачу от пользователя системе»Но это теория, а было обещано, что вы увидите как деление Вид/Контроллер выглядело на практике. И для того чтобы это сделать нам потребуется небольшой экскурс в историю. Дело в том, что вначале термина CONTROLLER вообще не существовало. Вместо него Реенскауг использовал термин EDITOR (MODEL-VIEW-EDITOR) и писал о разделении пользовательского интерфейса на View и Editor. \"The hardest part was to hit upon good names for the different architectural components. Model-View-Editor was the first set.\"(Trygve Reenskaug). Попробую объяснить почему. Не смотря на наличие технических возможностей (растровые экраны) во времена создания MVC и SmallTalk пользовательские интерфейсы преимущественно все еще оставались командными (Command-driven Interface) и представляли собой по сути обычные текстовые редакторы. В SmallTalk интерфейс так и назывался – Editor. И вот как он выглядел: Можно сказать что Editor объединял в себе функции Контроллера и Вида. Он давал возможность относительно удобно вводить команды и данные (отображая нажатые клавиши и обрабатывая ввод с клавиатуры), и одновременно выводил информацию о выполнении команд и вообще о том что происходит в системе. Лишь постепенно этот Editor преобразовывался в то, что мы сейчас привыкли понимать под GUI. Вначале возникла простая но весьма плодотворная идея – разделить единое окно на множество панелей. Парадигма многопанельности появились существенно раньше MVC (многопанельные браузеры точно присутствовали уже в Smalltalk-76) и была значительным продвижением сама по себе. Она до сих пор активно используется практически во всех текстовых интерфейсах. Реенскауг ее, естественно, тоже использовал. Тут нужно иметь в виду, что Dynabook, вокруг которого в Xerox Parc создавались и smallTalk и графические интерфейсы и, в частности, MVC, задумывался как «детский компьютер». Поэтому стояла задача сделать работу с компьютерными программами доступной любому неподготовленному пользователю, в частности ребенку. Реенскауг исходил из того, что пользователь может вообще ничего не знать о программе и о том как она устроена. И для того чтобы он мог с программой взаимодействовать нужно каким-то образом отобразить ему основную информацию о системе и доменной модели, лежащей в ее основе, чтобы пользователь понимал с чем имеет дело. Выводилась такая информация на отдельных панелях, которые собственно и стали называться View. Как правило доменная модель отображалась при помощи нескольких различных Видов: \"MVC задумывался как общее решение, дающее возможность пользователям контролировать большие и сложные наборы данных… Он особенно полезен тогда, когда пользователю нужно видеть Модель одновременно в разных контекстах и/или с разных точек зрения.\" И поскольку Реенскауг считал, что графическое представление информации нагляднее чем текстовое, то в основном виды у него представляли собой разного рода графики и диаграммы. Далее. Так как пользователь ничего (или почти ничего) не знает о системе, а видит лишь всевозможные View, удобно и наглядно отображающие нужную ему информацию, то соответственно и управление системой должно было выглядеть так, как если бы пользователь управлял непосредственно самими View и тем что на них отображено. Поэтому для ввода команд стал использоваться не один «general Editor», а целое множество специализированных редакторов, каждый из которых был связан со своим Видом и был заточен на ввод команд (взаимодействие с доменной моделью) лишь в контексте этого Вида – permits the user to modify the information that is presented by the view. Таким образом, если для специалиста интерфейсом обычно служил некий общий редактор, дающий возможность вводить в систему любые команды (но для этого нужно было штудировать мануалы, знать команды и понимать, как система работает), то для неподготовленного пользователя минимальным интерфейсом становится пара – View и связанный с ней специализированный Editor (который собственно и будет впоследствии переименован в Контроллер). Такой интерфейс предоставлял весьма редуцированный набор команд и возможностей, зато им можно было пользоваться без предварительной подготовки. Первый доклад Реенскауга так и назывался: \"THING-MODEL-VIEW-EDITOR. An Example from a planning system\". В нем была представлена первая реализация MVC на примере системы планирования и управления неким большим проектом (своего рода task-manager). Доменной моделью являлась сеть (network) «активностей», которая описывала что должно быть сделано (активность), в каком порядке, за какое время, кто в каких активностях участвует и какие ресурсы для каждой активности требуются. Пример призван был показать что “одна Модель может быть отображена с помощью многих различных Видов” и вот как там выглядел пользовательский интерфейс: Доменная модель отображалась с помощью трех различных диаграмм (Видов). Нужно отметить, что виды и у Реенскауга и в SmallTalk вовсе не были \"пассивными\", они самостоятельно обрабатывали относящиеся к ним действия пользователя, в частности позволяли делать скроллинг и выделение элементов: “A ListView has fields where it remembers its frame, a list of textual items and a possible selection within the list. It is able to display its list on the screen within its frame, and reacts to messages asking it to scroll itself.” Это важный момент и я еще к нему вернусь. Первая диаграмма отображает множество активностей, относящихся к некоторому проекту/сети и связи между ними. Подобно тому, как список позволяет выбрать/выделить некий свой элемент, данная диаграмма позволяла выбрать некую активность. С диаграммой связан редактор, который дает возможность запрашивать у системы и редактировать информацию, относящуюся к выбранной активности. Например: длительность активности, кто в ней участвует, какие активности предшествуют и тп Вторая диаграмма – GanttView (календарный или временной график Гантта) показывает расположение проекта и его активностей во времени. Эта диаграмма также позволяет выбрать некую активность. Редактор, связанный с GanttView дает возможность \"to pass on operations on the network and its activities that are related to this particular View\". В частности он позволяет изменить запланированную дату начала и окончания выбранной активности, а также осуществлять планирование и управление проектом/сетью как целым. Третья диаграмма — это диаграмма ресурсов, требуемых для осуществления активностей, в зависимости от времени. Любопытно то, что с этой диаграммой связан не редактор, а список и в зависимости от того какая активность в этом списке выбрана, диаграмма ресурсов отображает только те ресурсы, которые относятся к выбранной активности. Такое сочетание двух видов, один из которых \"управляет\" отображаемой информацией другого характерно для многих интерфейсов. А идея использовать для «управления» не текстовый редактор а список ляжет в основу большинства контроллеров в SmallTalk-80. Термин Controller возник практически перед самым уходом Реенскауга из Xerox PARC \"After long discussions, particularly with Adele Goldberg\". И из-за этого оригинальные работы Реенскауга сложно читать, поскольку одним и тем же словом «Editor» он называет: Панели-редакторы которые связывались с соответствующими View для ввода команд и «управления View» и которые в Smalltalk-80 стали Контроллерами (\"Smalltalk-80 Controller у меня назывался Editor\"). Пару – Вид и связанный с ним Контроллер, которая стала базовым блоком для построения любых интерфейсов. В этих случаях Реенскауг пишет о разделении Editor на View и Controller Весь интерфейс целиком, который мог включать в себя несколько блоков Вид-Контроллер. И в этом случае Реенскауг пишет о первичном разделении приложения на Model (domain model) и Editor. Позже для обозначения таких сложно составных интерфейсов Реенскауг будет использовать термин Tool. Также к обязанностям Контроллера Реенскауг относил управление самим интерфейсом и в частности множеством входящих в него Видов: \"Controller was responsible for creating and coordinating its subordinate views\". Соответственно иногда он пишет, что Контроллер это связь между пользователем и системой, а иногда что это связь между пользователем и Видами и что Контроллер передает команды Видам. Тем не менее, как видно из примера, Реенскауговский «Editor-Controller» был вполне себе видим и имел графическое представление. Посмотрим как дело обстояло в SmallTalk-80. Первое: в SmallTalk-80 текстовые редакторы, которые Реенскауг использовал для ввода команд, «официально» стали Контроллерами. «ParagraphEditor» и «TextEditor», обладающие стандартными функциями ввода и редактирования текста, в SmallTalk-80 являлись потомками класса «Controller»: Поскольку, как уже упоминалось, редакторы объединяли в себе функции Ввода и Вывода, то они также использовались для отображения текстовой информации во многих Видах – TextView, TextEditorView. Стив Барбек дает по этому поводу довольно подробное разьяснение: \"Все контроллеры, которые принимают ввод с клавиатуры, являются наследниками «ParagraphEditor» в иерархии Контроллеров. «ParagraphEditor» предшествовал созданию парадигмы MVC. Он одновременно выполняет две функции – обрабатывает ввод текста с клавиатуры и отображает его на экране. Поэтому в некотором смысле он представляет собой нечто среднее между Видом и Контроллером. Виды, которые используют подклассы «ParagraphEditor» в качестве контроллера, полностью переворачивают стандартные роли – для того чтобы отобразить текст они посылают его своему контроллеру\" [All controllers that accept keyboard text input are under the ParagraphEditor in the Controller hierarchy. ParagraphEditor predates the full development of the MVC paradigm. It handles both text input and text display functions, hence it is in some ways a cross between a view and a controller. The views which use it or its subcasses for a controller reverse the usual roles for display; they implement the display of text by sending controller display]. О причине подобных «парадоксов» я постараюсь написать дальше, пока же просто обратите на это внимание Второе: в SmallTalk-80 с каждым Видом (подвидом) ОБЯЗАТЕЛЬНО был связан свой Контроллер, который давал возможность производить некие операции с той информацией, которую Вид отображает Соответственно любое множество Видов (подвидов) входящих в состав пользовательского интерфейса на самом деле всегда сопровождалось точно таким же множеством связанных с ним Контроллеров. Опять таки у Стива Барбека этой теме посвящен целый раздел который так и называется — \"Communication Between Controllers\". Третье: главное усовершенствование состояло в том, что для ввода команд преимущественно стала использоваться мышь, а не клавиатура. В SmallTalk-80 помимо контроллеров-редакторов (ParagraphEditor и TextEditor) появляется MouseMenuController, который становится основным средством ввода команд. Пользовательские интерфейсы из Command-driven становятся Menu-driven (User Interfaces) Доступные команды для каждого Вида формировались явно в виде списка. А связанный с Видом MouseMenuController предоставлял для их отображения и удобного ввода специальное графическое средство – всплывающие pop-up menu, которые появлялись при нажатии соответствующей кнопки мыши. Вот так они выглядели: Повторю: pop-up menu относились к MouseMenuController. Меню являлись специальным графическим инструментом/средством для удобного отображения и ввода команд (определенных в контексте некоторого вида), который Контроллер, связанный с этим Видом, предоставлял пользователю. Вот что пишет Краснер: \"Хотя меню можно рассматривать как пару вид-контроллер, но чаще всего они считаются входными устройствами и следовательно относятся к сфере контроллера… За создание всплывающих меню при нажатии какой-нибудь кнопки мыши отвечает класс MouseMenuController… По умолчанию PopUpMenus возвращают числовое значение которое вызвавший их контроллер использует для того чтобы определить какое действие ему нужно совершить… Из-за широкого использования всплывающих меню большинство контроллеров пользовательского интерфейса являются подклассами MouseMenuController\". Так что основной контроллер SmallTalk-80 (MouseMenuController) тоже имел свою графическую часть – pop-up menu, и разделение пользовательского интерфейса на Виды и Контроллеры стало выглядеть следующим образом (иллюстрация взята из статьи Гленна Краснера): Меню относящиеся к MouseMenuController-ам можно видеть абсолютно во всех SmallTalk приложениях и примерах. Вот так с развернутыми меню выглядят уже упоминавшиеся Workspace и Inspector: А вот так выглядит Browser, включающий в себя 5 Видов (подВидов) и соответствующих им Контроллеров Предполагаю, что именно из-за широкого использования всплывающих меню Реенскауг и писал, что контроллер в smalltalk \"это эфемерный компонент, который View создает при необходимости в качестве связывающего звена между View и входными устройствами такими как мышь и клавиатура\". Так что, можем развеять очередной миф: Мифы: Все графические элементы пользовательского интерфейса относятся к Виду. Контроллер это исключительно логика обработки движений мыши, нажатий клавиш на клавиатуре и других входящих событий производимых пользователем.На самом деле и в реализации Реенскауга и затем в Smalltalk-80 большинство Контроллеров имели «графическую составляющую, помогающую пользователю вводить команды и данные». И именно такие Контроллеры в основном использовались в пользовательских приложениях. Хотя, конечно же, были Контроллеры и без графической составляющей, но они в основном применялись для более низкоуровневых системных задач (об этом чуть позже). Если просуммировать, то можно сказать что Контроллер это часть пользовательского интерфейса, которая отвечает за то чтобы 1) предоставить пользователю удобные средства для ввода команд и данных, а затем 2) действия пользователя перевести в вызовы соответствующих методов Модели и передать их ей. Вот как определял Контроллер сам Реенскауг: \"Контроллер это связь между пользователем и системой. Он предоставляет пользователю меню и другие средства для ввода команд и данных. Контроллер получает результат таких действий пользователя, транслирует их в соответствующие сообщения и передает эти сообщения\" [ A controller is the link between a user and the system. It provides means for user output by presenting the user with menus or other means of giving commands and data. The controller receives such user output, translates it into the appropriate messages and pass these messages on]. Современные графические интерфейсы (GUI) для ввода команд используют весь спектр доступных средств: текстовые и графические меню, кнопки, всплывающие pop-up меню, всевозможные переключатели (как в реальных приборах), текстовые поля для ввода данных (TextEditor свелся к TextField). Все эти элементы служат в основном для управления, а не для отображения информации. По английски они так и называются — controls (элементы управления). И если продолжить аналогию, то разделение Вид/Контроллер в современных системах выглядело бы примерно следующим образом: Можно сказать, что Контроллер это «панель управления» (Control panel). А Вид это «обзорная панель» или «панель наблюдения за системой» включающая в себя текстовые описания, списки, таблицы, графики, шкалы, световые табло, и всевозможные индикаторы состояния. Красота MVC заключается в том, что его идеи универсальны и применимы не только к информационным системам. Не важно прибор это или программа, в простейшем случае интерфейс, как правило, содержит блок/панель управления, позволяющий вводить команды – Контроллер, и блок отображения информации – Вид. Реализация Видов и Контроллеров Что нам это дает? Ну во первых, становится понятно что логика работы Вида никак не может быть помещена в Контроллер: Если всю логику работы GUI вынести в Контроллер, то это нарушало бы сразу несколько принципов: главный принцип определяющий качество декомпозиции – High Cohesion + Low Coupling, который говорит что «резать» на модули нужно так, чтобы связи, особенно сильные, оставались преимущественно внутри модулей, а не между ними Принцип единственной ответственности (Single responsibility principle). Второе. Тонкий Вид, рассматриваемый исключительно как набор графических элементов, не выполняет никакой функции и поэтому в плане пере-использования мало полезен. Если же мы рассматриваем Вид как полноценный функциональный модуль, решающий довольно общую и востребованную задачу – визуализация и удобное представление данных, то при правильном подходе он становится идеальным кандидатом на пере-использование. В этом смысле разделение пользовательского интерфейса на Вид и Контроллер очень красивый шаг – Контроллер вобрал в себя большую часть зависимостей. Виду от Модели нужны лишь данные для отображения в определенном формате. Соответственно потенциально один и тот же Вид может быть использован для визуализации информации в разных приложениях. Последнее время активно разрабатывается и используется концепция Dashboard-ов (информационных панелей), для которых создаются наборы универсальных виджетов, позволяющих наглядно и удобно визуализировать «все что угодно». В отличие от «тупого вида» такие «полноценные блоки визуализации» (инкапсулирующие свою логику, настройки и способные работать самостоятельно) очень востребованы и ценны сами по себе. Когда Вид и Контроллер, трактуются как функциональные модули, отвечающие за решение определенных задач, то становится понятно, что для того чтобы инкапсулировать свою логику работы им вовсе нет нужды смешивать ее с графикой. Ведь любой модуль, при необходимости, может быть разделен на подмодули и обладать своей внутренней структурой. Как мы выяснили основной Контроллер SmallTalk-80 — MouseMenuController вовсе не являлся «всего лишь обработчиком действий пользователя с мышью и клавиатурой», на самом деле он делал довольно много вещей: Задавал набор и названия команд, доступных пользователю в контексте некоего Вида, Определял логику того, как эти команды транслировать в вызовы соответствующих методов Модели Отображал доступные команды Обрабатывал низкоуровневые движения мыши и создавал высокоуровневые события, к которым удобно привязывать выполнение команд (Event Driven подход). Для таких Контроллеров просто «просилась» MVC архитектура. И в SmallTalk-80 она была использована: \"pop-up menu are implemented as a special kind of MVC class\" (Краснер). Тут важно понимать, что Модель в этом «внутреннем» MVC не имеет никакого отношения к доменной модели, это именно внутренняя вспомогательная модель, описывающая «состояние» самого контроллера (в частности то, какая команда выбрана) и логику изменения этого состояния. Аналогично дело обстоит и с Видом. Мифы: То, что Вид это всего лишь «графика», является такой же идеализацией как и то, что Контроллер это «исключительно логика».Визуализация информации является непростой задачей, для решения которой, как правило, требуются дополнительные данные, характеризующие именно сам процесс отображения. Большинство Видов, используемых в реальных приложениях, это довольно сложные объекты со своим «состоянием» и логикой его изменения. Например, Вид редко когда может отобразить всю Модель целиком, обычно он отражает лишь какую-то ее часть и ему необходимо «знать» какая именно «часть Модели» должна быть отражена в данный момент. Многие виды позволяют «выделять» какие-то элементы, а значить им где-то нужно хранить информацию о выделениях. Где хранились такого рода дополнительные данные и логика их изменения? В принципе ответ очевиден и, если вы помните, Реенскауг ответил на этот вопрос – конечно же в самом Виде. Но! Не в перемешку с графикой, а в отдельном под-модуле/классе/скрипте, то есть в некоторой внутренней модели. И раз есть вспомогательные внутренние модели, то должны быть и специальные Контроллеры, которые этими внутренними моделями управляют. Такие Контроллеры изменяют исключительно состояние самого Вида и не не имеют никакого отношения к Контроллеру приложения, изменяющему состояние доменной модели. Положением фрейма, например управлял ScrollController. То есть в общем случае Виды тоже имеют структуру MVC. Но пока отложим вопрос с Контроллерами и сосредоточится на главном — на внутренних моделях. Фаулер такие внутренние модели, являющиеся частью представления называет — Presentation Model: Presentation Model pulls the state and behavior of the view out into a model class that is part of the presentation. И вот тут внимание! В первой части статьи подробно рассказывается о том, как в MVC был «потерян» Фасад и что из-за этого его роль на себе вынуждены брать другие компоненты. Так вот хотя Фаулер и пишет что \"Presentation Model is not a GUI friendly facade to a specific domain object\" на практике PresentationModel, ViewModel, ApplicationModel не только описывают состояние и поведение представления, но одновременно являются еще и Фасадами к доменной модели. В примере, который Фаулер подробно разбирает, хорошо видно, что его PresentationModel является именно смесью Фасада и модели представления. Ну а Microsoft прямо пишет: \"Presentation model class acts as a façade on the model with UI-specific state and behavior, by encapsulating the access to the model and providing a public interface that is easy to consume from the view\" (MSDN: Presentation Model) Безусловно такой подход противоречит Принципу единой ответственности, но он существует, используется во фреймворках и в принципе работает. Поэтому о нем стоит знать. В отличие от него в Java подобного смешения стараются не допускать. В Java Swing «application-data models» и «GUI-state models» разделены гораздо более четко. Все, наверное, читали или слышали что Java Swing компоненты реализованы в виде MVC. И большинство наверняка уверены в том, что M в этой триаде это тот самый интерфейс к доменным данным. И по сути это правильно, почти… Давайте внимательно посмотрим на список JList. У него действительно есть модель, обеспечивающая доступ к доменным данным – ListModel. Но кроме нее у списка имеется еще одна модель — ListSelectionModel, которая отвечает исключительно за внутреннюю логику выделения элементов списка (item selection). И вот эта модель как раз и является в чистом виде внутренней моделью представления – «GUI-state model»: \"The models provided by Swing fall into two general categories: GUI-state models and application-data models. GUI state models are interfaces that define the visual status of a GUI control, such as whether a button is pressed or armed, or which items are selected in a list. GUI-state models typically are relevant only in the context of a graphical user interface (GUI). An application-data model is an interface that represents some quantifiable data that has meaning primarily in the context of the application, such as the value of a cell in a table or the items displayed in a list. These data models provide a very powerful programming paradigm for Swing programs that need a clean separation between their application data/logic and their GUI\" (см статью от создателей A Swing Architecture Overview). У таблицы JTable помимо application-data модели, обеспечивающей доступ к доменным данным – TableModel, имеются целых две внутренних GUI-stateмодели — ListSelectionModel и TableColumnModel. А вот у кнопки JButton имеется лишь GUI-state модель – ButtonModel. Что в принципе и логично. Кнопка не отображает доменных данных, это в чистом виде Контроллер с внутренней моделью, которая определяет состояние кнопки (нажата/не нажата). При использовании графических компонент нам, в основном, приходится иметь дело с application-data моделями, через которые собственно и осуществляется взаимодействие домена и интерфейса. С GUI-state моделями мы сталкиваемся лишь тогда, когда возникает необходимость изменить дефолтное поведение компонента. Поэтому это нормально и правильно что многие даже не знают о наличие GUI-state моделей. Видно, что в таблице некоторые модели отмечены одновременно и как GUI и как data. Особых пояснений не дается, написано что это зависит от контекста использования модели. Я могу высказать по этому поводу лишь предположение. Все такого рода промежуточные модели относятся к компонентам, которые являются либо разновидностью скроллбара либо разновидностью переключателя (кнопка с состоянием). Такие компоненты предназначены прежде всего для управления и по сути представляют собой контроллер (в SmallTalk скроллбар и был контроллером — ScrollController) но с некоторым внутренним состоянием. Соответственно у таких компонент имеется лишь GUI-state модель и в дефолтной реализации состояние этих компонент никак не связано с состоянием доменной модели, а зависит лишь от действий пользователя, то есть от того, в какое состояние пользователь этот контрол/кнопку перевел. Но при этом выглядит состояние таких контроллеров так, как если бы оно было согласовано с состоянием доменной модели: мы перевели переключатель в состояние «On», в доменную модель передалась соответствующая команда, там что-то включилось… и доменная модель тоже перешла в некое состояние «On». Достигается такая псевдо-согласованность как правило \"сама собой\", автоматически. Тем не менее возможны ситуации когда может понадобиться настоящее реальное согласование доменной модели и состояния переключателя. В этом случае будет написана реализация ButtonModel, в которой метод isSelected() перестанет зависеть от действий пользователя с кнопкой, а вместо этого будет напрямую отображать состояние домена (некий его флаг). Кнопка при этом становится отчасти видом, а ButtonModel перестает быть внутренней GUI-state и становится application-data В заключение темы привожу схемы обоих описанных тут подходов чтобы каждый мог выбрать то, что ему больше подходит: Я стараюсь первого варианта избегать. Мое ИМХО что подобное смешение хоть и соблазняет своей кажущейся простотой но на практике приводит к тому что PresentationModel превращается в большую \"свалку\". Представьте что есть реальный сложный интерфейс. Если использовать подход (2), то мы этот интерфейс разобьем на ui-модули, и каждый модуль будет инкапсулировать свою логику в виде небольшой внутренней ui-state модели. А вот если использовать подход (1), то логика работы всего этого большого интерфейса будет свалена в кучу вперемешку с логикой фасада в PresentationModel. Пока такая PresentationModel или ApplicationModel создается и управляется автоматически неким фреймворком все хорошо. Но если подобное писать ручками… то мозг начинает ломаться и часто это приводит к тому, что логика представления рано или поздно просачивается через фасад в доменную модель. Даже у такого гуру как Фаулер, в его детском примере это таки произошло (интересно, кто-нибудь еще заметил это место?). В архитектуре, где фасад и GUI-state модели разделены, вероятность такого рода ошибок значительно ниже. Кто хочет развлечься, то в том же примере у Фаулера можно найти ненужное копирование данных, о котором писалось в первой части. Объединенный ВидКонтроллер. «Упрощенный MVC» Раз уж мы коснулись Java Swing, то нужно сказать еще об одной его важной особенности – в отличие от SmallTalk-80 где и скроллбар и pop-up меню были реализованы в виде полноценного MVC (с внутренней моделью, внутренним низкоуровневым контроллером и видом) Swing в реализации базовых gui компонент использует «упрощенный MVC», в котором Вид и Контроллер объединены в единый компонент, который одновременно отображает данные и обрабатывает действия пользователя. Обычно он так и называется: объединенный ViewController или UI-object, IU-delegate. Вот еще одна статья, где это подробно описывается: MVC meets Swing. Самое интересное и неожиданное заключается в том, что такой «упрощенный MVC» де-факто используется в большинстве GUI библиотек и фреймворках пришедших на смену SmallTalk-80: VisualAge Smalltalk от IBM, Visual SmallTalk, VisualWorks SmallTalk, MacApp… (Smalltalk, Objects, and Design стр 124-125). Фактически классический вариант MVC, с обязательным разделением Видов и Контроллеров, особенно на низком уровне, только в SmallTalk-80 и был реализован. Почему? Опять таки я могу высказать лишь предположение. Основное отличие SmallTalk-80 от всех последующих систем заключалось в том что он работал без операционной системы. Поэтому весь объем низкоуровневой работы по отслеживанию движений мыши а также нажатий клавиш на клавиатуре приходилось выполнять Контроллеру. Контроллер в тех условиях был необходим, фактически он выполнял роль драйвера входящих устройств и кроме ссылок на Модель и Вид обязательно содержал ссылку на «сенсор». Соответсвенно именно эта сторона его деятельности акцентировалась и выходила на первый план. После того, как эту работу взяли на себя операционные системы, низкоуровневая «обработка действий пользователя» в большинстве случаев становится достаточно простой для того чтобы с ней мог справится сам Вид. И для простых компонент это оказывается плюсом, потому как разделение функций ввода и вывода хорошо работает для интерфейса в целом или для сложных компонент. А в случае создания базовых ui-компонент, как пишут создатели Swing: \"this split didn't work well in practical terms because the view and controller parts of a component required a tight coupling (for example, it was very difficult to write a generic controller that didn't know specifics about the view)\". На самом деле и в SmallTalk-80 Виды и Контроллеры тоже были очень тесно связанными: Контроллер всегда содержал ссылку на Вид, а Вид на Контроллер. Кроме того, соответствующие классы Вида и Контроллера обычно еще и разрабатывались совместно: \"Поскольку классы вида и контроллера часто разрабатывались совместно, для многих подклассов вида был определен класс контроллера, используемого по умолчанию, и метод для получения его экземпляра – defaultControllerClass. И конкретный контроллер связанный с видом часто создавался автоматически просто как экземпляр такого класса\" [Because view and controller classes are often designed in consort, a view's controller is often simply initialized to an instance of the corresponding controller class. To support this, the message defaultControllerClass, that returns the class of the appropriate controller, is defined in many of the subclasses of View – Glenn Krasner ]. Что еще делал Контроллер? Помимо обработки низкоуровневых действий пользователя и создания высокоуровневых событий, Контроллер также содержал \"логику перевода этих высокоуровневых событий в соответствующие методы Модели\". Но как мы выяснили в первой части статьи, Моделями в SmallTalk-80 являлись не сами доменные объекты а интерфейсы и фасады к ним, причем клиент ориентированные. Такие Модели-фасады изначально формировались (были заточена) под требования клиента, так что команды, которые Контроллер отображал в popup menu, практически всегда однозначно соответствовали методам Модели. Вот что пишет Краснер: \"В конце концов сообщения контроллера почти всегда напрямую передавались модели; это означает что в ответ на выбор пункта меню «aMessage» контроллеру посылалось сообщение aMessage и в результате почти всегда вызывался метод модели, который так и назывался «aMessage».\" [Finally, the controller messages were almost always passed directly on to the model; that is, the method for message aMessage, which was sent to the controller when the menu item aMessage was selected, was almost always implemented as ↑model aMessage]. В конце своей статьи Краснер приводит несколько примеров из которых видно, что Контроллеры приложений были «пустыми» и не содержали никакой логики. Их создание сводилось к тому что нужно было указать названия команд, отображаемых в pop-up меню, а затем для каждого названия указать метод модели, который должен был вызываться. По сути это означает, что вся логика находилась в моделях: бизнес-логика – в доменной модели, логика перевода команд пользователя в команды системы – в моделях-фасадах, логика работы самого GUI – в ui-state моделях. От Контроллера требовалось лишь связать вызовы методов этих моделей с соответствующими высокоуровневыми событиями (нажатие кнопки, выбор пункта меню) а это настолько тривиально что и эту функцию тоже легко может выполнить сам Вид. Следующий важный шаг заключается в том, что в современных GUI библиотеках исчезла и существовавшая в smallTalk-80 классификация самих базовых ui-компонент по типу Вид или Контроллер. Как выяснилось далеко не все графические элементы были Видами: pop-up меню относились к Контроллеру, скроллбар и TextEdit просто были Контроллерами. Сейчас такое разделение не делается и мы имеем «ui-компоненты» или «виджеты», которые изначально проектируются так, чтобы быть универсальными – они одновременно могут отображать информацию и создавать высокоуровневые события, к которым удобно привязывать выполнение команд. И таким образом могут играть роль Вида или Контроллера в зависимости от контекста. Дело в том, что многие ui-компоненты оказались похожи на ParagraphEditor, о котором писал Стив Барбек – они одновременно могут отображать информацию и позволяют ее изменять, объединяя в себе функции Вида и Контроллера. Такими интерактивными элементами являются текстовые поля, формы, всевозможные переключатели (toggle button, radio button, check box) и аналоги скроллбара... Для таких компонент грань Вид или Контроллер оказывается размытой. Один и тот же элемент (объект) может быть Видом или Контроллером в зависимости от контекста в котором он используется и от того какую функцию он в данный момент выполняет. Список, используемый для отображения и ввода команд, являлся частью Контроллера (popUpMenu), а тот же список используемый для отображения данных – частью Вида (ListView). Аналогично — текстовое поле. Как напишет Реенскауг в своих более поздних работах: \"Модель, Вид и Контроллер это на самом деле роли, которые могут исполняться объектами\" (Model, View and Controller are actually roles that can be played by the objects… – The DCI Architecture: A New Vision of Object-Oriented Programming). То, что в SmallTalk-80 сами объекты пытались поделить на Модели, Виды и Контроллеры, вызывало лишь ненужную путаницу, которая в частности проявлялось в том, что TextView для отображения посылал текст своему Контроллеру. Так осталось ли что нибудь от Контроллера? На мой взгляд да. То, что команды в систему вводятся \"с помощью мыши и клавиатуры\" это ведь тоже своего рода миф. В действительности Контроллер всегда предоставлял пользователю некие средства (как правило графические) помогающие вводить команды. Сначала это были текстовые редакторы, отображающие нажатые пользователем клавиши, затем pop-up меню, отображающие список доступных команд и дающие возможность вводить их «в один клик». Современные gui-библиотеки предоставляют уже целый арсенал средств — кнопки, переключатели, текстовые и графические меню, слайдеры… И вот эта часть работы Контроллера – поиск все более наглядных, удобных и «интуитивно понятных» средств/форм для ввода команд и управления системами, продолжает быть актуальной. И каждый раз когда мы разрабатываем интерфейс или ui-компонент мы явно или неявно ее решаем. Интерфейс как Composite Еще одна важная идея, которая безусловно осталась – это понимание того, что интерфейс нужно делить на модули, а не делать единым блоком или страницей. Как видно из примеров, большинство интерфейсов были составными и включали в себя множество Видов и Контроллеров. Для обозначения подобных сложно-составных Интерфейсов Реенскауг в своей второй более поздней работе использует специальный термин \"Tool\" (инструмент пользователя) и у него этой теме посвящен отдельный раздел, который так и называется “Tool as a Composite”. Замечание: обратите внимание на подпись на рисунке Реенскауга – \"User in the driver's seat\". Если уж рассматривать MVC на избитом примере машины или самолета, то довольно наивно ассоциировать Вид с их дизайном и внешним видом. Пользовательский интерфейс машины это прежде всего водительское место, включающее в себя: приборную панель со спидометром и прочими шкалами, отражающими состояние машины и то, что с ней происходит (именно отсюда и был заимствован термин «Dashboard» для информационных панелей) – Вид; рычаги/элементы управления машиной, такие как руль, педали газа и тормоза, переключатель скоростей и т.п. – Контроллер. Ну а доменной моделью машины будет все то, что составляет ее основу и позволяет ей ездить. На рисунке в качестве Tool приведен уже знакомый нам интерфейс из первого доклада Реенскауга состоящий из трех блоков (которые Реенскауг называет Editor). И вот что по этому поводу пишет Реенскауг: The Model that is responsible for representing state, structure, and behavior of the user’s mental model. One or more Editors that present relevant information in a suitable way and support the editing of this information when applicable. A Tool that sets up the Editors and coordinates their operation. (E.g., the selection of a model object that is visible in several Editors). Complex Editors may again be subdivided into a View and a Controller.This solution is a composite pattern. Как видите, шаблон Composite у Реенскауга относится ко всему интерфейсу, а вовсе не к Виду. Откуда же взялась идея что Composite в MVC относится исключительно к Виду? Все просто – поскольку в SmallTalk-80 с каждым Видом обязательно был связан свой Контроллер, то в явном виде композиция там действительно задавалась только для Видов, а соответствующая композиция (иерархия) Контроллеров просто «вычислялась»: \"Since each view is associated with a unique controller, the view/subView tree induces a parallel controller tree within each topView\". Это решение было не особо удачным и приводило к хрупкости системы – стоило какой-нибудь Вид оставить без Контроллера и вся система рушилась. Поэтому в SmallTalk-80 был придуман «костыль» – Контроллер, который назывался «NoController». Этот контроллер ничего не делал и по умолчанию связывался с Видами, которые “исключительно отображали информацию” и не нуждались в контроллере. Его единственное назначение состояло в том, чтобы цепочка контроллеров, соответствующих Видам, не прерывалась. (подробно можно почитать У Стива Барбека в разделе “Communication Between Controllers”). Так что в действительности и в SmallTalk-80, и у Реенскауга, пользовательский интерфейс всегда включал в себя не только композицию Видов но и точно такую же композицию соответствующих им Контроллеров. И если уж говорить о шаблоне Composite, то, конечно же, более корректно относить его не к Виду а ко всему пользовательскому интерфейсу, как это сделано у Реенскауга и как это делается в современных GUI библиотеках. Интерфейсы больших приложений делятся на «gui-компоненты» или виджеты, которые в свою очередь могут делиться на более простые компоненты и образовывать древовидную структуру. Каждый такой gui-компонент является автономным модулем, который одновременно отображает некую информацию и обрабатывает относящиеся к нему действия пользователя, порождая высокоуровневые события к которым удобно привязывать выполнение команд (View+Controller). Также он инкапсулирует свою логику работы как правило в виде внутренней GUI-state модели. То есть по сути представляет собой MVC (вернее «упрощенный MVC»). И вот такие полноценные gui-компоненты уже действительно можно разрабатывать параллельно и независимо, а также переиспользовать Следствием шаблона Composite и того, что каждый gui-компонент может быть реализован в виде небольшого MVC является некая иерархичность или рекурсивность MVC. Но из-за того что об этом редко пишут, аналоги этой идеи разработчики вынуждены пере-открывать. Вот известная статья на эту тему – Hierarchical model–view–controller и интересная дискуссия – Recursive Model View Controller. А вот картинка из статьи: В реальных же проектах непонимание этого аспекта приводит к следующим крайностям: Сделать весь проект в виде одного большого MVC (разбив его на три «класса») Сначала проект разбивается на страницы а затем каждая страница реализуется в виде одного средних размеров MVC (некрасивое но жизнеспособное решение) Все приложение сходу бьется на сотни крошечных MVC (соответствующих каждому графическому компоненту в интерфейсе) Хотя все что нужно, это разбивать приложение на функциональные модули иерархически. Сначала исходное приложение делится на доменную модель (функциональное ядро приложения) и пользовательский интерфейс. Связь между этими двумя модулями ослабляется за счет использования шаблонов Фасад и Наблюдатель. А затем сам пользовательский интерфейс делится модули. А эти модули, в свою очередь, на более мелкие подмодули и тд. Причем любой ui-модуль на любом уровне иерархии может быть реализован как угодно и иметь любую структуру, но чаще всего реализуется тоже в виде MVC. На практике такой подход к построению интерфейсов из независимых полноценных ui-компонент активно использует и развивает ebay. Подробно об этом можно почитать в их замечательной статье Don’t Build Pages, Build Modules: \"Когда дело касается view люди все еще мыслят страницами вместо того чтобы строить UI модули. Мы обнаружили что с ростом сложности страниц их становится экспоненциально сложнее поддерживать. Что мы хотим это разделить страницу на маленькие управляемые части, каждую из которых можно разрабатывать независимо. Мы хотим уйти от идеи непосредственно строить страницы. Вместо этого мы разбиваем страницу на логические UI модули и делаем это рекурсивно до тех пор пока модуль не станет FIRST. Это означает что страница строится из высокоуровневых модулей, которые в свою очередь строятся из подмодулей\". Суммируя Я вовсе не хочу сказать что «original MVC» единственно правильный. Моя цель состояла лишь в том, чтобы показать, что он был намного сложнее и богаче чем те упрощенные схемки, которые нам обычно преподносятся в качестве MVC. Создавать на их основе реальные приложения это все равно что строить самолет на основе схем из детского конструктора и удивляться что он не летает. С другой стороны, если рассматривать MVC не как схему, а прежде всего, как набор архитектурных идей, то он действительно становится прост, логичен и очень понятен, и буквально выводится из этих идей. И когда есть понимание, что же именно делается, с помощью каких «инструментов», ради чего, то тогда MVC перестает быть «догмой» и его можно варьировать в зависимости от потребностей конкретного проекта. Да и термины становятся не так важны. Когда я слышу или читаю про фронт-контроллер или что \"контроллер это единая точка входа в систему\", то мне понятно что термином Контроллер у этих ребят называется Фасад. И это вовсе не означает что их архитектура неправильная. Наоборот, как минимум хорошо, что у них Фасад вообще есть, ну а дальше нужно смотреть, как он реализован, не оттягивает ли на себя реализацию бизнес логики и тп Когда говорится, что Контроллер это бизнес-логика, то и это не страшно… Просто принимаешь что в этом случае Контроллером называется доменная модель. Важно не то, как она называется, а то как она реализована и отделена ли от пользовательского интерфейса. Часто термин Вид используют как синоним термина Пользовательский Интерфейс. Так тоже \"можно\". Лишь бы он при необходимости был грамотно разбит на модули и не трактовался исключительно как графика. Дело ведь не в терминах, а в сути. Мне кажется, что корень большинства проблем заключается в том, что как раз о сути MVC мало кто пишет. Вместо этого термины Модель, Вид и Контроллер вырываются из контекста архитектурных идей, им даются какие-то формальные определения, а затем они нередко применяются для обозначения модулей в некачественной декомпозиции, навешивается шаблон Наблюдатель и все это преподносится под «брендом MVC». Как писал Вирт: \"Самой трудной проектной задачей является нахождение наиболее адекватной декомпозиции системы на иерархически выстроенные модули, с минимизацией функций и дублирования кода\". Поэтому основная мысль, которую мне хотелось донести, заключается в том, что MVC он не про Модель, Вид и Контроллер, не про то как они связаны между собой и не про шаблон Наблюдатель. MVC он про то, как нужно грамотно разбивать систему на функционально осмысленные модули, слабо связанные друг с другом и полезные сами по себе. Модули, которые на самом деле можно разрабатывать и использовать независимо, переиспользовать… Что собственно и является основой любой хорошей архитектуры. А Модель, Вид и Контроллер это всего лишь результат начальной декомпозиции, предложенной нам талантливыми людьми. Также как паттерны Фасад, Наблюдатель, Компоновщик — это просто инструменты, которые ими были использованы для ослабления связанности и уменьшения сложности. Так что не бойтесь думать самостоятельно, не верьте «брендам» и анализируйте предлагаемые рынком решения, хотя бы грубо, на соответствие принципам. Архитектурные принципы не всегда говорят, как нужно делать, но часто хорошо детектируют то, как делать не нужно. Спасибо всем кто «дотянул» до конца. Будем очень признательны за обоснованную критику. Особенно интересно мнение людей близко знакомых со SmallTalk."], "hab": ["Проектирование и рефакторинг", "Анализ и проектирование систем"]}{"url": "https://habrahabr.ru/post/322424/", "title": ["Серия видеоуроков по Git для новичков", "tutorial"], "text": ["Скорее всего, если вас привлекло название статьи, то вы начинаете свой путь знакомства с системой контроля версий Git. В данной статье я приведу 10+ видео о пошаговом вхождении в контроль версии используя Git. Данного курса будет вполне чем достаточно для работы с такими популярными сервисами как GitHub и Bitbucket. Однажды мой знакомый, который только начинал свой путь в ИТ кинул мне данный мемчик что слева, с вопросом \"А чем плохо то?\", поэтому чтобы понимать данную шутку и уметь работать с самым популярным на сегодня VCS (Version Control System) рекомендую к ознакомлению серии видеоуроков, которую я привел ниже. Прежде хочу сказать, что серия по Git не завершена и новые видео активно публикуются каждую неделю. Для тех кто желает следить за серией прошу перейти в плейлист по Git куда добавляются новые видео. Содержание: Урок 0. Подготовка и Введение Урок 1. Первый коммит Урок 2. Проверка состояния Урок 3. Индексация файлов Урок 4. История коммитов Урок 5. Git checkout - Назад в будущее Урок 6. Отмена индексированных файлов Урок 7. Revert - Отмена коммита Урок 8. Решение простого конфликта Урок 9. Ветки и их применение Урок 10. Слияние веток и решение конфликтов слияния Урок 11. Rebase vs. Merge - Что такое git rebase? Очень надеюсь данная серия видео кому-то поможет изучить Git либо улучшить его понимание. Приятного изучения!"], "hab": ["GitHub", "Git"]}{"url": "https://habrahabr.ru/post/322656/", "title": ["Как я писал компилятор С++. Пересказ спустя 15 лет"], "text": ["15 лет назад не было Хабрахабра, не было фейсбука, и что характерно, не было компилятора С++, с выводом диагностических сообщений на русском. С тех пор, вышло несколько новых стандартов С++, технологии разработки сделали гигантский скачок, а для написания своего языка программирования или анализатора кода может потребоваться в разы меньше времени, используя существующие фреймворки. Пост о том, как я начинал свою карьеру и путем самообразования и написания компилятора С++, пришел к экспертному уровню. Общие детали реализации, сколько времени это заняло, что получилось в итоге и смысл затеи — тоже внутри. С чего все начиналось В далеком 2001-ом году, когда мне купили первый компьютер Duron 800mhz/128mb ram/40gb hdd, я стремительно взялся за изучение программирования. Хотя нет, сначала меня постоянно мучил вопрос, что же поставить Red Hat Linux, FreeBSD или Windows 98/Me? Ориентиром в этом бесконечном мире технологий для меня служил журнал Хакер. Старый такой, стебный журнал. К слову, с тех пор, стиль изложения в этом издании почти не поменялся. Виндузятники, ламеры, трояны, элита, линух — вот это все сносило крышу. Реально хотелось поскорей освоить этот весь стек, которые они там печатали и хакнуть Пентагон (без интернета). Внутренняя борьба за то, становиться ли Линуксоидом или рубится в игры на винде продолжалась до тех пор, пока в дом не провели интернет. Модемный, скрежечащий 56kb/s интернет, который занимал телефон на время подключения, и качал mp3-песню в районе получаса. При цене порядка 0.1$/mb, одна песня вытягивала на 40-50 центов. Это днем. А вот ночью, были совсем другие расценки. Можно было с 23.00 до 6.00 залипать во все сайты не отключая изображения в браузере! Поэтому все что можно было скачать из сети за ночь, качалось на винт, и далее прочитывалось уже днем. В первый день, когда мне домой провели и настроили сеть, админ передо мной открыл IE 5 и Яндекс. И быстро ретировался. Думая, что же первым делом искать в сети, я набрал что-то вроде «сайт для программистов». На что первой ссылкой в выдаче выпал совсем недавно открывшийся rsdn.ru. И на нем я стал зависать продолжительное время, испытывая чувство неудовлетворенности, от того, что мало что понимаю. На то время флагманом и самым популярным языком на форуме (да и вообще) был С++. Поэтому вызов был брошен, и ничего не оставалось, как догонять бородатых дядек в их знаниях по С++. А еще был не менее интересный сайт на то время — firststeps.ru. Я до сих пор считаю их метод подачи материала наилучшим. Маленькими порциями (шагами), с небольшими конечными результатами. Тем не менее все получалось! Активно скупая книги на барахолке, я стремился постичь все азы программирования. Одной из первых купленных книг было «Искусство программирования» — Д. Кнут. Не помню точную мотивацию купить именно эту книгу, а не какой-нибудь С++ для кофейников, наверное продавец порекомендовал, но я со всем своим усердием школьника взялся за изучение первого тома, с обязательным выполнением задач в конце каждой главы. Это была самая мякотка, и хотя с математикой у меня в школе не ладилось, но зато с мат.аном Кнута прогресс был, потому что было огромное желание и мотивация писать программы и делать это правильно. Осилив алгоритмы и структуры данных, я купил уже 3-ий том «Искусства программирования» Сортировка и поиск. Это была бомба. Пирамидальная сортировка, быстрая сортировка, бинарный поиск, деревья и списки, стеки и очереди. Все это я записывал на листочке, интерпретируя результат в своей голове. Читал дома, читал когда был на море, читал везде. Одна сплошная теория, без реализации. При этом я даже не догадывался, какую огромную пользу принесут эти базовые знания в будущем. Сейчас, проводя собеседования с разработчиками, мне еще не встретился человек, который смог бы написать реализацию бинарного поиска или быстрой сортировки на листочке. Жаль. Но вернемся к теме поста. Осилив Кнута, надо было двигаться дальше. Попутно я сходил на курсы Turbo Pascal, прочитал Кернигана и Ритчи, а за ними С++ за 21 день. Из С и С++, мне было не все понятно, я просто брал и переписывал тексты из книг. Загуглить или спросить было не у кого, но зато времени было вагон, так как школу я забросил и перешел в вечернюю, в которую можно было практически не ходить, или появляться на 3-4 урока в неделю. В итоге с утра до ночи, я фанатично развивался, познавая все новые и новые темы. Мог написать калькулятор, мог написать простое приложение на WinApi. На Delphi 6 тоже получалось что-то нашлепать. В итоге, получив диплом о среднем образовании, я уже был подготовлен на уровне 3-4 курса университета, и разумеется на какую специальность идти учится вопроса не стояло. Поступив на кафедру Компьютерных систем и сетей, я уже свободно писал на С и С++ задачи любого уровня сложности университета. Хотя, зайдя на тот же rsdn.ru, понимал, как много еще нужно изучить и насколько бывалые форумчане прокаченней меня в плюсах. Это задевало, непонимание и вместе с тем жгучее желание знать все, привело меня к книге «Компиляторы. Инструменты. Методы. Технологии» — А.Ахо, Рави Сети. В простонародье именуемой книгой Дракона. Вот тут и началось самое интересное. Перед этой книгой, был прочитан Герберт Шилдт, Теория и практика С++, в которой он раскрывал продвинутые темы разработки, такие как шифрование, сжатие данных, и самое интересное — написание собственного парсера. Начав скрупулезно изучать книгу дракона, двигаясь от лексического анализа, затем к синтаксическому и наконец к проверке семантики и генерации кода, ко мне пришло судьбоносное решение — написать свой компилятор С++. — А почему бы и нет, спросил себя? — А давай, ответила та часть мозга, которая с возрастом становится все скептичней ко всему новому. И разработка компилятора началась. Подготовка Модемный интернет к тому времени мне перекрыли, в силу смены телефонных линий на цифровые, поэтому для ориентира был скачан стандарт ISO C++ редакции 1998 года. Уже полюбившимся и привычным инструментом стала Visual C++ 6.0. И по сути задача свелась к тому, чтобы реализовать то, что написано в стандарте С++. Подспорьем в разработке компилятора была книга дракона. А отправной точкой, был парсер-калькулятор из книги Шилдта. Все части пазла собрались воедино и разработка началась. Препроцессор nrcpp\\KPP_1.1\\ Во 2-ой главе в стандарте ISO C++ 98 идут требования к препроцессору и лексические конвенции (lexical conventions). Вот и славно, подумал я, ведь это наиболее простая часть и может реализоваться отдельно от самого компилятора. Другими словами, сначала запускается препроцессинг файла, на вход которому поступает С++ файл в том виде, котором вы привыкли его видеть. А после препроцессинга, на выходе мы имеем преобразованный С++ файл, но уже без комментариев, подставленными файлами из #include, подставленными макросами из #define, сохраненными #pragma и обработанной условной компиляцией #if/#ifdef/#endif. До препроцессинга:#define MAX(a, b) \\ ((a) > (b) ? a : b) #define STR(s) #s /* This is the entry point of program */ int main() { printf(\"%s: %d\", STR(This is a string), MAX(4, 5)); } После препроцессинга:int main() { printf(\"%s: %d\", \"This is a string\", ((4) > (5) ? 4 : 5)); } В довесок, препроцессор делал еще много полезной работы, вроде вычисления константных выражений, конкатенации строковых литералов, вывода #warning и #error. Ах да, вы когда нибудь видели в С-коде Диграфы и триграфы? Если нет, знайте — они существуют! Пример триграфов и диграфовint a<:10:>; // эквивалент int a[10]; if (x != 0) <% %> // эквивалент if (x != 0) { } // Пример триграфа ??=define arraycheck(a,b) a??(b??) ??!??! b??(a??) // проеобразуется в #define arraycheck(a,b) a[b] || b[a] Подробнее в вики. Разумеется, основной пользой от препроцессора С++, является подстановка макросов и вставка файлов обозначенных в #include. Чему я научился в процессе написания препроссора С++? Как устроена лексика и синтаксис языка Приоритеты операторов С++. И в целом как вычисляются выражения Строки, символы, контанты, постфиксы констант Структура кода В целом, на написание препроцессора ушло порядка месяца. Не слишком сложно, но и нетривиальная задача, тем не менее. В это время, мои одногруппники пытались написать первый «Hello, world!», да хотя бы собрать его. Далеко не у всех получалось. А меня ждали следующие разделы стандарта С++, с уже непосредственной реализацией компилятора языка. Лексический анализатор nrcpp/LexicalAnalyzer.cpp Тут все просто, основную часть анализа лексики я уже написал в препроцессоре. Задача лексического анализатора — разобрать код на лексемы или токены, которые уже будет анализироваться синтаксическим анализатором. Что было написано на этом этапе? Конечный автомат для анализа целочисленных, вещественных и символьных констант. Думаете это просто? Впрочем просто, когда ты это прошел. Конечный автомат для анализа строковых литеров Разбор имен переменных и ключевых слов С++ Что-то еще, как пить дать. Вспомню допишу Синтаксический анализатор nrcpp/Parser.cpp Задача синтаксического анализатора — проверить правильность расстановки лексем, который были получены на этапы лексического анализа. За основу синтаксического анализатора были взяты опять же простенький парсер из Шилдта, прокаченный до уровня синтаксиса С++, с проверкой переполнения стека. Если мы например напишем: (((((((((((((((((((((((((((((0))))))))))))))))))))))))))))))))); // кол-во скобок может быть больше То мой рекурсивный анализатор съест стэк, и выдаст, что выражение слишком сложное. У внимательного читателя, может возникнуть вопрос. А зачем изобретать велосипед, ведь был же yacc и lex. Да, был. Но на том этапе, хотелся велосипед с полным контролем над кодом. Разумеется в производительности он уступал сгенерированному этими утилитами коду. Но не в этом была цель — техническое совершенство. Цель была — понять все. Семантика nrcpp/Checker.cpp nrcpp/Coordinator.cpp nrcpp/Overload.cpp Занимает соотвественно главы с 3-ей по 14-ую стандарта ISO C++ 98. Эта наиболее сложная часть, и я уверен, что >90% С++ разработчиков не знает всех правил описанных в этих разделах. Например: Знали ли вы, что функцию можно объявлять дважды, таким образом: void f(int x, int y = 7); void f(int x = 5, int y); Есть такие конструкции для указателей: const volatile int *const volatile *const p; А это указатель на функцию-член класса X: void (X::*mf)(int &) Это первое, что пришло в голову. Стоит ли говорить, что при тестировании кода из стандарта в Visual C++ 6, я не редко получал Internal Compiler Error. Разработка анализатора семантики языка заняла у меня 1.5 года, или полтора курса универа. За это время меня чуть не выгнали, по другим предметам кроме программирования, за счастье получалась тройка (ну, ок четверка), а компилятор тем временем разрабатывался и обрастал функционалом. Генератор кода nrcpp/Translator.cpp На этом этапе, когда энтузиазм немного начал угасать, уже имеем вполне рабочую версию фронт-енд компилятора. Что дальше делать с этим фронт-ендом, разработчик решает сам. Можно распространять его в таком виде, можно использовать для написания анализатора кода, можно использовать для создания своего конвертера вроде С++ -> C#, или C++ -> C. На этом этапе у нас есть провалидированное синтаксически и семантически AST (abstract syntax tree). И на этом этапе разработчик компилятора понимает, что он постиг дзен, достиг просветления, может неглядя понять почему код работает именно таким образом. Для добивания своей цели, создания компилятора С++, я решил закончить на генерации С-кода, который затем можно было бы конвертировать в любой существующий ассемблерный язык или подавать на вход существующим Сишным компиляторам (как делал Страуструп в первых версиях «С с классами»). Чего нет в nrcpp? Шаблоны (templates). Шаблоны С++, эта такая хитровымудренная система с точки зрения реализации, что мне пришлось признать, без вмешательства в синтаксический анализатор и смешивания его с семантикой — шаблоны должным образом работать не будут. namespace std. Стандартную библиотеку без шаблонов не напишешь. Да впрочем и заняло бы это еще много-много месяцев, так как занимает львиную долю стандарта. Внутренние ошибки компилятора. Если вы будете играться с кодом, то сможете увидеть сообщения вроде: внутренняя ошибка компилятора: in.txt(20, 14): «theApp.IsDiagnostic()» --> (Translator.h, 484) Это либо не реализованный функционал, либо не учтенные семантические правила. Зачем писать свой велосипед? А в заключении хочу отметить то, ради чего писалась этот пост. Написание своего велосипеда, даже если на это потрачено 2 с лишним года, кормит меня до сих пор. Это бесценные знания, база, которая будет с Вами на протяжении всей карьеры разработчика. Будут меняться технологии, фреймворки, выходить новые языки — но фундамент в них будет заложен из прошлого. И на их понимание и освоение уйдет совсем немного времени. github.com/nrcpp/nrcpp — исходники компилятора. Можно играться правя файл in.txt и смотреть вывод в out.txt. github.com/nrcpp/nrcpp/tree/master/KPP_1.1 — исходники препроцессора. Собирается с помощью Visual C++ 6."], "hab": ["Программирование", "Компиляторы", "C++"]}{"url": "https://habrahabr.ru/post/320988/", "title": ["Gitlab «лежит», база уничтожена (восстанавливается)"], "text": ["Вчера, 31 января, сервис Gitlab случайно уничтожил свою продакшн базу данных (сами гит-репозитории не пострадали). Дело было примерно так. По какой-то причине стала отставать hot-standby реплика базы (PostgreSQL) (реплика была единственная). Сотрудник gitlab какое-то время пытался повлиять на ситуацию различными настройками и т.д, потом решил всё стереть и налить реплику заново. Пытался стереть папку с данными на реплике, но перепутал сервера и стёр на мастере (сделал rm -rf на db1.cluster.gitlab.com вместо db2.cluster.gitlab.com). Интересно, что в системе было 5 разных видов бекапов/реплик, и ничего из этого не сработало. Был лишь LVM snapshot, сделанный случайно за 6 часов до падения. Вот, привожу сокращенную цитату из их документа. Обнаруженные проблемы: 1) LVM snapshots are by default only taken once every 24 hours. 2) Regular backups seem to also only be taken once per 24 hours, though YP has not yet been able to figure out where they are stored. 3) Disk snapshots in Azure are enabled for the NFS server, but not for the DB servers. 4) The synchronisation process removes webhooks once it has synchronised data to staging. Unless we can pull these from a regular backup from the past 24 hours they will be lost 5) The replication procedure is super fragile, prone to error, relies on a handful of random shell scripts, and is badly documented 6) Our backups to S3 apparently don’t work either: the bucket is empty 7) We don’t have solid alerting/paging for when backups fails, we are seeing this in the dev host too now. Таким образом, делают вывод gitlab, из 5 бекапов/техник репликации ничего не сработало надежно и как надо => поэтому идет восстановление из случайно сделанного 6-часового бекапа → Вот полный текст документа"], "hab": ["Системное администрирование", "Администрирование баз данных", "DevOps"]}{"url": "https://habrahabr.ru/post/322272/", "title": ["Как накрутить 40к просмотров на Хабрахабр. Баг или фича?"], "text": ["Всем доброго времени суток, скриншот выше сделан как раз перед публикацией статьи, о нём сегодня и пойдёт речь. В процессе создания и публикации статей на Хабре, я заметил одну очень интересную особенность работы счетчика просмотров. Заключалась она в том, что каждый раз при любом редактировании статьи, которая ещё не опубликована и сохранена как черновик, счётчик каждый раз увеличивается на +1. Получалось, что к примеру к моменту публикации, статья уже могла иметь от 1 до N просмотров. Я решил проверить свою догадку, и создал тестовую статью, которую сохранил как черновик: Вносим несколько изменений, каждый раз сохраняя статью, чтобы удостовериться в том, что счетчик просмотров действительно увеличивается: Хорошо, а что если создать скрипт, который будет делать тоже самое, но без участия пользователя? Наиболее простым вариантом тут было бы использовать JavaScript и запустить исполнение прямо в браузере. Скачав плагин Tampermonkey, я набросал в нём небольшой скрипт: // ==UserScript== // @name New Userscript // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match https://habrahabr.ru/* // @grant none // ==/UserScript== var postID = 322272; (function() { 'use strict'; // Your code here... setInterval(fakeEdit, 1000); })(); function fakeEdit() { if (location.href.indexOf('post/' + postID.toString()) > 0) location.href = 'https://habrahabr.ru/topic/edit/' + postID.toString() + '/'; else { text = document.getElementById('text_textarea'); text.value = Math.random().toString(36).substring(2) +'\\n'+ Math.random().toString(36).substring(2); to_draft = document.getElementsByName('draft')[0]; to_draft.click(); } } Что тут происходит: Мы запускаем бесконечный цикл с интервалом итерации в 1 секунду, цикл в свою очередь выполняет функцию fakeEdit Функция fakeEdit проверяет текущий адрес страницы: 2.1. если на данный момент это страница редактирования, то мы изменяем содержимое поля text_textarea, в котором как раз расположен текст статьи, затем имитирует сохранение, путём клика по кнопке «В черновики»; 2.2. если адрес текущей страницы содержит post, то переходим к редактированию статьи Таймаут тут нужен, для того, чтобы после загрузки страницы, все элементы успели прогрузиться. Запускаем и оставляем его на несколько дней. В результате через небольшой промежуток времени получаем примерно вот такой результат: Я не считаю описанное выше мной — уязвимостью, но всё же перед публикацией этой статьи, уведомил администрацию Хабра о таком нестандартном поведении счётчика, и вот их ответ: Здравствуйте! Приносим извинения за задержку с ответом. Счетчик просмотров, действительно, считает не только уникальные просмотры (собственно, как и подобные счетчики на большинстве ресурсов в сети Интернет). До вашего обращения нам не приходило в голову рассматривать это как уязвимость, ведь злоупотребить этим в нашем сообществе довольно трудно: если плохой материал попадет в «самое читаемое», то привлечет внимание большого числа пользователей, которые, в свою очередь, «сольют» рейтинг материала и карму автора, так что он сам себя накажет, а если попадет хороший, то и не жалко. Конечно каждый решает сам, использовать полученные знания или нет, но главное помнить, что у всего есть последствия. Я решил остановить скрипт на 40000 просмотрах, но вопрос о том есть ли предел, всё ещё остаётся, а так же что произойдёт при превышении этого передела?"], "hab": ["Тестирование веб-сервисов"]}{"url": "https://habrahabr.ru/post/322598/", "title": ["Анализ исходного кода движка Doom: рендеринг", "перевод"], "text": ["От экрана дизайнера к экрану игрока Карты разрабатывались дизайнером уровней в 2D с помощью редактора Doom Editor (DoomED). LINEDEFS описывали замкнутые секторы (SECTORS в исходном коде), а третье измерение (высота) указывалась посекторно. Первый уровень Doom E1M1 выглядит так: После завершения работы над картой она нарезается методом двоичного разбиения пространства (Binary Space Partitioning, BSP). LINEDEF рекурсивно выбирались и их плоскости превращались в секущие плоскости. То есть LINEDEF разрезались на сегменты (SEGS) до тех пор, пока не оставались только выпуклые подсектора (SSECTOR в коде). Интересный факт: И DoomED, и iBSP писались на… Objective-C на рабочих станциях NextStep. Пятнадцать лет спустя тот же язык почти в той же операционной системе выполняет игру на мобильном устройстве! [прим. пер.: в 2010 году Doom вышел на iPhone] Я немного поработал веб-археологом и мне удалось найти исходный код idbsp. На него стоит посмотреть. Ниже представлен пример рекурсивного разделения карты первого уровня. Уровень рекурсии 1 Синим отмечена выбранная стена, превращённая в секущую плоскость (красная). Секущая плоскость выбиралась таким образом, чтобы сбалансировать BSP-дерево, а также для ограничения количества создаваемых SEGS. Зелёные ограничивающие прямоугольники использовались позже для отбрасывания целых фрагментов карты. Уровень рекурсии 2 (только для правого подпространства) В результате секторы SECTORS разделялись на выпуклые подсекторы (обозначаемые как SSECTORS), а LINEDEFS разрезались на сегменты (обозначаемые как SEGS): Процесс работы в целом Вот как выглядит главный метод рендеринга (R_RenderPlayerView): void R_RenderPlayerView (player_t* player) { [..] R_RenderBSPNode (numnodes-1); R_DrawPlanes (); R_DrawMasked (); } Здесь выполняется четыре операции: R_RenderBSPNode: все подсекторы на карте сортируются с помощью BSP-дерева. Большие фрагменты отбрасываются с помощью ограничивающих прямоугольников (показаны на предыдущем изображении зелёным). R_RenderBSPNode: видимые сегменты SEGS проецируются на экран через таблицу поиска и обрезаются с помощью массива отсечения. Стены рисуются как столбцы пикселей. Размер столбца определяется по расстоянию от точки обзора игрока, позиция Y столбца через высоту связана с игроком. Основания и вершины стен создают плоскости visplanes. Эти структуры используются для рендеринга пола и потолка (они называются в коде flats). R_DrawPlanes: Плоскости visplanes преобразуются из столбцов пикселей в строки пикселей и рендерятся на экране. R_DrawMasked: Выполняется рендеринг «предметов» (врагов, объектов и прозрачных стен). Сортировка двоичного разбиения пространства Два примера с E1M1 (первой картой Doom) и BSP выглядят следующим образом: //Начало системы координат находится в левом нижнем углу // Уравнение плоскости ax + by + c = 0 с // единичным вектором нормали = (a,b) // Корневая плоскость (разделяющая карту на зоны A и B): normal = (-1,0) c = 3500 // Плоскость A (разделяющая зону A на зоны A1 и A2): normal = (1,0) c = -2500 // Плоскость B (разделяющая зону B на зоны B1 и B2): normal = (-0.24,0.94) c = -650 // Подстановка координаты любой точки (x,y) в // уравнение плоскости даёт нам расстояние от этой плоскости. Обход BSP-дерева всегда начинается с корневого узла с сортировкой обоих подпространств. Рекурсия выполняется для обоих дочерних узлов. Пример 1: Игрок (зелёная точка) смотрит сквозь окно из точки p=(2300,1900): // Позиция игрока = ( 2300, 1900 ) // R_RenderBSPNode выполняется для секущей плоскости AB (-x + 3500 = 0): -2300 + 3500 = 1200 Результат положителен, значит, ближайшее подпространство находится ПЕРЕД секущей плоскостью. (A ближе, чем B). // Затем R_RenderBSPNode рекурсивно выполняется для двух дочерних узлов корневого узла: секущих плоскостей A1/A2 и B1/B2. // R_RenderBSPNode выполняется для A1/A2 (x - 2500 = 0): 2300 - 2500 = -200 Результат отрицателен, поэтому ближайшее подпространство находится СЗАДИ от секущей плоскости. (A1 ближе, чем A2). // R_RenderBSPNode выполняется для B1/B2 (-0.24x +0.97y - 650 = 0): -0.24 * 2300 + 0.97 * 1900- 650 = 641 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (B1 ближе, чем B2). Результат: зоны отсортированы от самой близкой до самой далёкой: { A1, A2, B1, B2 } Пример 2: Игрок (зелёная точка) смотрит с секретного балкона в точке p=(5040, 2400): // Позиция игрока = ( 5040, 2400 ) // R_RenderBSPNode выполняется для секущей плоскости AB (-x + 3500 = 0): -5040 + 3500 = -1540 Результат отрицателен, поэтому ближайшее подпространство находится СЗАДИ от секущей плоскости. (B ближе, чем A). // Затем R_RenderBSPNode рекурсивно выполняется для двух дочерних узлов корневого узла: секущих плоскостей A1/A2 и B1/B2. // R_RenderBSPNode выполняется для B1/B2 (-0.24x +0.97y - 650 = 0): -0.24 * 5040 + 0.97 * 2400 - 650 = 468 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (B1 ближе, чем B2). // R_RenderBSPNode выполняется для A1/A2 (x - 2500 = 0): 5040 - 2500 = 2540 Результат положителен, поэтому ближайшее подпространство находится ПЕРЕД секущей плоскостью. (A2 ближе, чем A1). Результат: зоны отсортированы от самой близкой до самой далёкой: { B1, B2, A2, A1 } BSP-деревья позволили сортировать SEGS из любой точки карты с постоянной скоростью, вне зависимости от позиции игрока. Ценой за это стали одна операция умножения и одна операция суммирования для каждой плоскости. Кроме того, благодаря тестированию ограничивающими прямоугольниками отбрасываются крупные части карты. Примечание: Не сразу очевидно, но BSP сортирует все сегменты SEGS вокруг игрока, даже те, на которые он не смотрит. При использовании BSP необходимо применение отсечения по пирамиде видимости. Стены При сортировке BSP стен (SEGS) с ближней до дальней, рендерятся только ближайшие 256 стен. Две вершины каждого SEGS преобразуются в два угла (относительно позиции игрока). Примечание: В 1993 году только самые мощные машины 486DX имели FPU (сопроцессор для чисел с плавающей точкой), поэтому движок Doom вычислял все углы с помощью двоичного измерения углов (Binary Angular Measurement, BAM), работавшего только с числами int, формат float использовался редко. По той же причине точность, превышающая точность целых чисел, достигалась с помощью fixed_t, двоичного формата 16.16 с фиксированной запятой (подробнее об этом можно почитать здесь и здесь). После преобразования в углы координаты X экранного пространства получались с помощью таблиц поиска (viewangletox). Поскольку BAM выполнялись в int, углы сначала масштабировались с 32 до 13 бит с помощью 19-битного смещения вправо, чтобы уместиться в таблицу поиска размером 8 КБ. Затем стены обрезались согласно массиву отсечения (solidsegs. В некоторых статьях о движке Doom упоминается связный список, но не похоже на то, что он использовался). После обрезки оставшееся пространство интерполировалось и отрисовывалось как столбцы пикселей: высота и координата Y столбца пикселей были основаны на высоте сектора SEGS и расстоянии от точки обзора игрока, соответственно. Примечание об отсечении поверхностей: Отсечение невидимых поверхностей выполнялось с помощью angle2-angle1 > 180 . Рендерились только стены, находящиеся в области видимости. Примечание: Не все стены состояли из единых текстур. У стен могла быть нижняя текстура, верхняя текстура и средняя текстура (которая могла быть прозрачной или полупрозрачной). Как заметно на видео ниже, это было удобно для симулирования окон: «окно» на самом деле является сектором с высоким полом и отсутствующей средней текстурой. Интересный факт: Поскольку стены рендерились как вертикальные колонны, текстуры стен хранились в памяти повёрнутыми на 90 градусов влево. Этот трюк позволял полностью использовать функцию предварительного кэширования центрального процессора: процесс считывания тексела стены из ОЗУ также предварительно заполнял кэш ЦП восемью соседними текселами с каждой стороны. Поскольку последующие данные чтения уже находились в кэше ОЗУ, достигалось значительное снижение латентности считывания. Подробнее о предварительном кэшировании и выравнивании данных в памяти можно почитать в книге «The Art of Assembly Language programming» (раздел 3.2.4 Cache Memory). Плоские поверхности (пол и потолок), или печально известные visplanes При отрисовке столбцов стен верхние и нижние координаты экранного пространства использовались для генерирования «visplanes», областей в экранном пространстве (не обязательно непрерывных горизонтально). Вот как объявляется visplane_t в движке Doom. // // Что же такое visplane? // typedef struct { fixed_t height; int picnum; int lightlevel; int minx; int maxx; byte top[SCREENWIDTH]; byte bottom[SCREENWIDTH]; } visplane_t; Первая часть структуры хранит информацию о «материале», (height, picnum, lightlevel). Четыре последних члена определяют покрываемую зону экранного пространства. Если два подсектора имеют одинаковый материал (высоту, текстуру и уровень освещённости), то движок Doom пытался слить их вместе, но из-за ограничений структуры visplante_t это было не всегда возможно. Для всей ширины экрана visplane может хранить местоположение столбца пикселей (поскольку visplanes получаются проецированием стен на экран, они создаются как столбцы пикселей). Вот три основные visplanes начального экрана: Зелёная плоскость особенно интересна: она демонстрирует, что visplane_t может хранить прерывистые (но только в горизонтальном направлении) области. Поскольку столбец непрерывен, visplane может хранить его. Это ограничение проявляется в движке: некоторые подсекторы можно слить и рендерить с помощью одной visplane, но если между ними есть что-нибудь по вертикали, то слияние невозможно. Вот скриншот и соответствующее видео, показывающие фрагментацию visplane. Интересный факт: Жёстко заданный предел visplanes (MAXVISPLANES 128) был большой головной болью для моддеров, потому что игра вываливалась и возвращалась в DOS. Могут возникнуть две проблемы: \"R_FindPlane: no more visplanes\": Общее количество различных материалов visplanes (высота, текстура и уровень освещённости) больше 128. R_DrawPlanes: visplane overflow (%i): при фрагментации visplanes их количество превысило 128. Зачем ограничиваться числом 128? Два этапа конвейера рендеринга требовали выполнения поиска по списку visplanes (с помощью R_FindPlane). Поиск был линейным, и, возможно, для чисел больше 128 оказался слишком затратным. Ли Киллоу (Lee Killough) позже расширил этот предел, заменив линейный поиск реализацией хеш-таблицы с цепочками. Предметы и прозрачные стены После того, как все сплошные стены и стены с «прозрачной средней текстурой», а также поверхности потолков и полов будут отрендерены, остаются только «предметы»: враги, бочки, боеприпасы и полупрозрачные стены. Они рендерятся от самых дальних к самым ближним, но не проецируются в экранное пространство с помощью таблицы поиска стен. Процесс рендеринга выполняется вычислениями двоичных чисел 16.16 с фиксированной запятой. Примечание: В этом видео показан один из наихудших сценариев, когда некоторые пиксели приходится перерисовывать три раза. Профайлинг Загрузка Chocolate Doom в Instruments под Mac OS X позволила выполнить кое-какой профайлинг: Похоже, что порт [на iPhone] довольно точно соответствует «ванильному» Doom: бóльшую часть времени выполняется отрисовка стен (R_DrawColumn), потолка/пола (R_DrawSpan) и предметов (R_DrawMaskedColumn ). Кроме отрисовки я заметил высокие затраты ресурсов на интерполяцию стен (R_RenderSegLoop) и преобразование visplane из столбцов в строки пикселей (R_MakeSpans). Затем, наконец, дело доходит до AI (R_MobjThinker) и обход BSP-дерева (R_RenderBSPNode). С помощью инвертированного дерева вызовов можно увидеть, что бóльшая часть работы и в самом деле заключается в обходе BSP-дерева, рендеринге стен и генерировании visplanes: R_RenderBSPNode (второй столбец — процент потраченного времени). Всё вместе И вот, наконец, видео генерирования легендарного первого экрана, в котором можно увидеть по порядку: Стены как строки пикселей, с ближних до дальних Плоские поверхности как строки пикселей, с ближних до дальних Предметы с дальних до ближних. Интересные факты Поскольку Doom разрабатывался на системе NeXTSTEP с линейной моделью виртуальной памяти, id Software решила отказаться от EMS и XMS, которые использовались в большинстве игр того времени. Вместо этого разработчики использовали DOS/4G, расширитель памяти, позволявший ПО получать доступ к ОЗУ в защищенном режиме в операционной системе с реальным режимом (DOS). Рабочая станция NexT была настолько мощной, что оказалась способна выполнять редактор, игру и отладчик одновременно. Когда игра стала достаточно стабильной, код отправили по сети в PC, где он был скомпилирован под DOS/x86 компилятором Watcom. Благодаря DOS/4G код выполнялся в одинаковой модели памяти и на PC, и на NeXT. Интересное видео, дополняющее книгу «Masters of Doom»: Многие подробности можно изучить на сайтах Джона Ромеро (John Romero) rome.ro и planetromero.com. Рекомендуемое чтение Оригинальный исходный код, выпущенный в 1997 году, удобен для чтения, но в нём нет или почти нет комментариев, он не компилируется, в нём отсутствует исходный код звуковой подсистемы (из-за проблем с лицензированием). Chocolate Doom: О, ДА! Это просто потрясающий порт, он основан на SDL и с brio скомпилируется практически на любой платформе. Именно этот порт я хакнул, чтобы генерировать видео для статьи. Книга «Graphics Programming Black Book» Майкла Абраша. Помогает понять BSP-деревья и является отличным источником вдохновения. Этот парень может даже заставить вас полюбить ассемблер. Masters of Doom: история id Software со множеством подробностей о создании Doom"], "hab": ["Реверс-инжиниринг", "Разработка игр", "Алгоритмы"]}{"url": "https://habrahabr.ru/post/322622/", "title": ["Линус Торвальс высказался о коллизиях SHA-1 в репозиториях Git: бояться нечего"], "text": ["Несколько дней назад сотрудники компании Google и Центра математики и информатики в Амстердаме представили первый алгоритм генерации коллизий для SHA-1. За десять лет существования SHA-1 не было известно ни об одном практическом способе генерировать документы с таким же хешем SHA-1 и цифровой подписью, как в другом документе, но теперь такая возможность появилась. Хеш-функция SHA-1 используется повсеместно, поэтому известие о генерации документов с идентичным хешей вызвало естественную обеспокоенность у пользователей. В том числе у пользователей системы управления версиями Git, в которой тоже используются хеши SHA-1. Развёрнутый ответ на эти опасения дал Линус Торвальс. Если вкратце, то бояться нечего. Линус считает, что ничего критически важного эта атака на поиск коллизий не сделает. По его словам, есть большая разница между использованием криптографического хеша для цифровых подписей в системах шифрования и для генерации «идентификации контента» в системе вроде Git. В первом случае хеш — это некое заявление доверия. Хеш выступает как источник доверия, который фундаментально защищает вас от людей, которых вы не можете проверить иными способами. Напротив, в проектах вроде Git хеш не используется для «доверия». Здесь доверие распространяется на людей, а не на хеши, говорит Линус. В проектах вроде Git хеши SHA-1 используются совершенно для другой, технической цели — просто чтобы избежать случайных конфликтов и как действительно хороший способ обнаружения ошибок. Это просто инструмент, который помогает быстро выявить искажённые данные. Речь не о безопасности данных, а о техническом удобстве дедубликации и выявления ошибок. Другие системы контроля версий часто используют для выявления ошибок методы вроде CRC. Линус признаёт, что SHA-1 используется в Git также для подписи веток, так что в этом смысле он тоже является частью сети доверия, поэтому появление атаки на поиск коллизий действительно имеет негативные последствия для Git. Но в реальности нужно иметь в виду, что этой конкретной атаки очень легко избежать по нескольким причинам. Во-первых, в этой атаке злоумышленник не может просто создать документ с заданным хешем. Ему нужно создавать сразу два документа, поскольку атака проводится по идентичному префиксу. Во-вторых, разработчики атаки на поиск коллизий SHA-1 опубликовали научную статью и выложили инструменты, чтобы распознать признаки атаки. Можно очень легко распознать документы, в которых есть этот префикс, пригодный для генерации второго документа с идентичным хешем. То есть на практике, если внедрить соответствующие меры защиты против документов с этим префиксом, атака будет неосуществима. Кстати, такая защита уже реализована в Gmail и GSuite. Детектор уязвимых документов работает в открытом доступе на сайте shattered.io. Библиотека для обнаружения коллизий sha1collisiondetection опубликована на Github. Когда все данные лежат в открытом доступе, то реальная атака практически невозможна. Авторы научной работы приводят пример атаки на документы PDF с идентичным префиксом. Эта атака успешна, потому что сам префикс «закрыт» внутри документа, как блоб. Если же у нас открытые исходники в репозитории, то это совсем другое дело. Вряд ли можно сделать такой префикс из исходного кода (только из блоба). Другими словами, для создания идентичного префикса и последующей генерации веток кода с одинаковыми хешами SHA-1 придётся внедрить в код некие случайные данные, что сразу же будет замечено. Линус говорит, что есть места, куда можно спрятать данные, но git fsck уже вылавливает такие фокусы. Линус Торвальдс признаёт, что реальным опасением может быть только отслеживание документов PDF средствами Git. Здесь можно порекомендовать использовать инструменты для обнаружения признаков атаки, указанные выше. Такие патчи уже созданы для хостингов github.com и kernel.org, скоро они станут активными, так что здесь нечего волноваться. Ну и кроме всего прочего, Git в будущем уйдёт от использования SHA-1, сказал Линус, есть план, чтобы никому даже не пришлось конвертировать свои репозитории. Но как уже понятно, это не такая уж критическая вещь, чтобы спешить с ней. Кстати, упомянутая Торвальсом проблема отслеживания PDF-документов с идентичными хешами SHA-1 уже проявила себя в системе контроля версий Apache SVN, которая применяется в репозитории WebKit и других крупных проектах. В пятницу вечером на информационном сайте атаки на поиск коллизий SHA-1 появилась новая информация относительно действия атаки на систему контроля версий SVN. Там указано, что PDF-файлы с одинаковыми хешами SHA-1 уже ломают репозитории SVN. Оказалось, что если залить два разных файла с одинаковыми хешами, то система контроля версий не справляется с багом. Кто-то залил такие файлы в репозиторий WebKit, после чего он сглючил и прекратил приём новых коммитов. Вот эти два файла PDF с одинаковыми хешами: https://shattered.it/static/shattered-1.pdf https://shattered.it/static/shattered-2.pdf $ls -l sha*.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:01 shattered-1.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:14 shattered-2.pdf $shasum -a 1 sha*.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-1.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-2.pdf"], "hab": ["Криптография", "Информационная безопасность", "Git"]}{"url": "https://habrahabr.ru/post/322676/", "title": ["Анимационная пасхалка, или дань уважения студии при увольнении из нее"], "text": ["На Хабре много статей. Но не каждая показывает, как размышлял автор, его грабли и действия. Здесь я хочу вам рассказать, как я делал пасхалку для логотипа веб-студии в которой я работал. Как то раз увидев завораживающую анимацию от создателя библиотеки mo-js для svg-эфектов, я загорелся и решенил сделать что-то подобное. Как раз мне на глаза попалась обновленная главная страничка нашей студии, недолго думая я собрался сделать анимацию для логотипа. И раз эта статья рассказывает, как все это происходило, то к моему сожалению я не стану пересказывать весь мануал по данной библиотеки. Его вам придется прочитать самим, если конечно захотите сделать что-то подобное. Ну а чтобы сразу вникнуть в статью: → Ссылка на демо просмотр только от 1204x400, кликнуть на колокольчик → Ссылка на GitHub Первые начинания Мы все ошибаемся, но не всегда понимаем, что идем по каменистой дорожке. Изначально я хотел разбивать логотип на мелкие кусочки, сжимать их в 1 точку, и только потом как пазл собирать в единое целое со всевозможными эффектами (думаю это было бы красиво). Но быстро понял, что делаю очень быструю анимацию и естественно она будет без звука. Когда логотип создателя библиотеки, как я написал, был «Завораживающим». Свое второе вдохновение я нашел в одной навязчивой мелодии (Clannad песенка булочек), думаю многие щас вздохнули в меланхоличном припадке. Да она просто умиляющая. И раз это колыбельная, то и тематика ей должна быть подстать! Сюжет выдумывать особо и не пришлось. Колыбельная — «ночь, звезды, луна», замечательные ассоциации, большие варианты развития сюжета моей пасхалки. Первым делом я нашел минус этой песенки и нарезал ее до 30 секунд, это максимум времени, сколько может выдержать любой гость, не закрыв страницы (думаю на большее меня бы и самого не хватило). Далее сделал метки эффектов, наиболее подходящим инструментом я выбрал Adobe Audition CC 2017 (честно говоря другого инструмента я и не искал, просто он мне попался на глаза первым). Выглядело это так: Библиотека предоставляет вложенные временные шкалы объекта TimeLine(), я бы мог разделить все на отдельные блоки, и начинать их анимацию по отдельности. Но т.к. у меня была привязка к мелодии, я счел необходимым сделать все на 1 единой временной шкале, дабы не запутать себя самого с таймингом. Просто сделал большие блоки массивов с временем начала в виде предоставляемого библиотекой параметра задержки (delay). Радужный дождь и взращивание букв С точки зрения обычного пользователя, сюжет анимации всегда довольно простой. Падают радужные лучи, вырастают буквы, появляется рельеф, звезды и выходит луна, финиш, еще можно что-то там повозить. Но редко кто описывает, как он это делал, сколько вложил сил, терпения и слез своей музы. Структуризация кода всегда помогает мыслить в рамках отдельных модулей и у меня получилось 5 объектов для манипуляций над элементами анимации. FirstRainbow Анимация падения радуги мне далась довольно легко, я банально скопировал ее с примера в тутореал. Сделал массив с координатами и описал это в 1 методе rainbow(200, 500, 'A', 'str1') где параметры — это продолжительность, время задержки (в моем случае начала) анимации и последние 2 описывают координату. Эффект падения получился невзрачный: Скрытый текст Изменили на улучшенный: Скрытый текст Letters Описывает анимацию букв. Он стал первым по сложности для меня объектом. Концепция анимации заключалась в росте, будто растения. С буквами пришлось изрядно повозиться, думаю не каждый умеет работать в Illustrator (я в том числе). И как человек получивший в этом опыт, советую: Всегда вычищайте код svg после редактора. Переименовывайте классы в уникальные Не забывайте удалять лишнии слои. Потратил 60% общего времени только на синхронизацию, сверку координат, отрисовку отдельных линий (которые нельзя сымитировать стандартными фигурами). Мне приходилось по 20 раз менять 1 параметр, ради достижения хорошего эффекта. Но мой внутренний перфекционист пошел на компромисс со временем и сдался. Код же прост для обзора: ...that.plant_D(800, 10111), ...that.plant_A(800, 10200), ...that.plant_R(800, 10300), ...that.plant_N(800, 10400), ...that.plant_E(800, 10500), ...that.plant_O(800, 10600), ...that.plant_S(800, 12500), ...that.plant_T(800, 13100), ...that.plant_U(800, 13400), ...that.plant_D2(800, 13700), ...that.plant_I(800, 14000), ...that.plant_O2(800, 14300) Вторым с чем я столкнулся — это с собственной невежественностью в чтении мануалов, мы все в той или иной степени этим страдаем. Банально упустил пункт о том, что собственные заготовки svg должны быть в разрешении 100x100 и первая буква «D» пошла наперекосяк. Ну и конечно, в таких случаях мы всегда делаем костыль, что и случилось. Просто поменял ей размер, после инициализации (а раз я использовал объект Burst, то и менять мне надо сразу всех потомков). for (let el of equal.el.children) { el.style['height'] = '180px'; } В дальнейшем я задал вопрос на GitHub, на что меня ткнули в пунктик мануала) Добавление собственной svg происход след. образом: class elipceR2 extends mojs.CustomShape { getShape () { return '<path d=\"M0,120.5h13.9c39.6,0,59.6-30.6,59.1-61C73,29.8,52.8-0.2,13.3,0L0.1,0\"/>'; } getLength () { return 200; } // optional } добавить и использовать (простота во всем): mojs.addShape( 'elipceR2', elipceR2'); new mojs.Shape({ shape: 'elipceR2'}); Либо использовать дефолтные svg-фигуры. MoonRise Описывает горизонт, горы, выход луны. С луной возился долго, надо было соблюдать стилистику и луна с детальным рельефом тут не подходила: Скрытый текст Но, я удачно нагуглил более лучший вариант: Скрытый текст немного поправить напильником и получаем замечательную луну, хорошо подходящую к нашей стилистике. Для приличия код: that.moon(5000, 20000), that.mountains(1500, 18500), that.horizonLine(1600, 18500), Stars Второй по сложности объект. Стал основными источником синхронизации эффектов с музыкальной нарезкой «Семейства булочек». Я сделал несколько массивов звезд и вывел их в заданный момент, попутно закинув их в контекст объекта для дальнейших манипуляций c цветом, размером и их координатами. Где, параметры это время анимации, начало, и кол-во звезд. that.curentStars = [ ...that.star(200, 17050, 5), ...that.star(200, 17300, 15), ...that.star(200, 17600, 25), ...that.star(200, 17900, 30), ...that.star(200, 18200, 35), ]; Как-то размыть svg средствами библиотеки мне не удалось, пришлось их просто увеличивать. Как всегда, длительность анимации, ее начало, массив и кол-во звезд, которые мы берем рандомно, that.shineStars(10, 21200, that.curentStars, 3); that.shineStars(10, 21600, that.curentStars, 3); that.shineStars(10, 21900, that.curentStars, 3); that.shineStars(10, 22100, that.curentStars, 4); that.shineStars(10, 22400, that.curentStars, 4); that.shineStars(10, 22700, that.curentStars, 4); that.shineStars(10, 23300, that.curentStars, 10); that.shineStars(10, 23900, that.curentStars, 15); that.shineStars(10, 24500, that.curentStars, 12); that.shineStars(10, 25150, that.curentStars, 9); that.shineStars(10, 25700, that.curentStars, 6); that.shineStars(10, 26300, that.curentStars, 7); that.shineStars(10, 26600, that.curentStars, 4); that.shineStars(10, 27200, that.curentStars, 8); that.shineStars(10, 28170, that.curentStars, 3); попутно закидывая в свойство для падения this.curRimShineStar. Оказалось, что если выбирать звезды рандомно, то бывают моменты, когда звезда имеет очень маленькое расстояние к координате к которой она должна была начать движение. И как следствие я просто видел падающую точку. Отфильтровав массив по координатам на правую зону, я добился более продолжительной анимации и показу хвоста за звездой. that.shootingStar(500, 29600, {y: 52, x: PARAMS.COORDINATES_X.str1.D}); that.shootingStar(500, 29800, {y: 52, x: PARAMS.COORDINATES_X.str1.A}); that.shootingStar(500, 30100, {y: 52, x: PARAMS.COORDINATES_X.str1.R[0]}); that.shootingStar(500, 30400, {y: 52, x: PARAMS.COORDINATES_X.str1.N[0]}); that.shootingStar(500, 30700, {y: 52, x: PARAMS.COORDINATES_X.str1.N[1]}); that.shootingStar(500, 31070, {y: 52, x: PARAMS.COORDINATES_X.str1.E}); that.shootingStar(500, 31370, {y: 52, x: PARAMS.COORDINATES_X.str1.O}); Очень огорчило отсутствие некоторых подробностей по местоположению параметров в объектах, но все решилось банальным выводом его в консоль и чтением исходников (долго, но всегда гарантированный результат). Мне потребовалось узнать, когда объект завершит свою анимацию после всех манипуляций с ним и я не сразу понял что свойство Object.timeline._props.time его показывает, изначально я искал его в Object._o Где то я конечно схалтурил в коде, за что трудно себя простить. Но в целом анимация удалась замечательной. PointsTimer Чтобы гость мог покрутить ползунок и самому посмотреть, как это происходило, пришлось добавить и таймер, который завершал анимацию через пару секунд простоя. Тут моя фантазия уже закончилась и я просто добавил 3 уменьшающихся круга. По истечению которых, при помощи той же JQuery.animate() мы растворяем буквы в логотипе и удаляем все теги, что добавила библиотека. К сожалению библиотека не может делать очень долгих анимаций, это можно заметить, есть дернуть ползунок очень быстро в сторону, вы увидите как все пойдет кусками, где то будут элементы, где то нет. Решить это можно думаю лишь принудительным манипулированием скорости ползунка."], "hab": ["Программирование", "JavaScript"]}{"url": "https://habrahabr.ru/post/322704/", "title": ["Игра-головоломка NeoAngle. Работа с уровнями в Unity"], "text": ["Всем доброго времени суток! Я бы хотел вам рассказать историю своей новой игры-головоломки NeoAngle, а также поделиться опытом импортирования, хранения и генерации уровней в Unity. Начну с краткой предыстории 5-летней давности, когда я решил заняться геймдевом, впервые познакомившись с языком программирования action script 3.0 (для разработки flash-игр) в университете. Закончив семестр, я решил, что смог бы осилить собственную игру. Так, посидев какое-то время за блокнотом и карадашом, пришла идея головоломки с треугольниками, где игроку нужно путем переворачивания треугольного камня заполнить заданную форму и закончить уровень на финишной клетке. Таким образом, затратив неделю на разработку в одиночку, была выпущена моя первая flash-игра SeaQuest. И понесло меня разрабатывать дальше. Выпустив еще несколько портальных флэшек, я вернулся к идее с треугольниками, что привело к новой части, где геймплей был полностью переработан, а именно добавлены дополнительные интерактивные объекты: кнопки с препятствиями, телепорты и вращатели. Цель уровня также изменилась, теперь нужно было собрать все жемчужины и прийти к финишу. Результат под названием Stone Quest На этом линейка логических игр завершилась и была успешно мной позабыта. Далее последовал обещанный обвал флэш индустрии, который перекинул меня в unity разработку для мобильных. И вот, в декабре 2016 года, совершенно случайно мне в голову вернулась мысль о треугольно-ориентированных головоломках. Особенно подтолкнуло к разработке то, что геймплей отлично подходит под тачскрин. Было решено использовать механику предыдущей игры, но с другой стилизацией. Вот такая предыстория, которая привела к активной полуторамесячной разработке. Большая часть работы, как ни странно, ушла на геймдизайн. Из предыдущей версии почти все уровни мне показались недостаточно интересными, хотя возможности геймплея имеют гораздо больший потенциал. В следствие чего разработка началась с редактора уровней, который я написал за вечер на старом добром флэше, с возможностью тестирования написанных уровней и их экспорта/импорта в xml. Результат работы в редакторе продемонстрирован ниже: Прежде чем перейти к работе с Unity, мне необходимо было убедиться в том, что я смогу предоставить достаточное количество уровней для мобильной игры, которая требует разнообразный контент. Поэтому первые полторы недели я даже не открывал Unity, а работал в своем редакторе, параллельно занимаясь графикой. К слову, был выбран набирающий популярность ретро-стиль synthwave 80-ых. Он достаточно прост в исполнении, являясь при этом очень привлекательным. Таким образом, создав допустимый набор уровней, я приступил к их портированию в Unity, продолжив тратить часть времени на доработку и создание новых. В связи с этим, возникли следующие вопросы: каким образом импортировать, хранить и генерировать уровни в Unity из xml файла? Найденных вариантов решения было три: 1. При старте игры вычитывать xml файл и каждый раз динамически создавать уровни на runtime. 2. В edit mode сгенерировать уровни и создать на каждый по сцене. 3. В edit mode сгенерировать уровни и создать для каждого префаб. Очевидно, что первый вариант не самый лучший, т.к. все уровни заранее подготовлены и нету смысла их каждый раз заново создавать. Второй вариант больше подходит для более сложных и масштабных проектов, где каждый уровень имеет свою логику, интерфейс, структуру и т.п. В моем случае пришлось бы дублировать интерфейсы и ключевые объекты в каждую сцену с уровнем и при каждом изменении ui нужно было бы обновлять все сцены. Остается последний третий вариант, который как раз очень удачно подошел. О его реализации я и продолжу повествование. Для работы с объектами в edit mode добавим кастомную панель в редакторе Unity. Для этого создаем класс наследуемый от EditorWindow в папке Assets/Editor и добавляем туда следующий метод: [MenuItem (\"Window/Level Loader\")] // Level Loader - название панели в списке Window. public static void ShowWindow () { EditorWindow.GetWindow(typeof(LevelLoader)); // LevelLoader - имя созданного класса } Сразу же проверим, что панель создана в Window -> Level Loader: Далее, в методе OnGUI можно начинать добавлять кнопки и поля для нашего окна. Базовые примеры: void OnGUI() { GUILayout.Label (\"Custom Label\", EditorStyles.boldLabel); // добавление заголовка // создание поля для выбора GameObject GameObject customGO = EditorGUILayout.ObjectField(customGO, typeof(GameObject), true) as GameObject; int customInt = EditorGUILayout.IntField(customInt); // создание поля для ввода int значения EditorGUILayout.Space(); // добавление вертикального отступа if (GUILayout.Button(\"Custom Button\")) // добавление кнопки с обработчиком нажатия { ButtonHandler(); } } Больше информации по компонентам можно найти в официальной документации. А я продолжу описание моего Level Loader’a, продемонстрировав принцип работы ниже: Сперва, мы вычитываем уровни из xml файла, путем нажатия на Read Levels, после этого создается выпадающий список с возможностью выбора уровня и появляются дополнительные контролы, которые позволяют сгенерировать уровень на сцене, создать префаб выбранного уровня, пересоздать префабы всех уровней, показать/скрыть номера полей (для тестирования отрисовки и отслеживания ошибок). Для создания объектов на сцене в edit mode используется стандартная функция Instantiate, а для удаления DestroyImmediate. Какое-то время я создавал префабы уровней вручную, перетягивая их со сцены в папку Resources. Однако, это быстро мне надоело и я полез в интернет за информацией о том, как создавать префабы в edit mode программными средствами. Ниже конструкция, позволяющая это сделать: private void CreateLevelPrefab() { GameObject levelGO = GameObject.FindGameObjectWithTag(\"Level\").gameObject; // игровой объект Level содержит сгенерированный на сцене уровень Object levelPrefab = EditorUtility.CreateEmptyPrefab(\"Assets/Resources/\"+levelGO.name+\".prefab\"); EditorUtility.ReplacePrefab(levelGO, levelPrefab, ReplacePrefabOptions.ConnectToPrefab); } Далее, в режиме игры уровни добавляются на сцену следующим образом: GameObject levelGO = Instantiate (Resources.Load (\"Level\"+levelNum) as GameObject); Так, этапы добавления уровней получились следующие: Создание/редактирование уровня во flash-редакторе с последующим экспортом в xml Считывание xml уровня в unity Генерация уровня на сцене в edit mode Создание/обновление префаба уровня в папке Resources На этом всё. Для меня это был первый опыт работы с объектами в режиме редактирования. Буду рад ознакомиться с вашими идеями по поводу реализации редактора уровней прямо в Unity, миновав Flash. Спасибо за внимание! Поиграть в игру можно в Google Play по запросу NeoAngle."], "hab": ["Разработка мобильных приложений", "Разработка игр", "Unity3D"]}{"url": "https://habrahabr.ru/post/322674/", "title": ["Синдром самозванца: сражение с усталостью от фронтенда", "перевод"], "text": ["Недавно я разговаривал с другом из бэкенд-разработки о том, сколько часов провожу за программированием и изучением кода в свободное время. Он показал отрывок из книги Дяди Боба «Чистый код». Там разработчики, которые репетируют код перед запуском в работе, сравниваются с музыкантами, которые много часов готовят инструменты к концерту. Мне понравилась аналогия, но я не уверен, что готов полностью подписаться на такое; это тот самый тип мышления, который в первую очередь приводит к выгоранию. Хорошо, если вы хотите углубить своё мастерство и расширить навыки, но если делать это непрерывно в течение всего дня — долго не протянешь. Усталость от фронтенда очень реальна. Я видео много постов об усталости от JavaScript, но мне кажется, что проблема распространяется за пределы этого конкретного языка. Для ясности: это не очередная тирада о том, как всё плохо и быстро меняется — мне нравится, что технология развивается так быстро. Но я в равной степени я могу оценить, насколько проблема может подавлять и временами полностью промывать мозг. Насколько я могу сказать, здесь два уровня проблемы. Во-первых, фронтенд-разработчик думает, что в его арсенале полагается иметь следующее: HTML (писать с листа, семантическая разметка) CSS (масштабируемая и модульная архитектура) CSS-методологии (BEM, SMACSS, OOCSS) CSS-препроцессоры (что-то вроде LESS, SCSS, PostCSS) Современный CSS (Flexbox, Grid) JS Современный JS (ES6, Typescript) Фреймворки JS (Angular, React, Vue [вставьте сюда последние] JS-методологии (функциональное программирование, ООП) JS-библиотеки (Immutable, Ramda, Lodash) Принципы отзывчивого дизайна Тестирование (TDD) Фреймворки для тестирования (Jasmine, Karma) SVG WebGL Техники анимации Доступность Удобство использования Производительность Инструменты для сборки (Grunt, Gulp, скрипты NPM) Сборщики модулей (WebPack, Browserify) Экосистема NPM Знание различных трюков браузера Методологии Agile Сестемы контроля версий (обычно Git) Основы графического дизайна Навыки межличностного общения, тайм-менеджмент Базовое понимание всего, о чём говорят в бэкенде И поверх этого вы возитесь, или пока присматриваетесь к вещам вроде этого: Сервис-воркеры Прогрессивные веб-приложения (PWA) Веб-компоненты Второй пункт — это то, с чем вы не работаете ежедневно и вам не выделяют рабочего времени на их изучение, так как же вы собираетесь убедиться, что в вашем распоряжении есть все инструменты? Термин вроде «прогрессивное веб-приложение» может прозвучать довольно обескураживающе для разработчика. Новые техники и технологии ведут к чувству усталости — усталости от фронтеда (источник картинки) Теперь, как потребитель информации вы можете: Подписаться на ряд различных почтовых рассылок по разработке Изучить свой твиттер-фид Посетить еженедельное образовательное занятие на работе для разработчиков фронтенда Открыть нерабочий канал Slack, где общается горстка разработчиков Следовать онлайновым руководствам (в надежде, что они не устарели) Использовать обучающий видеокурс на сайте вроде Frontend Masters Купить книги по веб-разработке (в надежде, что они не устарели) Посетить митапы Посетить конференции Посетить обучающие курсы Как автор вы можете: Писать статьи в блоги/журналы Выступать с докладами Запустить подкаст Участвовать в open-source проектах Открыть собственные сторонние проекты Недавно я обнаружил, что моё внимание разделяется натрое: я писал код, слушая в наушники вполуха разговоры о коде и обсуждая программирование в чате Slack. Я решил, что это предел — все мои отверстия были забиты кодом и я был умственно опустошён. Хотя это явно экстремальный случай, но я уверен, что некоторые из вас испытывали нечто похожее. Кроме всего этого, у вас вероятно есть работа на полный день, друзья, хобби. Неудивительно, что столь многие из нас ощущают выгорание и сомневаются в правильном выборе профессии. Некоторые из моих знакомых-фронтендщиков выражали желание закончить всё это и перейти на работу, где можно отключаться в пять часов. Но что-то внутри меня подсказывает, что эта работа привлекает определённый тип людей, и даже если вы станете агентом недвижимости, всё равно захотите быть лучшим агентом, насколько это возможно. Будете посещать митапы в агентстве недвижимости, а в свободное время отслеживать тенденции изменения стоимости домов. Много месяцев назад я работал в финансах и по-прежнему изучал их по вечерам и читал книги, чтобы стать самым продвинутым, насколько возможно, в выбранной отрасли. Мы не одиноки в таком поведении. Многие профессии требуют уделять им много времени и изучения за пределами работы. Проблема с фронтенд-разработкой может быть в том, что технология развивается настолько быстро, что возникает ощущение несправедливости, словно кто-то расширяет футбольные ворота во время матча. Похоже, что каждый божий день я получаю письмо с уведомлением, что очередная технология \"XYZ\" мертва. Что никак не может быть правдой, потому что в таком случае я остался бы без рабочих инструментов. Экосистема постоянно изменяется, и я думаю, это хорошо. Лично мне нравится постоянно изучать программирование и подталкивать себя, но нужно признать, что временами это переходит рамки. Имею в виду вышесказанное, вот несколько вещей, которые я стараюсь держать в памяти, чтобы не дать голове взорваться. Некоторые общие советы, как избежать этой усталости. Мы все в одной лодке Все известные мне программисты, как на работе, так и за её пределами, — одни из умнейших людей, которых я знаю. Но все они чувствуют себя перегруженными. У большинства есть некий «список пожеланий» с технологиями, которые он пытается освоить. Может быть, имеется горстка людей наверху, знающих весь свой список, но большинство из нас находятся именно в такой позиции. Мы всё ещё уверены, что Google и Stack Overflow помогут, и у нас слишком много открытых вкладок с ответами на вопросы по веб-технологиям. Вы не одиноки! Нужно понять, что ты не становишься плохим разработчиком оттого, что ещё не попробовал все эти модные штучки. Да, даже «знаменитые технари» в той же лодке. Невозможно знать всё. И рок-звёзды программирования, на которых вы подписаны в твиттере, по-настоящему хороши только в нескольких областях каждый. Вы заметите, что это именно те области, которые сделали их знаменитыми за хорошую осведомлённость. И снова, будут исключения, но все они такие же люди как мы. :) Синдром самозванца есть у каждого из нас Я знаю нескольких отличных фронтенд-разработчиков, которые не хотят претендовать на вакансию, потому что считают чем-то вроде мошенничества претендовать на работу, не соответствуя всем заявленным требованиям. Процитирую одного из них: «90% джуниров, которых я вижу, заставляют меня чувствовать себя далеко позади. В реальности это настолько сильно меня беспокоит, что я думаю о том, как сохранить нынешнюю должность, и просто пытаюсь заработать побольше денег, потому что есть чувство, что с этими деньгами я и уйду» Факт в том, что большинство требований к вакансиям — фарс. Мой друг Бард хорошо проиллюстрировал это на одной картинке, которая даёт перевод текста в вакансии фронтенда. Перевод описания вакансии Просто помните, всё будет нормально. На каждой работе я поначалу чувствовал недостаточное погружение, но в конце концов вы привыкаете к их инструментам и рабочему процессу, вы обучаетесь и становитесь лучшим разработчиком для этого. Не бойтесь учиться на работе, лучший способ освоить новые навыки — ежедневно использовать их. Если у вас синдром самозванца, скорее всего, вы на самом деле приличный разработчик, потому что иначе у вас не хватило бы способностей самоанализа, чтобы осознать это. Придерживайтесь основ Легко отвлечься на блестящие новые штучки, но если базовые принципы слабы, то ваша разработка может не выдержать проверки временем. Как хороший друг однажды сказал мне: «Внимание к основам всегда было моей мантрой. Если ты можешь создать хорошую вещь и решить проблемы, то всё остальное неважно. Инструменты меняются и всегда будут меняться» Например, когда React взлетел к славе, он всегда как будто работал в связке с ES6, и я сконцентрировался на изучении этих изменений или добавлений к языку, а не на особенностях самого фреймворка. Когда React умрёт и исчезнет, со мной останутся знания, которые я приобрёл о последнем стандарте JavaScript. С многими функциями вы можете поиграться прямо в Chrome, так что необязательно запускать Babel и вязнуть в болоте зависимостей. Не нужно изучать всё Это действительно самое главное. Думаю, что нас убивают вовсе не новые фреймворки, библиотеки и модули, а именно наша вера в то, что мы должны все их изучить. С опытом я пришёл к мнению, что лучше всего фокусироваться на чём-то одном — в данный момент я копаюсь в функциональном программировании JavaScript в ES6. В моём списке много вещей, которые хотелось бы изучить, но я стараюсь не отвлекаться. Например, было бы хорошо освежить мои знания по доступности, поиграться с Polymer и углубиться в некоторые из последних техник CSS вроде Grid, но если я научну читать о слишком многих темах одновременно, то не усвою всю информацию. Другие вещи никуда не исчезнут, я займусь ими когда придёт время. Избегайте спешки, пытаясь потребить всё по данной теме. Не спешите и убедитесь, что совершенно поняли её. Если вы похожи на меня, то список тем для изучения у вас будет всё время расти, но не бойтесь его сокращать. Не каждая тема стоит потраченного на неё времени, и следует научиться распознавать, что стоит изучения, а что может исчезнуть в течение нескольких лет. Уделить время изучению шаблонов проектирования и технологий архитектуры всегда будет более полезно в долговременной перспективе, чем прыжки по модным фреймворкам. Всё закончится тем, что вскоре вам придётся снова играть в бинго с умными словечками. Большинство компаний не используют последние технологии Постоянно появляется много новых вещей, веб развивается с ошеломляющей скоростью, но обычно проходит длительное время, прежде чем компании реально начинают внедрять эти новые технологии. Большинство компаний подождёт некоторое время, пока технология созреет и докажет себя на практике. Angular создан шесть лет назад, и я впервые начал работать с ним в стартапе, который выбрал этот фреймворк для себя три года назад. Reactjs существует более трёх лет, а моя текущая компания начала использовать его перед Рождеством. Уверен, что большинство других фреймворков родились и исчезли за это время. Если бы я занялся ими, я был бы сумасшедшим. В сфере CSS Flexbox был доступен с 2010 года — шесть лет назад! Поддержка браузеров до сих пор неполная. Мы начали использовать его в продакшне недавно в этом году, но я не вижу, чтобы другие особо использовали его. Мой тезис в том, что не нужно спешить с изучением всего подряд. Когда технология быстро развивается, ваш работодатель может развиваться гораздо медленнее. Не нужно скакать впереди лошади, просто держи в поле зрения траекторию её движения. Чем больше ты знаешь, тем лучше ты понимаешь, что ничего не знаешь, и это нормально Это абсолютно нормально. Когда ты начинал, ты не знал, чего именно не знаешь. Потом изучил некие вещи и подумал, что ты гений. Затем шаг за шагом эта фантазия исчезает. Ты начинаешь понимать, как на самом деле много вещей вокруг, которых ты не знаешь. По существу, чем больше опыта, тем шире бездна. С этим нужно смириться, иначе она поглотит тебя. Если что, это чувство должно дать тебе уверенность, что ты двигаешься в правильном направлении. В нашей профессии фронтенда ты никогда не будешь комфортно сидеть на троне из накопленных знаний. Не трать всё свободное время на учёбу Несложно почувствовать, что ты настолько позади всех, что нужно постоянно программировать и учиться. Это билет в деревню выгорания. Установи определённое время для развития своих навыков и подумай, можно ли переговорить с начальником насчёт того, чтобы выделить это время, а в остальные часы работай над любимым делом. Некоторые из моих прозрений насчёт программирования пришли в тренажёрном зале. Физические упражнения чрезвычайно важны для вашего ума, как и для тела. Попробуйте 20-30 минут в день, чтобы держать не терять концентрацию и предотвратить выгорание. Уделите время семье и друзьям — не тратьте с ними время на пустую говорильню. Здесь рынок разработчика В наше время не нужно беспокоиться о том, чтобы найти работу. В данный момент мы находимся в очень удачной позиции, когда вакансий больше, чем разработчиков. Не знаю, как долго это продлится, но получайте пользу! Вы можете получить работу, не обладая полными знаниями. Я обнаружил, что во время интервью превосхожу 99% народу, которые только болтают. В крайнем случае, помните о золотом запасе легаси-кода. Если вы из тех, которые любят программировать по старинке, всегда найдутся компании, которые застряли на старом ПО, и им нужны разработчики. Заключение Надеюсь, что некоторые из этих наводок помогут избежать некоторых разочарований, которые могли вас настигнуть. Самое худшее — это дойти до предела и полностью выгореть, потому что если такое произойдёт, очень трудно будет вернуть былую страсть, из-за который вы начали заниматься программированием в первую очередь. Дэвид Бернер — разработчик фронтенда из Великобритании, который программирует с 1998 года, когда ещё были прозрачные разделительные gif'ы и теги <blink>. Он писатель, спикер и программист, увлечённый тем, чтобы двигать веб вперёд. В ы можете найти его бессвязные истории об этом (и любых вопросах фронтенда) на Fed || Dead."], "hab": ["Карьера в IT-индустрии"]}{"url": "https://habrahabr.ru/company/everydaytools/blog/322694/", "title": ["Как двухлетний репозиторий на GitHub стал трендовым за 48 часов", "перевод"], "text": ["GitHub предоставил возможность миллионам разработчиков с легкостью публиковать свои проекты и тем самым привлекать пользователей и единомышленников. Часто перед разработчиками возникает проблема неэффективного использования ресурсов — они тратят сотни часов на создание проекта с целью продвинуть его на GitHub, а получают максимум две звезды. Я оказался точно в такой ситуации, разрабатывая проект для некоммерческой организации Hack4Impact. Это студенческая группа, которая работает над техническими проектами для общественных организаций. Вместе мы разработали flask-base, которая использовалась как шаблонный код для всех наших продуктов. База данных включала в себя основные элементы flask веб-приложения: SQLAlchemy, Redis Queue и аутентификацию пользователя (наряду с несколькими другими функциями). Вы можете ознакомиться с нашим репозиторием по ссылке. Демонстрация бэкенда flask-base Разработанная нами flask-base относится к категории «plug and play», что является главным его преимуществом. Установить работоспособную версию на компьютер не составляет большого труда (и ее можно запустить на хостинге, таком как Heroku). Кроме того, база крайне минималистична по сравнению с аналогами, ее очень удобно кастомизировать. Разработка flask-base заняла два года и послужила шаблоном для 90% наших технических проектов. Проект помог претворить в жизнь продукты для таких организаций как Kiva, OSET, Juvenile Law Center, Givology. Мы смогли помочь общественным организациям Америки достичь социального влияния, к которому они стремились. Несмотря на все наши попытки популяризировать наш код, flask-base осталась неизвестным в широких кругах, но стало полезным для людей, работающих с ней. С чего мы начинали Наши провальные попытки расширить аудиторию приводили к отчаянию, так как мы были уверены, что другие разработчики и небольшие компании могли бы использовать плод нашего труда для значимых проектов. Но мы никак не могли найти способ продвижения нашего продукта. В результате этого, мы подвергли сомнению честность источников с открытым исходным кодом, которые должны освещать отличные идеи для широкого круга потребителей. То самое чувство Впоследствии мы обнаружили ошибку нашего подхода. Мы внимательно изучили, каким образом работает открытый исходный код с точки зрения пользователя. Мы нашли ключевые области, которые нужно было пофиксить и усовершенствовали наш проект, чтобы он стал пригодным для реального мира.Мы попали в точку. Я запостил наш продукт в сабреддит /r/Python. В течение 48 часов наш репозиторий получил более 200 звезд по сравнению со стартовыми 9. И мы продолжали расти. Внезапно мы начали получать комментарии и предложения от людей, которые были заинтересованы в нашем проекте, и это было превосходно. прошлые дни += 544 stars, 74 forks, and 16 watches В этой статье мы хотим показать, каким образом наша flask-base заняла свою лидирующую позицию на GitHub. Если у вас есть значимый проект, вы можете легко использовать наш опыт и получить из него максимум. Путь начинается с исследования Мы начинали с анализа историй успеха. Популярные репозитории на GitHub имеют сходства: Обзор продукта содержит иллюстрации и гифки; Документация; Статистический анализ кода; Уточняющие инструкции; Хорошо описанная инструкция к установке; Логотип. Разбивка первых строк README Хочу обратить ваше внимание на некоторые из лучших репозиториев GitHub: React-Router: более 19 тысяч звезд, 4,5 тысячи forks. ReactnRouter полезен в контексте управления отдельными страницами веб-приложений, а также является редким репозиторием, служащий туториалом по использованию структуры. Также он содержит исчерпывающий гид по установке, наряду с рекомендациями к ошибкам, с которыми могу столкнуться пользователи. Webpack: 23,5 тысячи звезд, 2,7 тысячи forks. Webpack можно назвать одним из лучших инструментов фронтенд-разработки, благодаря надежности и возможности разработки для разнообразных браузеров. README состоит из десятков знаков и примеров использования кейсов со ссылками на документацию. Webpack также подчеркивает роль общества в поддержке проекта (в частности, они имеют Sponsor и Backer секции) Теперь приведем пример неудачного репозитория: abhisuri97/leARn: я выбрал свой собственный репозиторий в качестве плохого примера – Hackathon проект, который выиграл PennApps XIII VR/AR и попал в Топ 10. Это был единственный проект, который я разрабатывал на Unity, имеющий огромное количество посторонних ненужных файлов. Наряду с подробным описанием функционала проекта, он не объясняет, как проект работает на конкретных системах и в чем его функции. После анализа множества репозиториев и мониторинга основных трендов мы выделили ключевую концепцию: разработчик, изучающий ваш репозиторий нуждается в веской причине, почему ему стоит попробовать именно ваш проект, то есть в понимании, что продукт максимально прост и удобен в установке. A.G.D. (Attention Grabbing Device – Методы привлечения внимания) Красноречие и README: В прошлом я принимал участие в конкурсах ораторского искусства, один из них назывался «Original Oratory» («Подлинное Красноречие»). Требовалось продекламировать жюри 10-минутную речь собственного написания. Каждое мое выступление начиналось с 2х минутного введения. Чаще всего это была история, предшествующая тезисам для речи, которые я собираюсь освещать. По сути, README является A.G.D. вашего проекта! README – первое, что увидит пользователь, обращаясь к вашему репозиторию. В связи с этим, стоит обратить пристальное внимание на его наполнение. Но что является первостепенным и как завладеть вниманием пользователя? При ознакомлении с вашим проектом, пользователь должен знать: что это; насколько хорош код; какой саппорт доступен; что входит в проект; как он выглядит; как установить проект. Давайте пройдемся по всем пунктам подробнее. Что это Огромный логотип сразу расскажет о вашем продукте Самый простой вопрос о репозиториях, но многие люди неправильно его толкуют. Ваш проект один из миллионов существующих. У вас крайне мало времени, чтобы произвести впечатление. Опишите ваш проект в твите (около 140 символов) – без лишних деталей: для них отведен раздел фич. Логотип также поможет, т.к. он выделяет название проекта среди черно-белого текста README (а также показывает ваши усилия, приложенные для его создания). Насколько хорош код. Тот самый вопрос, на который 90% репозиториев не в состоянии ответить. В то время как определение «хорошего» кода субъективно, есть несколько основных характеристик: он хорошо протестирован; пройдена проверка стиля (ESlint); он может быть составлен в текущем виде; он прошел статистический анализ (через такие сервисы как (Code Climate). Дашборд Code Climate выдает вам cредний балл качества кода Досконально изучать ваш код перед его использованием разработчикам не интересно. Равно как и «символы» на первой строке проекта. Особенность этих символов в легкости их применения, что делает проект действительно надежным в глазах посетителей, и им не нужно тратить время на изучение самого кода. Какая техподдержка доступна Саппорт состоит из двух видов: проблемы и обучение для пользователя. Саппорт по проблемам может быть реализован в FAQ. Но для новых проектов нет возможности узнать скрытые проблемы старого кода (и нет контента для FAQ). Единственное решение в данном случае – отвечать на вопросы, по ходу их возникновения и оперативно фиксить баги. Документация flask-base, созданная с помощью mkdocs Второй тип саппорта — это документация. Данная задача очень трудоемкая для разработчиков, но она крайне важна в контексте популярности проекта (и должна быть написана в любом случае). Проще всего создать документы в mkdocs, и можно сгенерировать gh-страницы из mkdocs CLI, которую впоследствии можно перенести на хостинг GitHub. Пользователи получат хорошие примеры того, как можно использовать проект и ознакомятся с объяснением тонкостей благодаря грамотно составленной документации. А также она будет детальным гидом к запуску проекта (если это веб-приложение). Что входит в проект X для Y Список фич должен содержать ключевые особенности, входящие в демо-версию, то есть не должен быть чрезмерно обширным. Максимум 10 пунктов в формате «Х для фичи Y» Как он выглядит Демонстрационный пример функции редактирования страницы администратора flask-base Картинка всегда лучше тысячи слов, поэтому лучше сделать .gif. Покажите как приложение работает, даже если это выход из командной строки. Данная информация предоставит возможность разработчику увидеть а) как будет выглядеть проект б) подходит ли для его нужд. Не стоит недооценивать влияние хорошей графики на выбор разработчика в пользу вашего проекта. Как установить проект Копирование репозитория Инициация Virtualenv (Если вы работаете на маке) Убедитесь, что у вас установлен xcode Добавьте переменные окружения Cоздайте файл типа .env, содержащий переменные окружения, используя следующей синтаксис: ENVIROMENT_VARIABLE=value. Например, переменные окружения почты могут быть заданы таким образом: Мы рекомендуем использовать Sendgrid в качестве почтового SMTP сервера, но любой другой также будет работать без проблем. В процессе разработки вы работаете на одном компьютере, на котором уже установлены все нужные утилиты. Но пользователь должен иметь возможность установить ваш проект и начать с ним работу за 3-4 шага. Если это предполагает создание MakeFile, сделайте это. Обязательно стоит предупредить, если вы использовали «глобальные» инструменты, такие как babel-cli, babel-core. Согласно основному правилу, если пришлось их использовать, то и другим придется это делать. Не забудьте сжать скрипты в единый файл (для Python это requirements.txt, а для node/javascript – package.json). Короче говоря, установка проекта должна занимать не более 5 минут. Как попасть в тренд Удержать пользователей поможет вам A.G.D. (README). Но как привлечь пользователей в ваш проект? Есть три решения: Hacker News/Product Hunt: оба предоставляют отличную возможность осветить проект для заинтересованного круга разработчиков (и получить медиа обзор). Но существует сложность попадания в топы – размещение и продвижение проекта требует тщательного планирования и помощи от пользователей на старте. Reddit: самый эффективный метод получить стартовые звезды для вашего репозитория. Но нужно определить целевую аудиторию. Для нашего продукта этой аудиторией был /r/Python, где мы без труда вышли в топы.Важно обратиться к аудитории, заинтересованной в вашем проекте. Но нужно быть осторожным, если вы публикуете свой пост на таких крупных сабреддитах, как /r/Programming, где ваш пост может запросто затеряться среди прочих. Workshops: не секрет, что воркшопы – отличный способ получить десятки звезд. Сделайте воркшоп о том, как вы создавали проект, как он функционирует и как его использовать. Veronica Wharton and я проводим воркшоп по Flask на PennApps XV Мы использовали этот метод на PennApps XV: с помощью обучающего воркшопа создания веб-приложения с Flask. Аудитория состояла из примерно 40 человек, и мы показали свой продукт как пример Flask-приложения, которое они могли использовать во время хакатона. Через 5 минут после окончания воркшопа мы проверили свои показатели: мы получили 17 звезд и 8 forks, что прибавило нам оптимизма. Мониторинг статуса Комментарии по улучшению. Будьте милы, делитесь своим мнением и радуйтесь обратной связи. Неизбежно появится тот, кто обнаружит баги после запуска вашего проекта. Удостоверьтесь в том, что вы ответите на все комментарии и учтете фидбек. Находиться в контакте с аудиторией является ключевым моментом получения отдачи от пользователей. Если вы получите рост с 30 до 40 звезд за короткий период (1-2 часа), значит ваш проект имеет отличный шанс стать трендом (естественно, данная информация касается алгоритмов работы трендов в GitHub). Топ трендинг Flask-base среди репозиториев Python на Github после 24 часов Наши достижения Проект вышел в топы для репозиториев python, 3 место в тренде и топ для /r/Python за неделю. Hack4Impact стал 4м топовым python-разработчиком и 5м среди всех разработчиков. Кроме этого, у нас более 80 клонов и 40 forks в настоящий момент. Могу сказать, что это безумно приятное ощущение, когда ты видишь, что люди используют код, который ты помогал писать. Письмо с благодарностью, полученное мной Наша аналитика на GitHub. Reddit реально помог. Если вы не смогли попасть в тренды, не отчаивайтесь. Просто встряхнитесь и повторите. Всем иногда везет, а иногда нет. Если вы предприняли попытку создать открытый исходный код, полезный для людей, вы и так уже вносите свой большой вклад в мир open source."], "hab": ["Python", "Open source", "Блог компании Everyday Tools"]}{"url": "https://habrahabr.ru/post/322170/", "title": ["«Hello, (real) world!» на php в 2017 году"], "text": ["Вы наверняка думаете, что писать на php — это просто. И «hello, world» выглядит примерно так так: <?php echo 'Hello, world!'; Конечно, чего еще ожидать от языка с низким порогом входа. Ну да, именно так и было раньше. Много лет назад. Но теперь, в 2017 году никто так уже не делает. Давайте рассмотрим, почему, и попробуем построить наше более реалистичное hello-world приложение по шагам, а их, скажу сразу, получилось не мало. → Полный исходный код «hello,world» можно посмотреть здесь. Для начала надо осознать тот факт, что без фреймворка сейчас приложения никто не делает. Если вы пишете вручную \"echo 'hello, world'\", то обрекаете проект на говнокод на веки вечные (кто потом этот велосипед за вас переписывать будет?). Поэтому возьмем какой-нибудь современный, распространенный в мире фреймворк, например Symfony. Но прежде, чем его устанавливать, надо бы создать базу данных. Зачем базу данных? Ну не хардкодить же строку «hello, world» прямо в тексте программы! База данных В 2017 году принято использовать postgresql. Если вы вдруг еще не умеете его устанавливать, я помогу: sudo apt-get install postgresql Убунта при установке создаст юзера postgres, из под которого можно запустить команду psql с полными правами на базу. sudo -u postgres psql Теперь создадим юзера базы с паролем (придумайте какой-нибудь посложнее). CREATE ROLE helloworlduser WITH PASSWORD '12345' LOGIN; И саму базу: CREATE DATABASE helloworld OWNER helloworlduser; Также надо убедиться, что в pg_hba.conf у вас разрешены коннекты к базе с localhost (127.0.0.1). Там должно быть что-то вроде этого: host all all 127.0.0.1/32 md5 Проверим соединение: psql -h localhost -U helloworlduser helloworld после ввода пароля должно пустить в базу. Сразу создадим таблицу: CREATE TABLE greetings ( id int, greeting text, primary key(id) ); INSERT INTO greetings (id, greeting) VALUES (1, 'Hello, world!'); Ну, супер, с базой всё. Теперь перейдем к фреймворку php-фреймворк Надеюсь, что в 2017 году у всех стоит composer на компьютере. Поэтому сразу перейдем к установке фреймворка composer create-project symfony/framework-standard-edition helloworldphp При установке он сразу спросит параметры соединения с базой: host: 127.0.0.1 database_name: helloworld database_user: helloworlduser database_password: 12345 остальное по умолчанию/по усмотрению. Надо только в конфиге config.yml поменть драйвер на driver: pdo_pgsql. (У вас ведь установлено php-расширение pdo_pgsql ?) Проверим, что всё более менее работает, запустив cd helloworldphp bin/console server:start Симфони запустит свой собственный сервер, который слушает порт 8000 и на нем можно дебажить код. Таким образом в браузере по адресу http://localhost:8000/ должно быть что-то вроде «Это симфони, блаблабла». Уфф! Казалось бы всё, контроллер уже есть, подправить вьюху, создать модель и понеслась, хелло ворлд уже близко! Но… нет. Извините, но не в 2017-ом. В этом году все делают SPA (single page application). Php-программист в 2017 году не может обойтись без js и верстки, теперь мы все full stack, а значит и helloworld должен быть соответствующий. Ну ладно, ладно, еще бывают чистые php-бекенд-разработчики, но давайте возьмем более общий случай JavaScript и его многочисленные друзья Поэтому находим в симфони вьюху (а дефолтная вьюха лежит в app/Resources/view/default/index.html.twig) и стираем там всё, заменяя на: <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> </head> <body> <div id=\"root\"></div> <script src=\"js/bundle.js\"></script> </body> </html> Т.е. всё будет лежат в bundle.js: сжатые javascript файлы прямо вместе со стилями и всем, чем нужно. Как нам создать этот бандл? Нужно написать приложение и настроить webpack для сборки. Webpack (или его аналоги) нам все равно бы понадобились, мы же не будем писать код на чистом javascript в 2017-году, когда typescript явно в тренде. А typescript надо как-то преобразовать в обычную js-ку. Это удобно делать, используя webpack. Разумеется, на чистом typescript тоже никто не пишет. Нужен какой-то фреймворк. Одна из самых модных связок сейчас — это react + redux. А для верстки, так и быть, будем использовать старый добрый олдскульный bootstrap (через sass, конечно же). Нам понадобится куча js-библиотек. У вас ведь стоит nodejs и npm? Убедитесь, что у вас свежий npm и установите пакеты: npm init в зависимостях (в файле package.json) пропишем примерно такое: \"dependencies\": { \"@types/react\": \"^15.0.11\", \"@types/react-dom\": \"^0.14.23\", \"babel-core\": \"^6.23.1\", \"babel-loader\": \"^6.3.2\", \"babel-preset-es2015\": \"^6.22.0\", \"babel-preset-react\": \"^6.23.0\", \"bootstrap-sass\": \"^3.3.7\", \"css-loader\": \"^0.26.1\", \"node-sass\": \"^4.5.0\", \"react\": \"^15.4.2\", \"react-dom\": \"^15.4.2\", \"react-redux\": \"^5.0.2\", \"redux\": \"^3.6.0\", \"resolve-url-loader\": \"^2.0.0\", \"sass-loader\": \"^6.0.1\", \"style-loader\": \"^0.13.1\", \"ts-loader\": \"^2.0.0\", \"typescript\": \"^2.1.6\", \"url-loader\": \"^0.5.7\", \"webpack\": \"^2.2.1\", \"@types/node\": \"^7.0.5\" } И выполним npm install и еще нужно установить: npm install webpack -g чтобы была доступна команда webpack. Увы, это еще далеко не всё. Так как у нас typescript, еще надо создать файл tsconfig.json, примерно такой: tsconfig.json{ \"compilerOptions\": { \"module\": \"es6\", \"moduleResolution\": \"node\", \"sourceMap\": false, \"target\": \"esnext\", \"outDir\": \"web/ts\", \"lib\": [ \"dom\", \"scripthost\", \"es5\", \"es6\", \"es7\" ], \"jsx\": \"react\" }, \"include\": [ \"frontend/**/*.ts\", \"frontend/**/*.tsx\" ] } С конфигами пока что ок, теперь займемся нашим приложением на typescript. Сначала создадим компонент для отображения нашего текста: // файл frontend/components/Greetings.tsx import * as React from 'react'; export interface GreetingsProps { text: string; isReady: boolean; onMount(); } class Greetings extends React.Component<GreetingsProps, undefined> { componentDidMount() { this.props.onMount(); } render() { return ( <h1>{this.props.text}</h1> ); } } export default Greetings; Наше SPA будет подгружать текст надписи через Rest API. React — это просто view-компоненты, а нам еще нужна логика приложения и управление состоянием. Так что будем использовать redux, а также пакет для связи redux и react (react-redux). Поэтому надо будет еще создать компонент, который будет создавать наш компонент Greetings с нужными properties, и сможет сообщить хранилищу (store) состояния, что появилось новое действие (получены данные для отображения). Disclaimer: я только начал изучать redux, поэтому наверняка тут есть за что «бить по рукам». Выглядит этот компонент, допустим, примерно так: // файл frontend/components/App.tsx import * as React from 'react'; import {connect} from 'react-redux' import Greetings from './Greetings'; const mapStateToProps = (state) => { return state; } const mapDispatchToProps = (dispatch) => { return { onMount: () => { fetch(\"/greetings/1\").then((response) => { return response.json(); }).then((json) => { dispatch({type: 'FETCH_GREETING', text: json.greeting}) }); } } } export default connect(mapStateToProps, mapDispatchToProps)(Greetings); Ну и точка входа приложения, создание redux-стора, диспатчера и т.д. Тут всё сделано немного по рабоче-крестьянски, но для хелловорлда сойдет, пожалуй: // подгружает стили bootstrap import 'bootstrap-sass/assets/stylesheets/_bootstrap.scss'; import * as React from 'react'; import * as ReactDOM from \"react-dom\"; import {Provider} from 'react-redux'; import App from './components/App'; import {createStore} from 'redux'; const app = (state = {isReady: false, text: ''}, action) => { switch (action.type) { case 'FETCH_GREETING': return Object.assign({}, state, {isReady: true, text: action.text}); } return state; } const store = createStore(app); ReactDOM.render( <Provider store={store}> <App/> </Provider>, document.getElementById(\"root\") ); Примерно здесь происходит следующее: Первоначальное состояние системы — {isReady: false, text: ''}. Создан reducer под названием app, который умеет обрабатывать действие FETCH_GREETING и возвращать новое состояние системы. Создан store для обработки состояний. Всё отрендеривается в элемент, который мы прописали во вьюхе <div id=\"root\"></div> Ах да, совсем забыл. Конфиг вебпака: webpack.config.jsconst webpack = require('webpack'); const path = require('path'); const ENVIRONMENT = process.env.NODE_ENV || 'development'; let config = { context: path.resolve(__dirname, \"frontend\"), entry: './index.tsx', output: { filename: 'bundle.js', path: path.resolve(__dirname, \"web/js\") }, resolve: { extensions: [ \".js\", \".jsx\", '.ts', '.tsx'] }, module: { rules: [ { test: /\\.tsx?$/, use: [{ loader: 'babel-loader', query: { presets: ['es2015', 'react'] } }, { loader: 'ts-loader' }] }, { test: /\\.woff($|\\?)|\\.woff2($|\\?)|\\.ttf($|\\?)|\\.eot($|\\?)|\\.svg($|\\?)/, loader: 'url-loader' }, { test: /\\.scss$/, use: [ { loader: \"style-loader\" }, { loader: \"css-loader\" }, { loader: \"resolve-url-loader\" }, { loader: \"sass-loader\" } ] } ] }, plugins: [ new webpack.DefinePlugin({ 'process.env.NODE_ENV': JSON.stringify(ENVIRONMENT) }) ], node: { process: false } }; if (ENVIRONMENT == 'production') { config.plugins.push( new webpack.optimize.UglifyJsPlugin({ compress: { drop_console: false, warnings: false } }) ); } module.exports = config; Теперь мы можем запустить webpack или NODE_ENV=production webpack (чтобы получить минифицированную версию bundle.js) Pomodoro Не знаю как вы, а я уже задолбался писать этот hello, world. В 2017 году надо работать эффективно, а это подразумевает, что надо делать перерывы в работе (метод Pomodoro и т.д.). Так что, пожалуй, прервусь не надолго. [прошло какое-то время] Давайте продолжим. Мы уже умеем подгружать код с /greetings/1 на стороне javascript, но php-часть еще совершенно не готова. Doctrine Уже потрачено много времени, а в php-коде не создано ни одной сущности. Давайте исправим положение: <?php // src/AppBundle/Entity/Greeting.php namespace AppBundle\\Entity; use Doctrine\\ORM\\Mapping as ORM; /** * @ORM\\Entity * @ORM\\Table(name=\"greetings\") */ class Greeting { /** * @ORM\\Column(type=\"integer\") * @ORM\\Id */ private $id; /** * @ORM\\Column(type=\"string\", length=100) */ private $greeting; public function getId() { return $this->id; } public function getGreeting() { return $this->greeting; } } Супер. Осталось совсем чуть-чуть. REST Надо сделать-таки простенький REST API, который может хотя бы отдать json по запросу GET /greetings/1 Для этого в контроллере (файл src/AppBundle/Controller/DefaultController.php) добавим метод с роутом: /** * @Route(\"/greetings/{id}\") */ public function greetings($id) { $greeting = $this->getDoctrine()->getRepository(\"AppBundle:Greeting\")->find($id); return new JsonResponse(['greeting' => $greeting->getGreeting()]); } Всё, можно запускать. На экране отображается «Hello, world!». Внешне он, конечно, выглядит почти также как результат <?php echo «hello, world» ?> (если не считать бутстраповского шрифта), но теперь это современное приложение по всем канонам. Ну, скажем так, почти по всем канонам (не хватает тестов, проверок ошибок и много чего еще), но я уже задолбался это делать :) Выводы В последнее время сильно участились споры «зачем нужен php, если есть java». Уж не знаю, кто прав, а кто нет, холивары — дело такое. Но в каждом споре один из аргументов в пользу php — это простота для новичков. Как мне кажется, этот аргумент уже давно не валиден, что я и хотел показать этой статьёй. Новичку все равно придется кучу всего узнать и 100500 конфигов настроить: фреймворки (очень похожие на фреймворки java), базы данных, linux, javascript со всем своим зоопарком, верстка, http-протокол, различный тулинг и многое-многое другое. Даже если это не SPA. Upd. Статья уходит в глубокий минус, но я не собираюсь менять мнение. Оно примерно такое: 1) SPA всё больше проникает в наш мир, и надо это уметь, хотя бы в общих чертах. 2) Без фреймворков не построишь хорошее современное приложение."], "hab": ["Разработка веб-сайтов", "Symfony", "ReactJS", "PHP", "JavaScript"]}{"url": "https://habrahabr.ru/company/mailru/blog/322416/", "title": ["Выбор правильной стратегии обработки ошибок (части 1 и 2)", "перевод"], "text": ["Существует две фундаментальные стратегии: обработка исправимых ошибок (исключения, коды возврата по ошибке, функции-обработчики) и неисправимых (assert(), abort()). В каких случаях какую стратегию лучше использовать? Виды ошибок Ошибки возникают по разным причинам: пользователь ввёл странные данные, ОС не может дать вам обработчика файла или код разыменовывает (dereferences) nullptr. Каждая из описанных ошибок требует к себе отдельного подхода. По причинам ошибки делятся на три основные категории: Пользовательские ошибки: здесь под пользователем подразумевается человек, сидящий перед компьютером и действительно «использующий» программу, а не какой-то программист, дёргающий ваш API. Такие ошибки возникают тогда, когда пользователь делает что-то неправильно. Системные ошибки появляются, когда ОС не может выполнить ваш запрос. Иными словами, причина системных ошибок — сбой вызова системного API. Некоторые возникают потому, что программист передал системному вызову плохие параметры, так что это скорее программистская ошибка, а не системная. Программистские ошибки случаются, когда программист не учитывает предварительные условия API или языка программирования. Если API требует, чтобы вы не вызывали foo() с 0 в качестве первого параметра, а вы это сделали, — виноват программист. Если пользователь ввёл 0, который был передан foo(), а программист не написал проверку вводимых данных, то это опять же его вина. Каждая из описанных категорий ошибок требует особого подхода к их обработке. Пользовательские ошибки Сделаю очень громкое заявление: такие ошибки — на самом деле не ошибки. Все пользователи не соблюдают инструкции. Программист, имеющий дело с данными, которые вводят люди, должен ожидать, что вводить будут именно плохие данные. Поэтому первым делом нужно проверять их на валидность, сообщать пользователю об обнаруженных ошибках и просить ввести заново. Поэтому не имеет смысла применять к пользовательским ошибкам какие-либо стратегии обработки. Вводимые данные нужно как можно скорее проверять, чтобы ошибок не возникало. Конечно, такое не всегда возможно. Иногда проверять вводимые данные слишком дорого, иногда это не позволяет сделать архитектура кода или разделение ответственности. Но в таких случаях ошибки должны обрабатываться однозначно как исправимые. Иначе, допустим, ваша офисная программа будет падать из-за того, что вы нажали backspace в пустом документе, или ваша игра станет вылетать при попытке выстрелить из разряженного оружия. Если в качестве стратегии обработки исправимых ошибок вы предпочитаете исключения, то будьте осторожны: исключения предназначены только для исключительных ситуаций, к которым не относится большинство случаев ввода пользователями неверных данных. По сути, это даже норма, по мнению многих приложений. Используйте исключения только тогда, когда пользовательские ошибки обнаруживаются в глубине стека вызовов, вероятно, внешнего кода, когда они возникают редко или проявляются очень жёстко. В противном случае лучше сообщать об ошибках с помощью кодов возврата. Системные ошибки Обычно системные ошибки нельзя предсказать. Более того, они недетерминистские и могут возникать в программах, которые до этого работали без нареканий. В отличие от пользовательских ошибок, зависящих исключительно от вводимых данных, системные ошибки — настоящие ошибки. Но как их обрабатывать, как исправимые или неисправимые? Это зависит от обстоятельств. Многие считают, что ошибка нехватки памяти — неисправимая. Зачастую не хватает памяти даже для обработки этой ошибки! И тогда приходится просто сразу же прерывать выполнение. Но падение программы из-за того, что ОС не может выделить сокет, — это не слишком дружелюбное поведение. Так что лучше бросить исключение и позволить catch аккуратно закрыть программу. Но бросание исключения — не всегда правильный выбор. Кто-то даже скажет, что он всегда неправильный. Если вы хотите повторить операцию после её сбоя, то обёртывание функции в try-catch в цикле — медленное решение. Правильный выбор — возврат кода ошибки и цикличное исполнение, пока не будет возвращено правильное значение. Если вы создаёте вызов API только для себя, то просто выберите подходящий для своей ситуации путь и следуйте ему. Но если вы пишете библиотеку, то не знаете, чего хотят пользователи. Дальше мы разберём подходящую стратегию для этого случая. Для потенциально неисправимых ошибок подойдёт «обработчик ошибок», а при других ошибках необходимо предоставить два варианта развития событий. Обратите внимание, что не следует использовать подтверждения (assertions), включающиеся только в режиме отладки. Ведь системные ошибки могут возникать и в релизной сборке! Программистские ошибки Это худший вид ошибок. Для их обработки я стараюсь сделать так, чтобы мои ошибки были связаны только с вызовами функций, то есть с плохими параметрами. Прочие типы программистских ошибок могут быть пойманы только в runtime, с помощью отладочных макросов (assertion macros), раскиданных по коду. При работе с плохими параметрами есть две стратегии: дать им определённое или неопределённое поведение. Если исходное требование для функции — запрет на передачу ей плохих параметров, то, если их передать, это считается неопределённым поведением и должно проверяться не самой функцией, а оператором вызова (caller). Функция должна делать только отладочное подтверждение (debug assertion). С другой стороны, если отсутствие плохих параметров не является частью исходных требований, а документация определяет, что функция будет бросать bad_parameter_exception при передаче ей плохого параметра, то передача — это хорошо определённое поведение (бросание исключения или любая другая стратегия обработки исправимых ошибок), и функция всегда должна это проверять. В качестве примера рассмотрим получающие функции (accessor functions) std::vector<T>: в спецификации на operator[] говорится, что индекс должен быть в пределах валидного диапазона, при этом at() сообщает нам, что функция кинет исключение, если индекс не попадает в диапазон. Более того, большинство реализаций стандартных библиотек обеспечивают режим отладки, в котором проверяется индекс operator[], но технически это неопределённое поведение, оно не обязано проверяться. Примечание: необязательно бросать исключение, чтобы получилось определённое поведение. Пока это не упомянуто в исходных условиях для функции, это считается определённым. Всё, что прописано в исходных условиях, не должно проверяться функцией, это неопределённое поведение. Когда нужно проверять только с помощью отладочных подтверждений, а когда — постоянно? К сожалению, однозначного рецепта нет, решение зависит от конкретной ситуации. У меня есть лишь одно проверенное правило, которому я следую при разработке API. Оно основано на наблюдении, что проверять исходные условия должен вызывающий, а не вызываемый. А значит, условие должно быть «проверяемым» для вызывающего. Также условие «проверяемое», если можно легко выполнить операцию, при которой значение параметра всегда будет правильным. Если для параметра это возможно, то это получается исходное условие, а значит, проверяется только посредством отладочного подтверждения (а если слишком дорого, то вообще не проверяется). Но конечное решение зависит от многих других факторов, так что очень трудно дать какой-то общий совет. По умолчанию я стараюсь свести к неопределённому поведению и использованию только подтверждений. Иногда бывает целесообразно обеспечить оба варианта, как это делает стандартная библиотека с operator[] и at(). Хотя в ряде случаев это может быть ошибкой. Об иерархии std::exception Если в качестве стратегии обработки исправимых ошибок вы выбрали исключения, то рекомендуется создать новый класс и наследовать его от одного из классов исключений стандартной библиотеки. Я предлагаю наследовать только от одного из этих четырёх классов: std::bad_alloc: для сбоев выделения памяти. std::runtime_error: для общих runtime-ошибок. std::system_error (производное от std::runtime_error): для системных ошибок с кодами ошибок. std::logic_error: для программистских ошибок с определённым поведением. Обратите внимание, что в стандартной библиотеке разделяются логические (то есть программистские) и runtime-ошибки. Runtime-ошибки — более широкое определение, чем «системные». Оно описывает «ошибки, обнаруживаемые только при выполнении программы». Такая формулировка не слишком информативна. Лично я использую её для плохих параметров, которые не являются исключительно программистскими ошибками, а могут возникнуть и по вине пользователей. Но это можно определить лишь глубоко в стеке вызовов. Например, плохое форматирование комментариев в standardese приводит к исключению при парсинге, проистекающему из std::runtime_error. Позднее оно ловится на соответствующем уровне и фиксируется в логе. Но я не стал бы использовать этот класс иначе, как и std::logic_error. Подведём итоги Есть два пути обработки ошибок: как исправимые: используются исключения или возвращаемые значения (в зависимости от ситуации/религии); как неисправимые: ошибки журналируются, а программа прерывается. Подтверждения — это особый вид стратегии обработки неисправимых ошибок, только в режиме отладки. Есть три основных источника ошибок, каждый требует особого подхода: Пользовательские ошибки не должны обрабатываться как ошибки на верхних уровнях программы. Всё, что вводит пользователь, должно проверяться соответствующим образом. Это может обрабатываться как ошибки только на нижних уровнях, которые не взаимодействуют с пользователями напрямую. Применяется стратегия обработки исправимых ошибок. Системные ошибки могут обрабатываться в рамках любой из двух стратегий, в зависимости от типа и тяжести. Библиотеки должны работать как можно гибче. Программистские ошибки, то есть плохие параметры, могут быть запрещены исходными условиями. В этом случае функция должна использовать только проверку с помощью отладочных подтверждений. Если же речь идёт о полностью определённом поведении, то функции следует предписанным образом сообщать об ошибке. Я стараюсь по умолчанию следовать сценарию с неопределённым поведением и определяю для функции проверку параметров лишь тогда, когда это слишком трудно сделать на стороне вызывающего. Гибкие методики обработки ошибок в C++ Иногда что-то не работает. Пользователи вводят данные в недопустимом формате, файл не обнаруживается, сетевое соединение сбоит, в системе кончается память. Всё это ошибки, и их надо обрабатывать. Это относительно легко сделать в высокоуровневых функциях. Вы точно знаете, почему что-то пошло не так, и можете обработать это соответствующим образом. Но в случае с низкоуровневыми функциями всё не так просто. Они не знают, что пошло не так, они знают лишь о самом факте сбоя и должны сообщить об этом тому, кто их вызвал. В C++ есть два основных подхода: коды возврата ошибок и исключения. Сегодня широко распространено использование исключений. Но некоторые не могут / думают, что не могут / не хотят их использовать — по разным причинам. Я не буду принимать чью-либо сторону. Вместо этого я опишу методики, которые удовлетворят сторонников обоих подходов. Особенно методики пригодятся разработчикам библиотек. Проблема Я работаю над проектом foonathan/memory. Это решение предоставляет различные классы выделения памяти (allocator classes), так что в качестве примера рассмотрим структуру функции выделения. Для простоты возьмём malloc(). Она возвращает указатель на выделяемую память. Если выделить память не получается, то возвращается nullptr, то есть NULL, то есть ошибочное значение. У этого решения есть недостатки: вам нужно проверять каждый вызов malloc(). Если вы забудете это сделать, то выделите несуществующую память. Кроме того, по своей натуре коды ошибок транзитивны: если вызвать функцию, которая может вернуть код ошибки, и вы не можете его проигнорировать или обработать, то вы тоже должны вернуть код ошибки. Это приводит нас к ситуации, когда чередуются нормальные и ошибочные ветви кода. Исключения в таком случае выглядят более подходящим решением. Благодаря им вы сможете обрабатывать ошибки только тогда, когда вам это нужно, а в противном случае — достаточно тихо передать их обратно вызывающему. Это можно расценить как недостаток. Но в подобных ситуациях исключения имеют также очень большое преимущество: функция выделения памяти либо возвращает валидную память, либо вообще ничего не возвращает. Это функция «всё или ничего», возвращаемое значение всегда будет валидным. Это полезное следствие согласно принципу Скотта Майера «Make interfaces hard to use incorrectly and easy to use correctly». Учитывая вышесказанное, можно утверждать, что вам следует использовать исключения в качестве механизма обработки ошибок. Этого мнения придерживается большинство разработчиков на С++, включая и меня. Но проект, которым я занимаюсь, — это библиотека, предоставляющая средства выделения памяти, и предназначена она для приложений, работающих в реальном времени. Для большинства разработчиков подобных приложений (особенно для игроделов) само использование исключений — исключение. Каламбур детектед. Чтобы уважить эту группу разработчиков, моей библиотеке лучше обойтись без исключений. Но мне и многим другим они нравятся за элегантность и простоту обработки ошибок, так что ради других разработчиков моей библиотеке лучше использовать исключения. Так что же делать? Идеальное решение: возможность включать и отключать исключения по желанию. Но, учитывая природу исключений, нельзя просто менять их местами с кодами ошибок, поскольку у нас не будет внутреннего кода проверки на ошибки — весь внутренний код опирается на предположение о прозрачности исключений. И даже если бы внутри можно было использовать коды ошибок и преобразовывать их в исключения, это лишило бы нас большинства преимуществ последних. К счастью, я могу определить, что вы делаете, когда обнаруживаете ошибку нехватки памяти: чаще всего вы журналируете это событие и прерываете программу, поскольку она не может корректно работать без памяти. В таких ситуациях исключения — просто способ передачи контроля другой части кода, которая журналирует и прерывает программу. Но есть старый и эффективный способ передачи контроля: указатель функции (function pointer), то есть функция-обработчик (handler function). Если у вас включены исключения, то вы просто их бросаете. В противном случае вызываете функцию-обработчика и затем прерываете программу. Это предотвратит бесполезную работу функции-обработчика, та позволит программе продолжить выполняться в обычном режиме. Если не прервать, то произойдёт нарушение обязательного постусловия функции: всегда возвращать валидный указатель. Ведь на выполнении этого условия может быть построена работа другого кода, да и вообще это нормальное поведение. Я называю такой подход обработкой исключений и придерживаюсь его при работе с памятью. Решение 1: обработчик исключений Если вам нужно обработать ошибку в условиях, когда наиболее распространённым поведением будет «журналировать и прервать», то можно использовать обработчика исключений. Это такая функция-обработчик, которая вызывается вместо бросания объекта-исключения. Её довольно легко реализовать даже в уже существующем коде. Для этого нужно поместить управление обработкой в класс исключений и обернуть в макрос выражение throw. Сначала дополним класс и добавим функции для настройки и, возможно, запрашивания функции-обработчика. Я предлагаю делать это так же, как стандартная библиотека обрабатывает std::new_handler: class my_fatal_error { public: // тип обработчика, он должен брать те же параметры, что и конструктор, // чтобы у них была одинаковая информация using handler = void(*)( ... ); // меняет функцию-обработчика handler set_handler(handler h); // возвращает текущего обработчика handler get_handler(); ... // нормальное исключение }; Поскольку это входит в область видимости класса исключений, вам не нужно именовать каким-то особым образом. Отлично, нам же легче. Если исключения включены, то для удаления обработчика можно использовать условное компилирование (conditional compilation). Если хотите, то также напишите обычный подмешанный класс (mixin class), дающий требуемую функциональность. Конструктор исключений элегантен: он вызывает текущую функцию-обработчика, передавая ей требуемые аргументы из своих параметров. А затем комбинирует с последующим макросом throw: If```cpp #if EXCEPTIONS #define THROW(Ex) throw (Ex) #else #define THROW(Ex) (Ex), std::abort() #endif > Такой макрос throw также предоставляется [foonathan/compatiblity](https://github.com/foonathan/compatibility). Можно использовать его и так: ```cpp THROW(my_fatal_error(...)) Если у вас включена поддержка исключений, то будет создан и брошен объект-исключение, всё как обычно. Но если поддержка выключена, то объект-исключение всё равно будет создан, и — это важно — только после этого произойдёт вызов std::abort(). А поскольку конструктор вызывает функцию-обработчика, то он и работает, как требуется: вы получаете точку настройки для журналирования ошибки. Благодаря же вызову std::abort() после конструктора пользователь не может нарушить постусловие. Когда я работаю с памятью, то при включённых исключениях у меня также включён и обработчик, который вызывается при бросании исключения. Так что при этой методике вам ещё будет доступна определённая степень кастомизации, даже если вы отключите исключения. Конечно, замена неполноценная, мы только журналируем и прерываем работу программы, без дальнейшего продолжения. Но в ряде случаев, в том числе при исчерпании памяти, это вполне пригодное решение. А если я хочу продолжить работу после бросания исключения? Методика с обработчиком исключений не позволяет этого сделать в связи с постусловием кода. Как же тогда продолжить работу? Ответ прост — никак. По крайней мере, это нельзя сделать так же просто, как в других случаях. Нельзя просто так вернуть код ошибки вместо исключения, если функция на это не рассчитана. Есть только одно решение: сделать две функции. Одна возвращает код ошибки, а вторая бросает исключения. Клиенты, которым нужны исключения, будут использовать второй вариант, остальные — первый. Извините, что говорю такие очевидные вещи, но ради полноты изложения я должен был об этом сказать. Для примера снова возьмём функцию выделения памяти. В этом случае я использую такие функции: void* try_malloc(..., int &error_code) noexcept; void* malloc(...); При сбое выделения памяти первая версия возвращает nullptr и устанавливает error_code в коде ошибки. Вторая версия не возвращает nullptr, зато бросает исключение. Обратите внимание, что в рамках первой версии очень легко реализовать вторую: void* malloc(...) { auto error_code = 0; auto res = try_malloc(..., error_code); if (!res) throw malloc_error(error_code); return res; } Не делайте этого в обратной последовательности, иначе вам придётся ловить исключение, а это дорого. Также это не даст нам скомпилировать код без включённой поддержки исключений. Если сделаете, как показано, то можете просто стереть другую перегрузку (overload) с помощью условного компилирования. Но даже если у вас включена поддержка исключений, клиенту всё равно может понадобиться вторая версия. Например, когда нужно выделить наибольший возможный объём памяти, как в нашем примере. Будет проще и быстрее вызывать в цикле и проверять по условию, чем ловить исключение. Решение 2: предоставить две перегрузки Если недостаточно обработчика исключений, то нужно предоставить две перегрузки. Одна использует код возврата, а вторая бросает исключение. Если рассматриваемая функция не имеет возвращаемого значения, то можете её использовать для кода ошибки. В противном случае вам придётся возвращать недопустимое значение для сигнализирования об ошибке — как nullptr в вышеприведённом примере, — а также установить выходной параметр для кода ошибки, если хотите предоставить вызывающему дополнительную информацию. Пожалуйста, не используйте глобальную переменную errno или что-то типа GetLastError()! Если возвращаемое значение не содержит недопустимое значение для обозначения сбоя, то по мере возможности используйте std::optional или что-то похожее. Перегрузка исключения (exception overload) может — и должна — быть реализована в рамках версии с кодом ошибки, как это показано выше. Если компилируете без исключений, сотрите перегрузку с помощью условного компилирования. std::system_error Подобная система идеально подходит для работы с кодами ошибок в С++ 11. Она возвращает непортируемый (non-portable) код ошибки std::error_code, то есть возвращаемый функцией операционной системы. С помощью сложной системы библиотечных средств и категорий ошибок вы можете добавить собственные коды ошибок, или портируемые std::error_condition. Для начала почитайте об этом здесь. Если нужно, то можете использовать в функции кода ошибки std::error_code. А для функции исключения есть подходящий класс исключения: std::system_error. Он берёт std::error_code и применяется для передачи этих ошибок в виде исключений. Эту или подобную систему должны использовать все низкоуровневые функции, являющиеся закрытыми обёртками ОС-функций. Это хорошая — хотя и сложная — альтернатива службе кодов ошибок, предоставляемой операционной системой. Да, и мне ещё нужно добавить подобное в функции виртуальной памяти. На сегодняшний день они не предоставляют коды ошибок. std::expected Выше упоминалось о проблеме, когда у вас нет возвращаемого значения, содержащего недопустимое значение, которое можно использовать для сигнализирования об ошибке. Более того, выходной параметр — не лучший способ получения кода ошибки. А глобальные переменные вообще не вариант! В № 4109 предложено решение: std::expected. Это шаблон класса, который также хранит возвращаемое значение или код ошибки. В вышеприведённом примере он мог бы использоваться так: std::expected<void*, std::error_code> try_malloc(...); В случае успеха std::expected будет хранить не-null указатель памяти, а при сбое — std::error_code. Сейчас эта методика работает при любых возвращаемых значениях. Комбинация std::expected и функции исключения определённо допускает любые варианты использования. Заключение Если вы создаёте библиотеки, то иногда приходится обеспечивать максимальную гибкость использования. Под этим подразумевается и разнообразие средств обработки ошибок: иногда требуются коды возврата, иногда — исключения. Одна из возможных стратегий — улаживание этих противоречий с помощью обработчика исключений. Просто удостоверьтесь, что когда нужно, то вызывается callback, а не бросается исключение. Это замена для критических ошибок, которая в любом случае будет журналироваться перед прерыванием работы программы. Как таковой этот способ не универсален, вы не можете переключаться в одной программе между двумя версиями. Это лишь обходное решение при отключённой поддержке исключений. Более гибкий подход — просто предоставить две перегрузки, одну с исключениями, а вторую без. Это даст пользователям максимальную свободу, они смогут выбирать ту версию, что лучше подходит в их ситуации. Недостаток этого подхода: вам придётся больше потрудиться при создании библиотеки."], "hab": ["Совершенный код", "Проектирование и рефакторинг", "Анализ и проектирование систем", "C++", "Блог компании Mail.Ru Group"]}{"url": "https://habrahabr.ru/post/322332/", "title": ["Почему не нужно учить python первым языком"], "text": ["Если вы будете искать ответ на вопрос: «Какой язык программирования выбрать первым», то где-то в 90% всех случаев вам будет предложен Python — как наиболее простой в изучении язык. И очевидно, что определенное число людей, которые до этого не учили программирование, выберут Python из-за этих рекомендаций. И вот тут у нас начинается проблема, о которой пойдет речь ниже. Конечно, с описанием того, как я дошел до такой жизни. О себе Еще в студенческие годы я понял, что моя специальность не такая уж радужная, как мне казалось в 18 лет. Поэтому я стал думать о том, как заработать адекватные деньги. И наслушавшись историй о том, как мой двоюродный брат получал безумные на то время деньги в 1С, я также решил связать свою жизнь с IT. Изначально это были шаблонные сайты на конструкторах и wordpress, потом я занялся SEO, и в один момент наткнулся на Хабр, после чего решил стать полноценным программистом. Высшей математики у меня не было, поэтому я решил выбрать сферу, где она не требуется – веб-разработка. У меня появился очевидный вопрос: какой язык выбрать – php/python/ruby. Насмотревшись статей на Хабре, почитал хейт в сторону php, посмотрев пару мотивационных роликов от Yandex. Я выбрал Python. Преимущества языка, я надеюсь, вы знаете, поэтому не буду про это говорить. Первичное обучение языку Обучение языку я совмещал с основной работой, поэтому читал книжки, смотрел туториалы, пилил небольшие проекты в вечернее время. В общем, за год я 1) Изучил книги: Марк Лутц — Изучаем Python Марк Лутц — Программирование на Python Чед Фаулер – Программист Фанатик Билл Любанович – Простой Python 2) Изучил множество роликов от Украинских/Буржуйских авторов по Django 3) Прошел курс от codeacademy 4) Освоил PyCharm Свой первый проект Далее у меня появилась идея небольшого сервиса на весьма специфичную тематику, который я решил сделать, чтобы закрепить знания Python + Django. В создания сайта я 1) Изучил книги: Джон Дакетт — HTML и CSS. Разработка и дизайн веб-сайтов Дэвид Флэнаган — JavaScript. Подробное руководство Бен Форта — Освой самостоятельно SQL. 2) Изучил документацию Django под свои задачи 3) Изучил деплой проектов на Django Gunicorn + nginx + centOS Свой первый нормальный проект После того, как первый адекватный сайт провалился, я решил создать уже что-то стоящее, выбрал идею, выбрал схему реализации и за 3 месяца по вечерам его сделал. Проект показал свою жизнеспособность (по сей день приносит мне определенные деньги, чему я безумно рад). И я решил уже его прокачать получше. После прочтения книги «Percival H. — Test-Driven Development with Python», решил написать тесты сначала на основе компонентов Django, потом поднял документацию селениума, и уже сделал внешние тесты. Я хочу быть крутым Открыв вакансии по Python-Django разработчикам, я посмотрел что еще обычно требуется в таких вакансиях: Django Rest Framework Celery Tornado/Twisted/ asyncio (На выбор что-то одно) Class-based view Django Angular/React (На выбор что-то одно) Потратил 3 месяца на знакомство/пробование с этими штуками. Также поднял стандартную библиотеку Python + внешняя библиотека для парсинга beautifulSoup. Ты не тру без C/C++ Бытует мнение, что без знания C/C++ программист не может называть себя программистом. Поэтому когда у меня было свободное время, я познакомился с книгами: Брайн Керниган – Язык программирования С Стенли Б ЛиппМан – Язык программирования С++. Базовый курс Прочитал книги, поковырялся с кодом, посмотрел на компиляцию, посмотрел примеры кода. В общем, теперь я не делал большие глаза при упоминании ссылок, указателей, сортировок, ООП и туче разных массивов с разными скоростями обработки элемента, в зависимости от его позиции. Я готов к бою! И вот тут мы приходим к самому важному моменту. Потратив в общей массе 2 года на изучение всех элементов веб-программирования, о которых я говорил выше. Я посчитал себя достаточно готовым, чтобы претендовать на позицию веб-разработчика на Python. Конечно, что-то я знал не очень хорошо, что-то поверхносто, а что-то вообще не знал (например, Flask), но общее понимание и навыки были неплохими. И вот тут начались проблемы с Python, на которых люди чаще всего не заостряют внимание. А именно на востребованности бизнеса в Python-разработчиков junior/pre-middle уровня. С этим вопросом я вышел на рынок Хотя на первый взгляд кажется, что вакансий на Python достаточно много, но когда начинается конкретика, все резко меняется. 1. Сразу идет большой отсев вакансий, где Python является исключительно вспомогательным языком. Чаще всего это позиции Java-разработчиков, Системных Администраторов, QA-Автоматизация. Также сейчас идет большой отсев по Data Learning, где требуется мат-образование + язык R. Т.е. с одним Python вы эту вакансию не сможете подобрать. 2. Оказалось, что в моем городе вакансий под Python нет, от слова вообще нет. Расширив поиск по всей области, я также получил неудовлетворительный результат. Пару вакансий на PHP, где Python шел «будет плюсом». Открыв фильтр за последние 3 года, я также обнаружил, что вакансий на Python не было вовсе. Т.е. бизнес в провинции чаще всего выбирает более простые и популярные технологии, нежели Python. 3. Открыв вакансии на Python в общем поисковике, я обнаружил следующие тенденции: 90% + вакансий находятся в Москве или Санкт-Петербурге 90% + вакансий требуют уровень middle+ / seniour ~100% вакансий junior позиций в Москве или Санкт-Петербурге (чаще всего от гигантов) Другими словами получилась ситуация, что если ты не живешь в Москве, Санкт-Петербурге и не собираешься ехать их «покорять», то тебе практически негде получить свою первую работу. Конечно, есть пару очагов, где Python еще используется, например, в Казани. Но чаще всего это какая-то одна фирма, где с Вакансиями тоже весьма middle+ / seniour. 4. Вариант поиска удаленки на текущий уровень также показал, что работодатели не готовы идти на такой риск. Мало опыта + удаленка = это какая-то фантастика. Тем не менее, я все же смог найти пару вариантов, но уже в ходе первичного собеседования стало понятно, что это ерунда по типу: «Ты у нас три месяца поработай, и если клиент заплатит за твою работу, мы тебе тоже заплатим». Не самый лучший вариант. 5. Поговорил с парой HR из крупных компаний, они высказали такую тенденцию. «Мы обычно берем людей с опытом на Python от года, плюс опытом на другого языке (3+ года). Чаще всего php/Java». Другими словами, они вообще не рассматривали варианты, чтобы взять человека с одним лишь Python. 6. Поговорив с ребятами с профильных форумов, стало понятно, что это достаточно типичная ситуация. Из их рассказов стало понятно, что люди после тщетных поисков либо шли работать на php/1c, либо как-то пролазили через upwork/собственный проект/автоматизацию тестирования. Но опять же от случая к случаю. В общем, оказалось, что Python – это отличный язык, который позволяет делать мощные проекты. И так уж сложилось, что их концентрация находится в столицах. И раз это сложные проекты, то и сотрудники туда требуются уже уровня middle+. Готов ли человек, который только что изучил Python получить такую вакансию? Трудно! Но есть другой путь! В настоящий момент только в моем городе находится 24 вакансии на php различного уровня (начиная от небольших компаний, которым нужно поддерживать текущий сайт, заканчивая гигантами e-commerce, которые предлагают последовательное расширение функционала). И примерно столько же вакансии на 1С. И где-то на половине из этих вакансий готовы взять человека, который хотя бы что-то знает в программировании. Скорее всего, это не самые лучшие места, но это уже первая работа, после который вы официально для HR станете программистом с опытом. Так что в итоге Получается ситуация, что можно изучить клевый язык программирования Python и остаться на улице. А можно выучить «ненавистный» php/1c и получить работу. Качество этой работы, конечно же, оставляет много вопросов – но это уже опыт. Что касается меня, то в моих условиях (не ехать в Москву/СПб) я фактически потратил время на изучение языка, который сейчас востребован исключительно в моих собственных проектах. Найти работу на месте или удаленке у меня не получилось. Сейчас иду в сторону php, так как на нем банально есть работа. Поэтому если вы не живете в Москве, СПб, не являетесь студентом тех-вуза, то я бы не советовал вам учить Python первым языком. Обратите внимание на PHP – под него всегда есть места, есть работа, есть опыт. А дальнейший путь развития уже за вами. P.S. Как подсказал мне мой знакомый, на Ruby почти такая же ситуация. Но тут я уже говорить с уверенностью не могу."], "hab": ["Разработка веб-сайтов", "Python", "PHP"]}{"url": "https://habrahabr.ru/post/322702/", "title": ["Механический Шекспир: способны ли машины на литературное творчество?"], "text": ["Была ночь, огни Бориспольской трассы пролетали мимо окон такси. Водитель выключил музыку, невыносимо давившую мне на мозг после тяжелого перелета, и, чтобы не заснуть, начал говорить. Сначала, конечно, о политике, «довели страну», и все в таком роде, потом о чем-то личном. Я тоже не хотел отключаться прямо на переднем сидении, поэтому пытался его слушать. —… И тогда нам всем придет конец, — донеслись до меня обрывки фразы. — Точнее только им, не мне. Я надежно подстраховался. Когда их всех: водителей такси, маршруток, даже трамваев выкинут на улицу, меня уже там не будет. Я буду сидеть в тепле, пить кофе и громко-громко смеяться. — Почему-почему их выкинут на улицу? — заспанно переспросил я. — Ты что, про Убер не слышал? Что они с водителями делают — только репетиция, да. Скоро, уже очень скоро они запустят свои автопилоты. Это будет дешевле, безопаснее, круче! Всех этих бездарностей ждет работа на стройке. Или бомжатник. Но не меня, я умнее их. — Да? — протер я глаза. — Да! Я роман начал писать! Остросюжетный! Знаешь, у меня тут, — постучал пальцем по виску, — столько сюжетов сидит — закидайся. Вожу это разных людей, каждый что-то ляпнет. А память у меня с детства — дай Бог каждому. Кто у меня здесь только не ездил: трансвестит, сделавший уже три операции, все никак определиться не может; элитная проститутка — всю дорогу тараторила о шумерской клинописи; любовник министра одного ехал, столько всего нарассказывал… Имена только меняй и пиши — блокбастер будет! —А ... — Единственное, что пишу я пока не очень. Скачал себе пару учебников, читаю после работы. Каждый вечер для тренировки записываю, что услышал за день. Кстати, может ты что-нибудь интересное расскажешь, а то сегодня почему-то голяк полный. Могу даже скидку сделать. — Что у меня интересного… Я программист, лечу вот из ... — Слышь, программист, — перебил меня водитель, — о жизни своей можешь не рассказывать. Она у вас всех одинаковая, неинтересно. Скажи мне лучше другое: эти роботы, которые машины научились водить, они романы скоро писать начнут? Только честно. Я не обижусь, не боись. Просто если скоро — в моем плане нет никакого смысла, нужно  на программиста идти тогда, кому-то ж этих роботов ремонтировать потом, настраивать ... Я, честно сказать, растерялся. С одной стороны, насколько я на тот момент знал, вроде ни один алгоритм еще не умел писать даже приблизительно осмысленный текст, не говоря уже о художественном. А с другой, брать на себя сейчас такую ​​ответственность за чужую жизнь, сказать, что профессия писателя в безопасности, а потом, спустя пару лет, ждать этого таксиста с обрезом у своего дома? — Давай так, — сказал я ему после неловкой паузы. Я тут немного погуглю, почитаю, осмыслю. И напишу статью. Может кому-то еще будет интересно, у кого те же проблемы. Потом я расплатился и ушел досыпать после тяжелой дороги. Утром о своем обещании конечно же забыл. Вспомнил чуть позже, а там уже совесть замучила: пришлось действительно писать. Надеюсь, мой случайный друг, (я так и не спросил, как тебя зовут), ты поверил невыспавшемуся пассажиру и читаешь сейчас эту статью. А если нет — черт с тобой. Кому-то пригодится. Так вот. Пессимисты прошлого На первый взгляд, задача программного написания художественных произведений выглядит просто: Создаем Супер-Мега-Искусственный Интеллект. Заставляем его написать роман. Ну нет, скажете вы, это же робот! Белая пластиковая фигня! Он только команды выполнять умеет, куда ему творить? Он не понимает ни человеческих эмоций, ни мотивов, о чем он вообще может написать? Об электроовцах? С таким мнением вы не останетесь в одиночестве. Также полагала одна очень умная женщина: дочь известного поэта, а заодно первый в истории программист — Ада Лавлейс. Она утверждала, что компьютер, даже очень мощный, не способен ни на творчество, ни даже на то, что мы называем «интеллектом». Ведь любая машина — всего лишь бездумный исполнитель алгоритмов, заложенных в нее человеком. То есть даже если компьютер когда-нибудь и напишет стихотворение или роман — все равно авторские права будут принадлежать кодеру, который его на это запрограммировал. Понять мы ее, конечно, можем. Перед глазами благородной дамы стоял пример батюшки, известного повесы и дебошира: только такой склад характера может быть у настоящего поэта. Если ее любимая, логическая и точная машина обречена стать чем-то подобным — лучше уничтожить ее прямо сейчас, чтобы потом не пришлось отправлять десант в прошлое. С другой стороны, ее ключевой аргумент о «выполнении программой только заранее заданных алгоритмов» в последнее время трещит по швам. Началось все с того, что над сложными программными комплексами начали работать сотни, а то и тысячи людей. И программистов, которые бы понимали, что делает абсолютно каждая функция, в крупных проектах обычно не водится. То есть за счет сотрудничества разных людей, программы уже стоят на уровень выше того, что способен создать один человек. Представьте себе роман с тысячей авторов. То-то же. Дальше — больше. Последние достижения машинного интеллекта базируются на алгоритмах, которые никто и никогда не писал. Программы создают их сами, основываясь на данных, которые им скармливают в невероятных количествах. Дошло уже до того, что на вопрос «как работает программа?» ее создатели лишь разводят руками. Назад в человеческие знания таинственные нолики и единицы просто так не конвертируются. Преемник Ады, Алан Тьюринг, был настроен немного оптимистичнее. Он даже посвятил несколько строк доказательству того, что бессмертная душа может жить не только в человеческом теле, но и в любом другом сосуде. Например, в боевом человекообразном роботе. За такие пассажи его не очень любили на родине, в консервативной Англии. Даже несмотря на то, что, взломав «Энигму», он серьезно повлиял на результаты Второй Мировой, гендерные предпочтения оказались для публики важнее научных достижений, и в конце концов «народ-победитель» довел вчерашнего героя до самоубийства. Программистам Тьюринг известен прежде всего своей «машиной», на основе которой возникли все существующие языки программирования, но популярной культуре более знакомо другое его творение — знаменитый Тест Тьюринга. Мнение о том, что оценить «интеллект» робота можно просто с ним побеседовав, захватила умы ведущих фантастов середины прошлого века и настолько успела всем надоесть, что когда несколько лет назад тест таки был пройден, никто, в конце концов, ничего и не заметил. Дело в том, что специально наученные лексические генераторы кроме как «выдать себя за человека»  собственно ничего и не умеют. А для повседневных проблем гораздо полезнее оказались системы, которые не пытались скопировать человеческое поведение, а просто решали поставленные перед ними задачи. Кстати, похожая революция произошла на столетие раньше в автомобиле- и самолетостроении. Как только конструкторы перестали пытаться создать «металлического коня» или «деревянную птицу», сразу же удалось построить механизмы, оказавшиеся гораздо полезнее произведений слепой эволюции. Человек тоже, если честно, — так себе вычислительный механизм. Будучи предназначенным для конкретной цели — выживания среди диких животных и себе подобных, человеческий организм в некоторых областях безнадежно отстает от примитивнейших механизмов. Какой смысл пытаться имитировать то, что можно сделать лучше? Именно поэтому после Тьюринга понятие «искусственный интеллект» поселилось разве что в фантастических романах, а программисты тем временем взялись за решение конкретных задач, стоявших перед экономикой. В космос летали ракеты, на биржах торговались акции. Компьютеры успешно рассчитывали оптимальные расписания движения поездов и самолетов. Пока романтики зачитывались правилами робототехники, простые работники бобины и перфокарты вводили в ламповые машины алгоритмы разной степени сложности. Логично было бы предположить, что проблему сочинения литературных произведений можно решить аналогичным способом. Мол, выпишем алгоритмы, согласно которым писатель или поэт составляет слова из букв, закинем их в жерло компьютера и вуаля — держите новую поэму. Или стихотворение. Или строчку текста хотя бы, нет? По привычке программисты обратились к экспертам, ожидая получить четкий набора правил бизнес-логики. Когда-то они так же ходили к физикам, железнодорожникам и даже биологам. Последние с математикой дружили так себе, но среди них оказалось много физиков, говорить с которыми программистам было проще. Но с филологами вышел полный швах. Формальные признаки текста, сказали они, конечно же существуют. Стихотворный размер, жанры и поджанры, стили речи и письма, в конце концов. Но если расставить ударения машину научить еще можно (после долгого прописывания примеров вручную), то сформулировать в математических терминах особенности стиля Бодлера филологи отказывались наглухо. Мол, ни в какие цифры настоящее искусство загнать нельзя. И не надо! Ибо кто тогда нам, филологам, зарплату платить будет? Идите словом, дорогие программисты, рассчитывать траектории ядерных ракет. Это нам в ближайшее время понадобится гораздо больше. Вот они и пошли. Единственной составляющей художественного текста, хоть как-то подходящей для алгоритмизации, долгое время был сюжет. Ведь в большинстве случаев в произведении были главные герои, которые совершали определенные действия. Например, двигались из точки А в точку Б. Прямо как ракеты, подумали программисты и взялись за различные генераторы сюжетов. Некоторые, честно говоря, даже давали неплохие результаты. Кто-то умудрялся оформить их в целые книги. Только вот Ада Лавлейс, критически прищурив глаз, заметила бы, что такие программы — не более, чем вариации «шляпы сюжетов», когда автор пишет на клочках бумаги сначала имена персонажей, затем события, а потом по очереди вытягивает бумажки, описывая в литературной форме, что произошло и с кем. И если и есть какое-то творчество в этом процессе, то исключительно со стороны автора, который на этих листочках пишет. А со стороны машины здесь один лишь генератор случайных чисел. Доливали масла в огонь мистики: творчество, мол, — это медитативный процесс, своеобразное общение писателя с духами. Если существуют какие-то правила, то только в их эфирном мире. Пока мы не научим наши компьютеры подключаться туда по HDMI, никакого творчества от компьютеров ждать не стоит. Эту позицию разделяют и авторы учебников по \"creative writing\". Большинство советов там о том, как заставить себя сесть за стол, расслабиться и начать «слушать свою брокколи». После того как она надиктует вам текст, стоит только исправить ошибки, выбросить все наречия и можно отправлять рукопись в издательство. Что ж, метод может и действенный, но для написания программы непригоден абсолютно. Ведь, чтобы запрограммировать что-то, нужно знать, как это что-то работает. Не так ли? На самом деле… не совсем. Но об этом немного позже. Мир как правила и представление Если копнуть глубже, проблема формализации возникает не только в литературе. Философы Древней Греции тоже очень любили давать определение всему, с чем встречались на своем пути. Сначала получалось неплохо. Только вот оказалось, что наибольшие проблемы возникают с простыми понятиями, понятными даже ребенку. Добро, зло, красота, уродство, добродетель, порок, Бог. После нескольких анекдотичных случаев вроде «человек — это двуногое без перьев», Платон устал работать над определениями и постановил, что настоящие правила этого мира, так называемые Идеи, существуют где-то в параллельном мире, а к нам долетают только их тени. Человеческая душа, имея доступ к этому «Идеальному миру», может без труда их различать, а наше мышление, ограниченное миром материальным, понять этого величия не может. Поговорить об Идеях под хорошее вино, впрочем, никто не запрещает. Хотя нет, все-таки запрещает. Витгенштейн, один из ведущих философов двадцатого века, в свое время безапелляционно заявил: «О чем невозможно говорить — о том лучше молчать». Нет, он не был врагом свободы слова. Просто речь, отметил он, в отношении нашего мышления, — откровенно вторична. Она может помочь в базовом общении и даже передаче какой-то части знаний в печатном виде, но то главное, что происходит с нами, — в слова конвертируется слабо. Максимум, что мы можем, — это давать названия неким фактам, элементарной единице этого мира. В отличие от «понятий», «факты» описывают то, что мы видим, без лишних обобщений. Конкретный стол стоит в конкретной комнате при конкретной температуре, атмосферном давлении и уровне освещения. Никакой Идеи Стола в комнате нет, а значит и говорить о ней нет смысла. Весь мир вокруг — просто конфигурация атомов и волн, в которую болезненное человеческое воображение пытается впихнуть термины, которые само не понимает. Ребенок приходит к отцу и говорит: «Папа, я придумал слово карчапан. Что оно значит?». Такими Витгенштейн видел всех философов прошлого. Зато «факты» такими проблемами не страдают. Мы уже знаем, что мы видим. Остается понять — почему? На язык описания «фактов» серьезно претендовала математика. Стройными рядами формул пытались объяснить все: от взаимодействия атомов до несчастных браков. Только вот когда модель становилась хоть немного сложнее, формулы сразу теряли свою элегантность и превращались в десятиэтажные нотации тензоров и «странных аттракторов» (единственный термин, который я до сих пор помню из курса функционального анализа). Таким образом, перевести авангард современной математики в булеву логику оказалось еще сложнее, чем художественные тексты. К тому же и сама математика, как показал Гедель, оказалась не такой уж и всесильной. Словом, полное фиаско. Настолько полное, что часть философов окончательно пустилась во все тяжкие и создала странное учение под названием «постструктурализм». Проблему описания окружающего мира в текстовом формате они решили очень просто. Текст, мол, не описывает, а порождает этот мир. Своеобразное учение Витгенштейна наоборот: если об этом никто не написал — этого на самом деле не существует. Эта точка зрения могла бы очень понравиться программистам, ведь ввести в компьютер текст гораздо проще, чем заставить собирать информацию о мире миллионами разнообразных датчиков. Только вот сами постулаты теории оказались настолько близки к полной чуши, что, кроме самих создателей, их, кажется, так никто и не понял. Поэтому после смерти Фуко и Делеза течение медленно, но верно растворилась в ризоме. Окончательно разочаровавшись в философах, программисты обратились к представителям более приземленных специальностей. Например, к уже знакомым биологам. Уподобиться мозгу Двадцатый век, кроме прорывов в кибернетике и способах убивать друг друга, принес еще и неплохие результаты в понимании того, как устроены люди. Все больше и больше процессов, которые ранее могли объяснить только общением с духами, теперь приписывали мозгу. До полного понимания того, как работает наше сознание, еще, конечно, очень и очень далеко, но в базовых вещах, вроде строения нейронов, наука уже ориентируется достаточно неплохо. Так, судя по всему оказалось, что знаменитая Пещера идей — это, на самом деле, просто область внутри нашего черепа. Мозг, получая из органов чувств информацию об окружающем мире, записывает ее на «жесткий диск»в только ему понятной форме из нейронов и связей между ними. Про «чтение» такой записи речь пока не идет, но мы знаем, что ключевыми характеристиками нашей «пещеры» являются: а) абстракция, то есть объединение черт различных предметов и явлений под одним «ключом» и б) постоянное обновление данных на основе входящей информации. Система хранения данных типа «мозг» — на несколько порядков лучше всего, пока что созданного человеком. И единственным, хотя и очень неприятным недостатком является то, что как в язык, так и в математику все наши знания и весь наш жизненный опыт конвертируются очень и очень плохо. Возникает даже забавная ситуация, когда специалист экстра-класса, способный выполнять сложнейшие задачи, не может объяснить ученикам, как это у него получается. Бедолагам остается только наблюдать за Мастером и пытаться воспроизвести каждое его движение. Как перевести это все на машинный язык, вопрос пока даже не стоит. Казалось бы, очередной тупик. Но ученые, почесав затылок, задумались. А почему бы нам не убрать посредников, неспособных даже между собой договориться, и не выпустить наши программы напрямую во внешний мир? Пусть они сами берут — и учатся! Соблазнительно, не так ли? Первой попыткой было сотрудничество Уоррена Маккалока и Уолтера Питтса в 1943 году. Стоит отметить, что ни один из них не был программистом. Маккалок — нейропсихолог, нейрофизиолог, философ, поэт, имел с математикой достаточно сложные отношения. Но это компенсировал Питтс — вундеркинд-математик, от которого в восторге был сам Рассел. Питтс сбежал из дома в 15 лет, чтобы ошиваться вокруг MIT, куда его упорно не принимали из-за возраста и отсутствия формального образования. Маккалок всю жизнь мечтал построить модель человеческого мозга. Питтс помог ему в этом, упростив нейрон до булевого оператора, получающего на вход двоичные числа и на выходе либо активирующийся (результат «да») или остающийся спокойным (результат «нет»). Такая модель мозга была слишком упрощенной, но вместе с тем ее значимость трудно переоценить. На тот момент самым популярным психологическим течением был психоанализ — полумистическое учение об Иде, Эго и Оргоне, понятиях несовместимых с научным взглядом на мир. Попытка формализовать самый таинственный процесс во Вселенной, человеческое мышление, не могла не выстрелить. После публикации первой статьи «A logical calculus of the ideas immanent in nervous activity» работой пары заинтересовался Норберт Винер, впоследствии сделавший Питтса своей правой рукой. Позже к ним присоединился и сам фон Нейман. Без преувеличения можно сказать, что модель Питтса позволила построить компьютер таким, как мы знаем его теперь. Начиная с определенного этапа, нейронные сети отделились от кибернетики. Моделирование человеческого мозга перестало быть самоцелью, к тому же, новые результаты ученых-биологов показали, что мозг работает несколько сложнее, чем модификация линейной регрессии. Тем не менее, определенные задачи даже таким приемам удавались на ура: базовое распознавание изображений, математическое прогнозирование. Даже медицина получила свою порцию инноваций: нейросети заставляли анализировать кардиограммы. После стремительного взлета часто бывает очень болезненное падение. Так случилось и в этот раз. Известный и очень авторитетный в своих кругах математик и философ Марвин Мински (сосед Питтса по комнате) в 1969 году издал книгу «Перцептроны». В отличие от широкой публики, он понимал, чем именно занимаются программисты, и спокойно, без эмоций, доказал, что в данной интерпретации так называемые «нейронные» сети имеют очень четкие ограничения. Даже оператор «XOR», базовое понятие программирования, однослойными моделями, которые тогда использовались, описать было невозможно. Наложившись на первые неудачи в промышленном использовании и отсутствие новых прорывов в лабораториях, эта книга стала могильщиком нейросетей: отсутствие грантов и, как следствие, отток специалистов отправили отрасль в длительный криогенный сон, из которого она стремительно вышла только полстолетия спустя. Второе пришествие, как водится, стало еще ослепительнее предыдущего. За последние годы нейросети ворвались практически во все области компьютерных наук, и ситуация везде выглядела одинаково: десятилетия исследований, тысячи экспертов, миллионы страниц доводили эффективность алгоритма до фантастической цифры в 80%. А потом приходила нейросеть и без специальных знаний, простым перебором данных показывала результат 90-95%. Так произошло с распознаванием изображений, машинным переводом, генерацией и пониманием человеческой речи, прогнозированием и профессиональной аналитикой. Да что там, сейчас вообще трудно назвать отрасль, куда еще не пролезло вездесущее «машинное обучение». Нейросети играючи раскусили ранее нерешаемую задачу распознавания изображений: научились отличать кота от собаки — им просто показали миллион картинок и сказали, где что изображено. Все характеристики «кошачести», которые не смогли сформулировать сотни ученых за десятилетия исследований, сеть определила для себя сама. Удивив весь мир, несколько лет назад команда AlphaGo преодолела одного из сильнейших игроков в Го. Эта игра, в отличие от шахмат, математическому решению не подлежит — слишком велико число вариантов развития событий. Программе математика и не понадобилась. Вместо этого, она просто «прочитала» все матчи между людьми, доступные в записи, а затем еще несколько миллионов раз сыграла сама с собой. Самым оскорбительным для свергнутого чемпиона стало то, что в обучении программы даже не участвовали известные гроссмейстеры — ей объяснили только базовые правила и критерий победы. Все остальное — набор проб и ошибок. Подход оказался достаточно общим. Совсем недавно таким же образом программа обыграла игроков людей в покер. Что же поменялось, спросите вы, по сравнению с серединой двадцатого века? И почему методы, признанные ранее полностью провальными, теперь вдруг заработали так эффективно? Во-первых, улучшились сами методы. Даже несмотря на отсутствие финансирования, фанатики в пыльных лабораториях одна за другой выписывали сложные формулы, оптимизируя быстродействие и разрабатывая методы избавления от шума. Во-вторых, конечно, железо. Всем известен пример айфона, вычислительная мощность которого превышает суммарные ресурсы машин, отправивших человека на Луну. Как всегда, двигателем прогресса стала индустрия развлечений: современные системы машинного обучения работают в основном на видеокартах, производители которых сходили с ума, пытаясь выдать геймерам картинку пореалистичнее. Третьим, и самым главным фактором, стали данные. Ребенку, чтобы он понял, что такое «стол», достаточно показать один-два примера. Нейросети хорошо учатся на выборках, начинающихся от десятков тысяч. Можно, конечно, попенять на несовершенство современных методов, но в защиту бедного алгоритма следует отметить, что ребенок на самом деле получает гораздо больше информации о предмете. Он видит его с разных сторон, может понюхать, потрогать и даже порой отгрызть кусочек лака, чтобы получить как можно больше разных характеристик. Сеть мы учим на фотографиях — количество каналов уменьшено до одного, еще и без пространственной ориентации. Вот и приходится компенсировать качество количеством. Сейчас человечество выбрасывает в интернет больше данных, чем было создано за всю историю. Развитие цифровой фотографии привело к тому, что, наверное, ни один уголок планеты не остался незадокументированным. С другой стороны, радикально выросло и число текстов, написанных человеком. Теперь значительная часть общения между людьми происходит в сети, а художественных текстов в последнее время стало на порядки больше. Казалось бы, ничто не мешает обучить нейросеть на них и вперед — к новым литературных вершинам. На самом деле, не все здесь так просто. Давайте разберемся почему. В поисках нейропегаса Несмотря на всю свою мощность, сети не могут пока и близко сравниться с человеком в некоторых сферах. Создание осмысленных текстов — в их числе. Причин этому много, но самые важные — две. Первая — те же пресловутые правила. Люди не учатся исключительно на примерах. Им объясняют учителя, дают информацию книги, у людей есть физические и химические формулы, синтаксические законы, словари и энциклопедии. А вот нейронную сеть, обученную на живых примерах, подкорректировать вручную уже невозможно — разве что сгенерировать на основе «правил» дополнительные примеры, но даже в этом случае есть большая вероятность переобучения и, как следствие, — полной непригодности системы для реальной жизни. (Также, кстати, нет никакой возможности перевести правила, обнаруженные машиной, назад в человеческую речь. Поэтому победа над Ли Седолем вряд ли научит чему-то новому игроков-людей: почему сеть сделала тот или иной ход навсегда останется для нас тайной). Вторая и главная разница между нашим мозгом и битами в памяти компьютера — исходные данные. Человек не получает абсолютно все свои знания из окружающего мира. Большинство информации идет в виде «заводской прошивки» — инстинктов, эмоциональных реакций, базовых правил поведения. Этими данными человек обязан миллионам поколений предков, вплоть до бактерий, которые своими постоянными попытками, победами и неудачами формировали генетический код — наверное, самый плотный носитель информации из всех нам известных. Мы не то, что не можем внедрить эти данные в нейросеть, мы и осознать их не способны. Конечно же, обе эти проблемы решаются любимым методом нейронных сетей: «Нам надо больше, больше, БОЛЬШЕ данных», но оказывается, что для решения в лоб задачи такой сложности, как генерация прозы, даже всех букв, когда-либо написанных человечеством, пока недостаточно. Дело в том, что текст — не герметичный набор данных, а скорее отражение внешнего мира, и, чтобы постичь «связи» в тексте, нужно понять «связи» самого мира, или, по сути, построить Универсальную Модель, которая будет знать о мире практически все, что знает среднестатистический человек. То есть опять же построить Сильный Искусственный Интеллект, до чего сейчас еще очень и очень далеко. (На самом деле задачи на понимание мира могут быть вполне тривиальными. Классический пример — предложение “Игрушка не влезла в коробку, потому что она слишком большая”. Что здесь — “она”? Ребенок ответит сразу: он понимает значение слова “большая” и умеет соединить его со значением “не влезла”. Для компютера же оба смысла равны, на уровне самого текста ответа на эту задачу не существует.) Но с другой стороны, что мешает программам просто сгенерировать набор символов, не заморачиваясь их смыслом? Ничего, этим они в последнее время и занимаются. Давайте разберемся как. У продвинутого читателя уже давно должен был возникнуть один вопрос. Нейросети, мол, в большинстве случаев строятся для классификации и прогнозирования. То есть, мы имеем много данных на входе и ограниченный (часто скалярный) результат на выходе. Мы скармливаем сети сотню тысяч квартир с набором параметров и цену каждой из них. Затем натравливаем на еще одну позицию, цены которой мы не знаем. Результат — одно число, прогнозируемая стоимость квартиры. Мы показываем алгоритму миллион картинок кошек и миллион картинок собак, указывая, кто есть кто. Затем показываем новую картинку, и алгоритм должен угадать, кот на ней изображен, или собака. Как такой механизм в принципе можно использовать для генерации хоть чего-то? Первый метод — обратные вычисления. Нейросеть по своей сути — система линейных уравнений, а, следовательно, ее можно спокойно вывернуть в противоположную сторону. Мы не ставим задачу «что на этой картинке?». Вместо этого мы указываем готовый результат (например, «кот») и спрашиваем: «Как по-твоему он должен выглядеть?». Именно так поступили разработчики Гугла в 2015 году. Их результаты, напоминающие скорее произведения сюрреалистов, хоть и продемонстрировали наглядно работу нейросети, на какую-то эстетическую ценность претендуют со скрипом. К тому же, один раз обученная нейросеть с таким подходом может дать только один, детерминированный результат для каждого набора входных данных — ограничение не из приятных. Подходит такой способ для генерации текста? Честно говоря, вряд ли. Вообще говоря, обработка текста по сравнению с обработкой изображений имеет несколько достаточно неприятных аспектов. Во-первых, картинки можно легко сжимать. Вследствие особенностей человеческого зрения, предназначенных для рассматривания далеких предметов, мы даже на иконках 16х16 способны разглядеть какие-то детали. Для текста сжатия без потерь — абсолютно нетривиальная задача. Нельзя просто взять и объединить соседние символы, как мы поступаем с пикселями. Вместо этого нужно искать обходные пути — анализировать части речи, чтобы выбросить эпитеты, строить модель событий, чтобы объединить несколько предложений в одно. Словом, вся «слепота» нейросетей к бизнес-логике сразу куда-то исчезает. Во-вторых, текст гораздо чувствительней к шуму. Если для картинки несколько не слишком ярких точек не на своих местах погоды не сделают, то слу»ча%йны*е си$мво^лы, разбросанные по тексту, абсолютно неприемлемы — читать такие опусы чрезвычайно трудно, а порой от замены всего нескольких брюкв вообще меняется весь смысл. Второй способ генерирования чего-то нового построен на умении нейронной сети прогнозировать. Пусть текст, предполагают адепты этого метода, является определенным временным рядом. И каждая следующая буква каким-то образом зависит от предыдущих. Теперь остается только научить систему на заданных «взаимозависимостях» и заставить ее прогнозировать, какие символы должны быть в тексте следующими. Несмотря на тривиальность этого подхода, результаты, полученные с его помощью, впечатляют. Первая программа-генератор, описанная Андреем Карпати, умышленно оперирует исключительно символами в отличие от аналогов, пытавшихся искать взаимные связи между словами. И даже в такой постановке удалось получить хоть и бессмысленный, но читабельный и даже стилистически близкий к оригиналу кусок текста. Генерации подверглись Шекспир, речи Обамы, рекламные заголовки и даже тексты выступлений на конференции TED. Интересные выводы можно сделать из генерации карт для игры Хартстоун. На первых порах система выдает полную ерунду, но уже после нескольких итераций некоторые карты начинают выглядеть осмысленно — их уже спокойно можно включать в игру. Мощности современных алгоритмов хватает даже для поэзии (в том числе, китайской и хокку ) или текстов песен, но успех там, честно говоря, достигается в основном за счет ручного выбора исследователем приличных вариантов. Хорошую прозу так пока сгенерировать не получится. Во-первых, проблема в объеме. Как говорилось ранее, сжимать текст, особенно художественный, — достаточно трудно. А количество взаимосвязей длинного прозаического текста пока превышает вычислительные возможности даже самых мощных кластеров. Вторая проблема — обучение с учителем. Большинство данных, которые сейчас используются для машинного обучения, были когда-то размечены людьми вручную. Существует даже специальный ресурс «Механический турок», где люди за небольшую плату помогают ученым со всего мира обучать свои алгоритмы. Для размещения на этом сайте задачи должны занимать максимум несколько секунд эффективного времени обычного человека: «Есть ли на картинке лицо?», «В какую сторону поворачивает дорога?», «Какое слово говорит голос на записи?». За счет простоты, можно получить очень большую базу данных за относительно короткое время. Заставить пользователей читать рассказы, сгенерированные работами, трудно. Во-первых, текст, составленный наполовину из тарабарщины, а наполовину из гениальных предложений, все равно остается бредом. Во-вторых, даже если роботы когда-нибудь научатся писать что-то осмысленное, второй вариант того же текста не будет иметь того же эффекта, что первый, — читать одно и то же скучно. Значит, придется наполнять базу произведениями людей. Но и здесь возникает вопрос: какими должны быть критерии оценки? Как мы уже упоминали ранее, машинное обучение конвертирует большие объемы данных в небольшой результат. Как выглядит результат для прозы? Первое и наиболее логичное предположение — написанное должно иметь хоть какой-то смысл. То есть текст, пусть даже сгенерированный с помощью машинного обучения, нужно сначала преобразовать в последовательность событий и определить, не противоречат ли они друг другу. Понимание текста, в отличие от генерации, продвинулось гораздо дальше. IBM Watson недавно победил в викторине, Google построил базу данных смыслов слов разных языков. На рынок постепенно выходят анализаторы текста для воспроизведения событий на основе тысяч статей в интернет-изданиях. Чрезвычайно важной является задача понимания человеческих команд — интернет вещей постепенно перекочевывает из фантастических сериалов в реальный мир, и ваше общение с микроволновкой через какие-то год-два превратится из первых признаков помешательства в практическую необходимость. Людей трудно переучивать на язык инструкций — роботы должны просто понимать, что от них хочет хозяин. Последствия этой революции могут сыграть значительную роль в генерации художественных текстов — программа сможет прогонять свои «творения» через базовые фильтры и абсолютного бреда на выходе уже не будет. Но этого, безусловно, мало. Ведь художественная литература — это много больше, чем просто описание событий. Поэзия — вообще в большинстве своем несюжетная. Попытки разобраться, что на самом деле означает та или иная метафора — работа не из легких даже для исследователей из плоти и крови. Однако, одно правило остается практически неизменным: любое искусство существует для того, чтобы вызвать у нас эмоции. Печаль, радость, злость, страх — читатель должен сопереживать героям. Соответственно, значения слов можно и не знать, достаточно просто провести параллели между предложениями и реакцией на них. Осталось только посадить миллион человек за машины и заставить их читать тексты, описывая после этого свои эмоции. Хотя нет, учитывая масштабность задачи, миллиона будет мало — надо как минимум миллиард. Вряд ли «Механический турок» потянет такие объемы — понадобится новая система, полностью заточенная под эту цель. Еще одна проблема — сами тексты. Можно начинать с каких-то базовых кусочков — старых анекдотов или газетных вырезок, но нашему миллиарда «подопытных» такое быстро надоест. Желательно, чтобы тексты они генерировали сами, хотя бы по несколько десятков каждый. Объем не так важен, главное — количество. Чем больше, тем лучше. Теперь главное — как заплатить миллиарду людей? Где взять деньги на такое масштабное исследование? Может попросить их работать бесплатно? Ради блага науки, например. Или для собственного развлечения. И чтобы весь эксперимент выглядел веселее, можно вместо обычного интерфейса добавить иконки. Например, такие: Итак, основной проблемой сегодняшнего дня является не генерация, а именно анализ текстов. Как только машина научится понимать и даже в каком-то смысле «сопереживать» творением людей, выделяя гениальные произведения среди гигабайт чуши и графомании, проблемы генерации уже не будет. Да что там, издатели и сейчас готовы заплатить миллионы за программу, которая будет способна выбрать шедевры из тысяч опусов, ежедневно появляющихся на самиздате Амазона, причем желательно, чтобы она делала это быстрее и эффективнее, чем бедные студенты-филологи, которые занимаются этим сейчас. Развитие индустрии, таким образом, будет двигаться с двух сторон: нейросети генерации будут совершенствоваться, чтобы извлекать больше информации из данных (RNN, LSTM), а нейросети распознавания будут приближаться к истинному пониманию текста, и, как следствие — настоящему пониманию окружающего мира. На уровне человека, а скорее сильно превосходя нас самих. *** В конце хочу отметить одну очень важную вещь. Большинство результатов, описанных в этой статье — очень новые. Речь идет даже не о десятилетиях: вышеупомянутая статья Андрея Карпати вышла в мае 2015 года! Прямо сейчас мы находимся на очередном витке взрывного роста: растет количество ученых, занятых в индустрии, компании тратят миллиарды долларов, чтобы не отставать в гонке Искусственного Интеллекта. Технологии тоже не стоят на месте: хардварные компании одна за другой выпускают железо, предназначенное в первую очередь для машинного обучения. И поэтому, хоть на данный момент и не существует ни одной системы, способной на создание хоть немного осмысленной прозы, шансы на то, что подобная появится в ближайшее время —очень и очень высоки. (Программистов угроза, кстати, тоже не миновала). 11 сентября 1933 на ежегодном съезде Британской Академии Наук лучший физик-ядерщик того времени Эрнест Резерфорд безапелляционно заявил, что все, кто занимается добычей энергии из трансформации атомов, — шарлатаны. И все их исследования — не более, чем очередной виток лженауки. Уже на следующее утро молодой исследователь Лео Силард описал принципы цепной реакции, благодаря которым через несколько лет появится первый ядерный реактор."], "hab": ["Машинное обучение"]}{"url": "https://habrahabr.ru/post/280105/", "title": ["NetApp FlexClone: Как технология ускоряющая разработку"], "text": ["Здесь хочу я коротко описать традиционную архитектуру ИТ-инфраструктуры для обеспечения отказоустойчивости БД и схему разработки, использующую эту архитектуру. И более подробно остановиться на том, как можно упростить жизнь тестировщикам и разработчикам при помощи технологии FlexClone, которые используют БД или любой другой большой набор данных необходимый для тестирования и разработки. Традиционный подход заключается в том, что создаётся кластер БД: основная и запасная. Далее, время от времени эта БД (как правило, на Stand-By стороне) копируется для разработчиков. И чем больше такая БД, тем реже она копируется и удаляется и тем сильнее нагружается СХД от таких операций. С другой стороны, разработчики и тестировщики получают устаревшую БД, она как правило месячной или даже полугодичной давности. Когда же приходит время внедрять отлаженный код в продуктив, оказывается, что БД успела сильно измениться: появились новые дополнительные данные, таблицы, поля и возможно удалены старые. Это приводит к тому что отлаженный код для старой БД не работает на новой, что сильно усложняет разработку, тестирование, внедрение и эксплуатацию нового кода. FlexClone более эффективный способ ведения Test/Dev с большими базами данных Технология FlexClone позволяет создавать тонкие клоны, на основе снепшотов NetApp, которые не влияют на производительность системы. Клонов можно снимать много и очень часто, прямо посреди рабочего дня. Благодаря этому простому явлению жизнь тестировщиков и разработчиков существенно упрощается, а качество кода увеличивается. Это вписывается в подход, набирающий популярности под названием Copy Data Management (CDM), который призван использовать один набор данных сразу для нескольких задач. Подход удобен благодаря тому, что: На тонкий клон можно зарезать QoS (это не касается Space Reclamation при использовании UNMAP) Он ничего не занимает, со временем он станет занимать только изменения, а так как он больше читается, то изменений будет не много Процесс копирования FullBackup не будет нагружать дисковую подсистему. Моментально снимается, а не занимает долгие часы Тестировщики получают наиболее свежие данные намного чаще и лучше могут отлаживать свой код на актуальных (а не сильно устаревших данных). Что очень важно для ускорения и улучшения качества отладки кода. Это не сложно автоматизируется при помощи ПО для снятия консистентных снепшотов Что для этого нужно? — Лицензия FlexClone; — Расположить Prod или Stend-By на СХД с прошивкой ONTAP (ONTAP Select/ONTAP Cloud/ на NetApp FAS платформе), чтобы при помощи технологии FlexClone выполнить тонкое клонирование БД; — Продукт, позволяющий интегрировать БД c Hardware-Assistant снепшотами NetApp, для достижения консистентности, с которых потом будут создаваться клоны: ПО для автоматизации и консистентности Нижеописанные продукты выполняют резервное копирование и каждый из них может создавать консистентные с точки зрения ПО снепшоты. SnapCreator бесплатный, но его нужно будет доработать при помощи скрипта, чтобы мочь замораживать БД и снимать снепшот NetApp SnapCenter платный продукт который полностью автоматизирует процесс клонирования, предоставляет удобный GUI Cammvault Simpana — платный продукт резервного копирования. Всё тоже самое что и SnapCenter, только с наворотами типа каталогизации и архивации на ленту. Другие сторонние продукты (Symantec, Veeam, Syncsort, EMC Networker, HP DataProtector, IBM Tivoli, Acronis и др.), которые умеют интегрироваться с БД и NetApp ONTAP для снятия Hardware-Assistant Snapshot. Если снимать клон со Stand-By, то можно ненадолго останавливать БД (потом снимать снепшот NetApp чтобы добиться консистентности) и снова её запускать, тогда не понадобится ПО для снятия консистентных снепшотов, без написания скрипта для взаимодействия с БД Операции по снятию консистентных снепшотов (смотри список выше), клонированию и подключению к БД легко можно автоматизировать, предоставляя их как сервис для отдела Test/Dev при помощи web-интерфейса бесплатного Workflow Automation. CodeEasy FlexClone Toolkit CodeEasy это набор скриптов и методология для DevOps, которая использует технологии NetApp такие как Snapshot и FlexClone, для сокращения время тестирования и разработки. Скрипты CodeEasy Toolkit используют NetApp Manageabilitys SDK для автоматизации шагов подготовки рабочего пространства для разработчиков и тестировщиков, который позволяет вообще без внесения изменений или с минимальными настройками их использовать. Системные требования: ONTAP (cDOT) 8.2 or later NetApp Manageability Software Development Kit (NMSDK) 5.3.x or later Скачать CodeEasy FlexClone Toolkit. NetApp-PowerShell Commandlets Для Windows машин доступен NetApp PowerShell Toolkit, который позволит создавать скрипты управления NetApp. NetApp Docker интеграция NetApp Docker Volume Plugin (nDVP) предоставляет связь между Docker и системами хранения NetApp (ONTAP, SolidFire и E-Series) для создания и подключения постоянного хранилища внутрь контейнеров. Эта тема заслуживает отдельного внимания и будет описана в одной из следующих статей. Резервное копирование FlexClone также очень часто используется в средах резервного копирования для тестирования работоспособности резервных копий. Так множество софта резервного копирования позволяют автоматизировать этот процесс. К примеру, Veeam On-Demand Sandbox интегрируется с ONTAP и благодаря использованию FlexClone, позволяет не затрагивая резервную копию моментально создать клон с виртуальной машины любого размера, запустить клон и выполнить тестирование его работоспособности и восстанавливаемости. Workflow Automation (WFA) Workflow Automation — это бесплатная графическая утилита доступная для Windows и Linux, позволяющая создавать наборы или связки задач, для автоматизации процессов управления ONTAP. К примеру, через неё можно настроить клонирование вольюмов или лунов, передачи их в тестовый SVM, создание новых файловых шар или iGroup, добавить в неё клонированные вольюмы и новые хосту-инициаторы с тестовой площадки, поднять новые LIF интерфейсы. А после временного разрыва отношений репликации, для клонирования данных, восстановить их обратно — и всё это практически по одному клику мыши. WFA также предоставляет возможность управления через PowerShell и Perl скрипты, а также через REST API. Производительность Важно отметить, что технология FlexClone не только существенно ускоряет разворачивание больших БД или большого количества любых других данных, кардинально уменьшая потребление дискового пространства, но и косвенно ускоряет их работу. Клонирование по сути выполняет функцию подобную дедупликации, т.е. уменьшает объем занимаемого пространства. Дело в том, что СХД NetApp FAS всегда помещает данные в системный кэш, а системный и SSD кэши в свою очередь является Dedup-Aware, т.е. они не затягивают дубликаты блоков, которые уже там есть, таким образом оба кэша логически вмещают намного больше данных, нежели физически они могут. Это сильно улучшает производительность во время эксплуатации СХД и особенно в моменты Boot-Storm благодаря увеличению попадания/чтения данных в/из кэш(а). Дополнительные сценарии использования Кроме тестирования/разработки тонкое клонирование может находить другие схемы применения. К примеру, иногда есть смысл разделить доступ к данным на задачи с разными приоритетами. Технология клонирования отлично дополняет функционал репликации SnapMirror, когда DR сайт используется не только как резерв на случай аварии, но и как среду разработки (клонировать Data Protection вольюм). Транзакции VS Отчёты Примером может быть БД, которая используется как транзакционная, а помесячно или поквартально она же используется ещё и для построения отчетов. Транзакции генерируют как правило мелкие, случайные операции чтения/записи, такие операции весьма успешно могут кэшироваться и должны выполняться с низкой скоростью отклика (обычно не выше 10мс). С другой стороны, есть отчёты, генерирующие большие последовательные операции чтения, а при сохранении результатов — записи (их обычно намного меньше, к примеру 10%). Отчёты строятся раз в месяц, квартал или год, они могут собираться в течении целых суток и нет принципиальной разницы если бы они собирались не за сутки, а, например, за трое суток. Так вот если к транзакциям добавить ещё и отчёты, то транзакции могут сильно пострадать при том всём что отчёты если опоздают на час или сутки не важно. Получается весьма несправедливое распределение ресурсов. Вот здесь могут прийти на помощь FlexClone, QoS и политики кэширования: Можно создать тонкий клон, на него зарезать MB/s, к примеру, в 3 раза меньше по сравнению с тем, как если бы отчёты запускались без ограничений, изменить политики кэширования и получить в три раза меньшее влияние на дисковую подсистему, и в 3 раза большее время построения отчёта (что как правило весьма приемлемо). И все эти процессы на СХД можно автоматизировать при помощи бесплатного ПО NetApp Workflow Automation. Всё тоже самое может быть применимо в схеме, когда клон снимается с Stand-By в кластере БД: хоть она и не нагружена продуктивными транзакциями напрямую, но всё равно должна поспевать за внесением изменения от основной БД. И чтобы её не тормозить, удобно опять-таки использовать FlexClone и QoS. Эти задачи достаточно легко автоматизируются при помощи платного или бесплатного ПО. Выводы Технология FlexClone позволяет не только более оптимально использовать ресурсы СХД (как в плане производительности, так и в плане пространства), но и существенно ускорить бизнес-процессы компаний, ускоряя и упрощая процесс разработки и отладки нового кода. Для скачивания ПО от компании NetApp может понадобиться логин и пароль. Сообщения по ошибкам в тексте прошу направлять в ЛС. Замечания, дополнения и вопросы по статье напротив, прошу в комментарии."], "hab": ["Тестирование IT-систем", "Разработка веб-сайтов"]}{"url": "https://habrahabr.ru/post/321652/", "title": ["UNIX-подобные системы содержат кучу костылей. Крах «философии UNIX»"], "text": ["В первой части статьи перечислю кучу костылей UNIX, и вообще разных недостатков. Во второй — про «философию UNIX». Статья написана наскоро, «полировать» дальше не хочу, скажите спасибо, что написал. Поэтому многие факты привожу без ссылок. Костыли в UNIX начали возникать ещё с момента появления UNIX, а это было ещё раньше появления не только Windows, но даже вроде бы Microsoft DOS (вроде бы, мне лень проверять, проверяйте сами). Если лень читать, хотя бы просмотрите все пункты, что-нибудь интересное найдёте. Это далеко не полный список, это просто те косяки, который я захотел упомянуть. В самом начале make был программой, которую один человек написал для себя и нескольких своих знакомых. Тогда он, недолго думая, сделал так, что командами воспринимаются строки, которые начинаются с Tab. Т. е. Tab воспринимался отлично от пробела, что крайне некрасиво и нетипично ни для UNIX, ни за его пределами. Он так сделал, потому что не думал, что make будет ещё кто-то использовать кроме этой небольшой группы. Потом появилась мысль, что make — хорошая вещь и неплохо бы включить его в стандартный комплект UNIX. И тогда чтобы не сломать уже написанные мейкфайлы, т. е. написанные вот этими вот десятью людьми, он не стал ничего менять. Ну вот так и живём… Из-за тех десятерых страдаем мы все. Почти в самом начале в UNIX не было папки /usr. Все бинарники размещались в /bin и /sbin. Но потом вся инфа перестала помещаться на тот диск, который был в распоряжении авторов UNIX (Томпсон, Ритчи). Поэтому они достали ещё один диск, создали папку /usr, а в ней — ещё один bin и ещё один sbin. И смонтировали новый диск в /usr. Оттуда и пошло. Так появилась «вторая иерархия» /usr, а потом в какой-то момент ещё и «третья иерархия» /usr/local, а потом ещё и /opt. Как пишет рассказчик этой истории: «Не удивлюсь, если когда-нибудь ещё появится /opt/local». UPD от 2017-02-12: я нашёл ссылку, где я почерпнул эту историю. Читайте, там более точная версия произошедшего. sbin изначально означало «static bin», а не «superuser bin», как можно было бы подумать. И содержал sbin статические бинарники. Но потом sbin стал содержать динамические бинарники, его название потеряло смысл. Windows часто ругают за наличие реестра и сообщают при этом, что подход UNIX-подобных систем (куча конфигов) якобы лучше. А между прочим однажды в ext4 появилась особенность (является ли это багом, вопрос спорный), из-за которой при резком выключении компа Gnome потерял все свои конфиги в рабочей папке юзера. И разработчик этой ext4 сказал в обсуждении баг репорта, что Gnome'у надо было использовать что-то вроде реестра для хранения инфы. UPD от 2017-02-12: источники: раз и два. Имя отписавшегося maintainer'а ext4: Theodore Ts'o. Вот его слова: If you really care about making sure something is on disk, you have to use fsync or fdatasync. If you are about the performance overhead of fsync(), fdatasync() is much less heavyweight, if you can arrange to make sure that the size of the file doesn't change often. You can do that via a binary database, that is grown in chunks, and rarely truncated. I'll note that I use the GNOME desktop (which means the gnome panel, but I'm not a very major desktop user), and «find .[a-zA-Z]* -mtime 0» doesn't show a large number of files. I'm guessing it's certain badly written applications which are creating the «hundreds of dot files» that people are reporting become zero lengh, and if they are seeing it happen a lot, it must be because the dot files are getting updated very frequently. I don't know what the bad applications are, but the people who complained about large number of state files disappearing should check into which application were involved, and try to figure out how often they are getting modified. As I said, if large number of files are getting frequently modified, it's going to be bad for SSD's as well, there are multiple reasons to fix badly written applications, even if 2.6.30 will have a fix for the most common cases. (Although some server folks may mount with a flag to disable it, since it will cost performance.) И это не говоря уж о том, что критичные файлы UNIX (такие как /etc/passwd), который читаются при каждом (!) вызове, скажем, ls -l, записаны в виде простого текста. И эти файлы надо заново читать и заново парсить при каждом вызове ls -l! Было бы гораздо лучше использовать бинарный формат. Или БД. Или некий аналог реестра. Как минимум, для вот таких вот критичных для производительности ОС файлов. Two famous people, one from MIT and another from Berkeley (but working on Unix) once met to discuss operating system issues. The person from MIT was knowledgeable about ITS (the MIT AI Lab operating system) and had been reading the Unix sources. He was interested in how Unix solved the PC loser-ing problem. The PC loser-ing problem occurs when a user program invokes a system routine to perform a lengthy operation that might have significant state, such as IO buffers. If an interrupt occurs during the operation, the state of the user program must be saved. Because the invocation of the system routine is usually a single instruction, the PC of the user program does not adequately capture the state of the process. The system routine must either back out or press forward. The right thing is to back out and restore the user program PC to the instruction that invoked the system routine so that resumption of the user program after the interrupt, for example, re-enters the system routine. It is called «PC loser-ing» because the PC is being coerced into «loser mode,» where «loser» is the affectionate name for «user» at MIT. The MIT guy did not see any code that handled this case and asked the New Jersey guy how the problem was handled. The New Jersey guy said that the Unix folks were aware of the problem, but the solution was for the system routine to always finish, but sometimes an error code would be returned that signaled that the system routine had failed to complete its action. A correct user program, then, had to check the error code to determine whether to simply try the system routine again. The MIT guy did not like this solution because it was not the right thing. — The Rise of «Worse is Better» By Richard Gabriel Если кратко и своими словами, то в начале разработки UNIX авторы UNIX решили попросту выдавать ошибку из ядра пользовательской программе, если пользовательская программа прервана по сигналу, и на этот сигнал повешен обработчик. Иными словами, если вы перехватили Ctrl-C (т. е. поставили на него обработчик) в своей программе, а юзер за терминалом нажал этот самый Ctrl-C, то ОС выполнит обработчик, а потом вместо простого продолжения того сисвызова, который выполнялся в момент Ctrl-C, просто прервёт его, вернув из ядра в пользовательскую программу EINTR. В результате программисту, пишущему эту программу придётся эту EINTR предусмотреть. А это усложняет этот userspace код. Ценой упрощения кода ядра. Да, нужно было сделать по-другому. Усложнить код ядра и упростить userspace код, который придётся писать всем программистам. Но тому человеку из Беркли из цитаты выше было пофигу. Он фактически сказал: «Да мне пофиг, что все будут страдать, главное, чтоб код ядра попроще был». Дальше — больше. Позже в UNIX-системах всё же пофиксили упомянутую особенность, добавив так называемый SA_RESTART. То есть вместо того, чтобы просто всё пофиксить, они добавили специальный флаг. Так мало того, что они это сделали, этот SA_RESTART ещё и не всегда работает! В частности, в GNU/Linux select, poll, nanosleep и др. не продолжают свою работу после перехваченного прерывания даже в случае SA_RESTART! Вообще, конкретные обстоятельства, возникшие во время разработки оригинальной UNIX, сильно оказали на неё влияние. Скажем, читал где-то, что команда cp названа именно так, а не copy, потому что UNIX разрабатывали с использованием терминалов, которые очень медленно выдавали буквы. А потому набрать cp было быстрее, чем copy. UPD от 2017-02-12: найти именно ту ссылку, которую я видел когда-то давно, и в которой приводился пример с cp и copy, мне не удалось. Но есть, например, вот эта ссылка. Commands — Are These Real Words? The basic AIX commands (and all UNIX system commands) are, for the most part, very short, cryptic, two-letter command names. Imagine back years ago, when computers had only very slow teletype keyboards and paper “displays.” (Some of us aren’t imagining, we’re remembering!) Imagine also, people who didn’t like typing long commands because there was such a long delay between commands and the computer response. If there were any mistakes, the user had to retype the whole thing (especially aggravating for folks that type with only two fingers!). Also, some UNIX commands came from university students and researchers who weren’t bound by usability standards (no rules, merely peer pressure). They could write a very useful, clever command and name it anything—their own initials, for example (awk by Aho, Weinberger, and Kernighan), or an acronym (yacc, Yet Another Compiler-Compiler). Вообще, названия утилит UNIX — это отдельная история. Скажем, название grep идёт от командй g/re/p в текстовом редакторе ed. (Ну а cat — от concatenation, я надеюсь, это все и так знали :) Ну и для кучи: vmlinuz — gZipped LINUx with Virtual Memory support). printf внезапно является далеко не самым быстрым способом вывода информации на экран или в файл. Не знали, да? А дело в том, что printf, как и сама UNIX в целом, был придуман не для оптимизации времени, а для оптимизации памяти. printf каждый раз парсит в рантайме строку формата. Именно поэтому в веб сервере H2O был придуман специальный препроцессор, который переносит парсинг строки формата на этап компиляции. UPD от 2017-02-12: источник. Когда Кена Томпсона, автора UNIX (вместе с Деннисом Ритчи) спросили, что бы он поменял в UNIX, он сказал, что назвал бы функцию creat (sic!) как create. UPD от 2017-02-12: источников полно, например этот. No comments. Замечу, что позже этот же Кен Томпсон вместе с другими разработчиками оригинальной UNIX создал систему Plan 9, исправляющую многие недостатки UNIX. И в ней эта функция называется create :) Он смог :) Ещё одна цитата: A child which dies but is never waited for is not really gone in that it still consumes disk swap and system table space. This can make it impossible to create new processes. The bug can be noticed whenseveral & separators are given to the shell not followed by ancommand without an ampersand. Ordinarily things clean themselves upwhen an ordinary command is typed, but it is possible to get into asituation in which no commands are accepted, so no waits are done;the system is then hung.The fix, probably, is to have a new kind of fork which creates aprocess for which no wait is necessary (or possible); also to limit the number of active or inactive descendants allowed to a process. — Источник Это цитата из очень раннего манула UNIX. Уже тогда существование зомби-процессов признавалось багом. Но потом на этот баг попросту забили. Понятное дело, что гораздо позже эта проблема всё же была решена. Т. е. в современном GNU/Linux инструменты для убивания зомби-процессов всё же существуют. Но о них мало кто знает. Обычном kill'ом зомби не убиваются. Про существование зомби-процессов все говорят: «It's for design». Ещё немного про уже упомянутый язык C. Вообще язык C разрабатывался одновременно с UNIX, поэтому критикуя UNIX, нужно покритиковать и C тоже. То, что C очень плох, написано много, я не буду повторять все эти аргументы. Там, синтаксис типов плохой, препроцессор ужасен, легко выстрелить себе в ногу, всякие 4[\"string\"], всякие sizeof ('a') != sizeof (char) (в C, не в C++!), всякие i++ + ++i, всякие while (*p++ = *q++) ; (пример из Страуструпа, второе дополненное издание) и так далее и тому подобное. Скажу лишь вот что. В C до сих пор не научились удобно работать со строками. Неудобство работы со строками постоянно приводит к разнообразным проблемам безопасности. И эту проблему до сих пор не решили! Вот относительно свежий документ от комитета C. В нём обсуждается весьма сомнительный способ решения проблемы со строками. И делается вывод, что этот способ плох. Год публикации: 2015. То есть даже к 2015-му году окончательного решения ещё нет! И это не говоря об отсутствии простой, удобной и мультиплатформенной системы сборки (а не этого монстра autotools, который ещё и не поддерживает винду, и другого монстра cmake, который поддерживает винду, но всё равно монстр), стандартного менеджера пакетов, удобного как npm (js) или carge (rust), нормальной portability library, с помощью которой можно было кроссплатформенно хотя бы прочитать содержимое папки и хотя бы даже главного сайта C, который был бы главной точкой входа для всех новичков и содержал бы в себе не только документацию, но и краткую инструкцию по установке инструментов C на любую платформу, по созданию простого проекта на C, а также содержал бы удобный поиск по пакетам C (которые должны быть размещены в стандартном репозитории) и, главное, был бы точкой сбора user community. Я даже зарегал домен c-language.org в надежде, что когда-нибудь я создам там такой сайт. Эх, мечты, мечты. (У меня ещё cpp-language.org заныкан, бугога :)) Но всего этого нет. Хоть это и есть у всех популярных языков, кроме C и C++. И даже у Haskell всё это есть. И у Rust. У Rust, у этого выскочки, который, кстати говоря, метит в ту же нишу, что и C. Есть единый конфиг, который одновременно является конфигом проекта, конфигом сборки и конфигом для менеджера пакетов (собственно, cargo — это менеджер проектов и система сборки одновременно). Есть возможность указания в качестве зависимости для данного пакета другого пакета, размещённого где-то в *GIT*, в том числе указание в качестве зависимости напрямую программы на *GITHUB*. Генерация из коробки документации из сорцов, записанной в комментах на *MARKDOWN*. И пакетный менеджер, использующий для версий *SEMVER*. Итак, *GIT*, *GITHUB*, *MARKDOWN*, *SEMVER*, короче говоря *BUZZWORDS*, *BUZZWORDS* и ещё раз *HIPSTERS' BUZZWORDS*. И всё сразу из коробки. Прямо вот заходишь на их главный сайт, и вот на тебе на блюдечке с голубой каёмочкой. И работает всё одинаково на всех платформах. Несмотря на то, что Rust — это вроде как язык системного программирования, а не какой-нибудь там javascript. Несмотря на то, что в Rust можно байты гонять. И арифметика указателей там есть. Так почему же у них, у этих выскочек-растовцев, эти хипстерские баззворды есть, а у нас, сишников, их нет? Обыдно. Я помню, один знакомый спрашивает у меня, где посмотреть список пакетов для C/C++. Пришлось сказать ему, что такого единого места нет. Он: «Программисты на C/C++ должны страдать?» Мне нечего было ему ответить. Ах да, забыл ещё одну вещь. Посмотрите, пожалуйста, на прототип функции signal в том виде, в котором он дан в стандарте C: void (*signal(int sig, void (*func)(int)))(int); и попытайтесь его понять. Терминал в UNIX — жуткое legacy. Имена файлов в файловых системах UNIX (ext2 и пр.) есть просто поток байтов без кодировки. В какой кодировке они будут интерпретированы, зависит от локали. То есть если создать файл на ОС в одной локали, а потом пытаться посмотреть его имя в ОС в другой локали, будет плохо. В виндовом NTFS такой проблемы нет. UNIX shell хуже PHP! Да, да, а вы что, не знали? Сейчас модно ругать PHP. Но ведь UNIX shell ещё хуже :) Особенно плохим он становиться, если пытаться на нём программировать, ведь полноценным языком программирования он не является. Но даже для своей ниши (скриптинг типичных задач по администрированию) он годится плохо. Виной тому примитивность shell, непродуманность, legacy, куча частных случаев, костылей, бардак с кавычками, бекслешами, специальными символами и повёрнутость shell'а (как и всего UNIX) на простом тексте. Начнём с затравки. Как рекурсивно найти в папке foo все файлы с именем \\? Правильный ответ таков: find foo -name '\\\\'. Ну или так: find foo -name \\\\\\\\. Последний вариант вызовет особенно много вопросов. Попробуйте объяснить человеку, плохо разбираемущемуся в UNIX shell, почему здесь нужно именно четыре бекслеша, а не два и не восемь (грамотеи, подскажите, как правильно написать это предложение, пишите в личку). А написать здесь нужно четыре бекслеша, потому что UNIX shell делает backslash expanding, и find тоже его делает. Как touch'нуть все файлы в папке foo (и во вложенных)? На первый взгляд, один из способ таков: find foo | while read A; do touch $A; done. Ну, на первый взгляд. На самом деле здесь можно придумать аж 5 нюансов, которые могут испортить нам малину (и привести к проблемам с безопасностью): Имя файла может содержать бекслеш, поэтому нужно писать не read A, а read -r A. Имя файла может содержать пробел, поэтому нужно писать не touch $A, а touch \"$A\". Имя файла может не только содержать пробел, но и начинаться с пробела, поэтому нужно писать не read -r A, а IFS=\"\" read -r A. Имя файла может содержать перевод строки, поэтому вместо find foo нужно использовать find foo -print0, а вместо IFS=\"\" read -r A нужно использовать IFS=\"\" read -rd \"\" A (тут я не совсем уверен). Имя файла может начинаться с дефиса, поэтому вместо touch \"$A\" нужно писать touch -- \"$A\". Итоговый вариант выглядит так: find foo -print0 | while IFS=\"\" read -rd \"\" A; do touch -- \"$A\"; done. Круто, да? И здесь мы, кстати, не учли, что POSIX не гарантирует (я не совсем в этом уверен), что touch поддерживает опцию --. Если учитывать ещё и это, то придётся для каждого файла проверять, что он начинается с дефиса (или что не начинается со слеша) и добавлять в начало ./. Теперь вы поняли, почему скрипты configure, генерируемые autoconf'ом такие большие и трудночитаемые? Потому что этому configure нужно учитывать всю эту муть, включая совместимость с разными shell'ами. (В данном примере для демонстрации я использовал решение с пайпом и циклом. Можно было использовать решение с -exec или xargs, но это было бы не так эффектно). (Ладно, хорошо, мы знаем, что имя файла начинается с foo, поэтому оно не может начинаться с пробела или дефиса). В переменной A лежит имя файла, нужно удалить его на хосте a@a. Как это сделать? Может быть так: ssh a@a rm -- \"$A\" (как вы уже заметили, мы тут уже учли, что имя файла может содержать пробелы и начинаться с дефиса)? Ни в коем случае! ssh — это вам не chroot, не setsid, не nohup, не sudo и не какая-нибудь ещё команда, которая получает exec-команду (т. е. команду для непосредственной передачи сисвызовам семейства execve). ssh (как и su) принимает shell-команду, т. е. команду для обработки shell'ом (термины exec-команда и shell-команда — мои). ssh соединяет все аргументы в строку, передаёт строку на удалённую сторону и там выполняет shell'ом. Окей, может быть так: ssh a@a 'rm -- \"$A\"'? Нет, эта команда попытается найти переменную A на удалённой стороне. А её там нет, потому что переменные через ssh не передаются. Может, так: ssh a@a \"rm -- '$A'\"? Нет, это не сработает, если имя файла содержит одинарную кавычку. В общем, не буду вас мучать, правильный ответ таков: ssh a@a \"rm -- $(printf '%q\\n' \"$A\")\". Согласитесь, удобно? Как зайти на хост a@a, с него — на b@b, с него — на c@c, с него — на d@d, а с него удалить файл /foo? Ну, это легко: ssh a@a \"ssh b@b \\\"ssh c@c \\\\\\\"ssh d@d \\\\\\\\\\\\\\\"rm /foo\\\\\\\\\\\\\\\"\\\\\\\"\\\"\" Слишком много бекслешей, да? Ну, не нравится так, давайте чередовать одинарные и двойные кавычки, будет не так скучно: ssh a@a 'ssh b@b \"ssh c@c '\\''ssh d@d \\\"rm /foo\\\"'\\''\"' А между прочим, если бы вместо shell'а был Lisp, и там функция ssh передавала бы на удалённую сторону не строку (вот она, повёрнутось UNIX на тексте!), а уже распарсенный AST (abstract syntax tree), то такого ада бекслешей не было бы: (ssh \"a@a\" '(ssh \"b@b\" '(ssh \"c@c\" '(ssh \"d@d\" '(rm \"foo\"))))) «А? Что? Lisp? Что за Lisp?» Интересно, да? На, читайте. И другие статьи Грэма. На русском тоже можно найти. Совместим предыдущие два пункта. Имя файла лежит в переменной A. Нужно зайти на a@a, с него — на b@b, далее на c@c, d@d и удалить файл, лежащий в переменной A. Это я оставляю вам в качестве упражнения :) (Сам я не знаю, как это сделать :) Ну, может, придумаю, если подумаю). echo вроде как предназначен, чтобы печатать на экран строки. Вот только использовать его для этой цели, если строчка чуть сложнее, чем «Hello, world!», нельзя. Единственно верный способ вывести произвольную строку (скажем, из переменной A) таков: printf '%s\\n' \"$A\". Допустим, нужно направить stdout и stderr команды cmd в /dev/null. Загадка: какие из этих шести команд выполняют поставленную задачу, а какие — нет? cmd > /dev/null 2>&1 cmd 2>&1 > /dev/null { cmd > /dev/null; } 2>&1 { cmd 2>&1; } > /dev/null ( cmd > /dev/null ) 2>&1 ( cmd 2>&1 ) > /dev/null Оказывается, правильный ответ — 1-я, 4-я и 6-я выполняют, 2-я, 3-я и 5-я — не выполняют. Опять-таки, выяснение причин этого оставляется в качестве упражения :) Вообще, этот пост появился в ответ на вот этот пост. Там говорилось, мол, в винде специальная дата используется как метка драйвера от Microsoft. Вместо ввода специального аттрибута или проверки производителя. Особенностей такого рода в UNIX полно. Является ли файл скрытым, выясняется на основе наличия точки в начале файла вместо специального аттрибута. Когда я сам впервые об этом узнал (да, да, в те далёкие времена, когда я впервые поставил Ubuntu), я был шокирован. Я подумал, вот идиоты. А сейчас привык. Но если вдуматься, это жуткий костыль. Далее, shell выясняет, является ли он login shell'ом на основе дефиса, переданного первым символом в argv[0] (?!). Это abuses (ну или misuses, неправильно использует, не знаю, как по-русски сказать) argv[0]. argv[0] не для этого предназначен. Вместо какого-нибудь другого способа. Любой другой способ был бы красивее. Как угодно, любым другим аргументом, переменной окружения. В BSD sockets юзер вынужден сам менять порядок байт у номера порта. А всё потому, что когда-то давно кто-то допустил в коде ядра UNIX ошибку, не предусмотрев смену порядка байт. И в качестве временного хака исправил user space код вместо кода ядра. Так и живём. Оттуда это и в Windows перешло (вместе с файлом /etc/hosts, он же C:\\windows\\system32\\drivers\\etc\\hosts). UPD от 2017-02-12: источник. «Философия UNIX». Есть мнение, что якобы UNIX прекрасна и идеальна. Что все её основные идеи («всё есть файл», «всё есть текст» и т. д.) прекрасны и составляют так называемую прекрасную «философию UNIX». Так вот, как вы уже начали догадываться, это не совсем так. Давайте разберём эту «философию UNIX» по пунктам. Сразу скажу: я не хочу сказать, что все пункты нужно отменить, просто я указываю на их неуниверсальность. «Всё есть текст». Как мы с вами уже выяснили на примере /etc/passwd, повсеместное использование простого текста может привести к проблемам с производительностью. И вообще, авторы UNIX фактически придумали для каждого системного конфига (passwd, fstab и так далее) свой формат. Со своими правилами экранирования специальных символов. Да, а вы что думали? /etc/fstab использует пробелы и переносы строк как разделители. Но что если имена папок содержат, скажем, пробелы? На этот случай формат fstab'а предусматривает специальное экранирование имён папок. Так что любой скрипт, читающий fstab, оказывается, должен это экранирование интерпретировать. Например, с помощью специально предназначенной для этого утилиты fstab-decode (запускать от рута). Не знали, да? Идите исправляйте свои скрипты :) В результате для каждого системного конфига нужен свой парсер. И было бы гораздо проще, если бы для системных конфигов использовался вместо этого какой-нибудь JSON или XML. А может быть даже некий бинарный формат. Особенно для тех конфигов, которые постоянно читаются разными программами. И для которых, как следствие, нужна хорошая скорость чтения (а у бинарных форматов она выше). Я не закончил по поводу «всё есть текст». Стандартные утилиты выдают вывод в виде простого текста. Для каждой утилиты фактически нужен свой парсер. Часто приходится парсить вывод той или иной утилиты при помощи sed, grep, awk и т. д. У каждой утилиты свои опции для того, чтобы установить, какие именно столбцы нужно выдавать, по каким столбцам нужно сортировать вывод и т. д. Было бы лучше, если бы утилиты выдавали вывод в виде XML, JSON, некоего бинарного формата или ещё чего-нибудь. А для удобного вывода этой информации на экран и для дальнейшей работы с ней можно было бы пайпить результат в дополнительные утилиты, которые убирают те или иные столбцы, сортируют по тому или иному столбцу, выбирают нужные строки и т. д. И либо выводят результат в виде красивой таблички на экран, либо передают его куда-то дальше. И всё это универсальным способом, не зависящим от исходной утилиты, которая сгенерировала вывод. И без необходимости парсить что-либо регексами. Да, UNIX shell плохо работает с JSON и XML. Но ведь у UNIX shell полно других недостатков. Нужно выкинуть его вовсе и заменить на некий другой язык, который помимо всего прочего может удобно работать со всякими JSON. Вы только представьте! Вот допустим, нужно удалить все файлы в текущей папке с размером, большим 1 килобайта. Да, я знаю, что такое надо делать find'ом. Но давайте предположим, что это нужно сделать непременно ls'ом (и без xargs). Как это сделать? Вот так: LC_ALL=C ls -l | while read -r MODE LINKS USER GROUP SIZE M D Y FILE; do if [ \"$SIZE\" -gt 1024 ]; then rm -- \"$FILE\"; fi; done. (LC_ALL здесь нужен был, чтобы быть уверенным, что дата будет занимать именно три слова в выводе ls). Мало того, что это решение выглядит некрасиво, оно ещё страдает рядом недостатков. Во-первых, оно не будет работать, если имя файла содержит перевод строки или начинается с пробела. Далее, нам нужно явно перечислить названия всех столбцов ls, ну или как минимум помнить, на каком месте находятся интересующие нас (т. е. SIZE и FILE). Если мы ошибёмся в порядке столбцов, то ошибка выяснится лишь на этапе выполнения. Когда мы удалим не те файлы :) А как бы выглядело решение в идеальном мире, который я предлагаю? Как-то так: ls | grep 'size > 1kb' | rm. Кратко, а главное смысл виден из кода, и невозможно ошибиться. Смотрите. ls в моём мире всегда выдаёт всю инфу. Специальня опция -l для этого не нужна. Если нужно убрать все столбцы и оставить только имя файла, то это делается специальной утилитой, в которую нужно направить вывод ls. Итак, ls выдаёт список файлов. В некоем структуированном виде, скажем, JSON. Это представление «знает» названия столбцов и их типы, т. е. что это, строка, число или что-то ещё. Далее этот вывод направляется в grep, который в моём мире выбирает нужные строки из этого JSON. JSON «знает» названия полей, поэтому grep «понимает», что здесь означает «size». Более того, JSON содержит инфу о типе поля size. Он содержит инфу о том, что это число, и даже что это не просто число, а размер файла. Поэтому можно сравнить его с 1kb. Далее grep направляет вывод в rm. rm «видит», что он получил файлы. Да, да, JSON ещё и хранит инфу о типе этих строк, о том, что это — файлы. И rm их удаляет. А ещё JSON отвечает за правильное экранирование специальных символов. Поэтому файлы со спецсимволами «просто работают». Круто? Идею я взял отсюда (там ещё есть ссылка на более подробный английский оригинал), посмотрите. Ещё замечу, что в Windows Powershell реализовано как раз что-то похожее на эту идею. UNIX shell. Ещё одна базовая идея UNIX. Причём о мелких недостатках UNIX shell я уже поговорил в первой части статьи. Сейчас будут крупные. В чём «крутость» UNIX shell? В том, что на момент своего появления (это было очень давно) UNIX shell был гораздо мощнее командных интерпретаторов, встроенных в другие ОС. И позволял писать более мощные скрипты. Да и вообще, на момент своего появления UNIX shell был, видимо, самым мощным из скриптовых языков вообще. Потому что нормальных скриптовых языков, т. е. таких, которые бы позволяли полноценное программирование, а не только скриптинг, тогда, видимо, вообще не существовало. Это потом уже в один прекрасный день один программист по имени Larry Wall заметил, что UNIX shell всё-таки недостаёт до нормального языка программирования. И он захотел соединить краткость UNIX shell'а с возможностью полноценного программирования из C. И создал Perl. Да, Perl и другие последующие скриптовые языки программирования фактически заменили UNIX shell. Это константирует даже Роб Пайк, один из авторов (как я считаю) той самой «философии UNIX» (про него мы ещё поговорим). Вот здесь на вопрос об «одной утилите для одной вещи» он сказал: «Those days are dead and gone and the eulogy was delivered by Perl». Причём я считаю, что эта его фраза относилась к типичному использованию UNIX shell, т. е. к ситуации связывания большого количества маленьких утилит в shell-скрипте. Нет, говорит Пайк, просто используйте Perl. Я не закончил про UNIX shell. Рассмотрим ещё раз пример кода на shell, который я уже приводил: find foo -print0 | while IFS=\"\" read -rd \"\" A; do touch -- \"$A\"; done. Здесь в цикле вызывается touch (да, я знаю, что этот код можно переписать на xargs, причём так, чтобы touch вызывался только один раз; но давайте пока забьём на это, хорошо?). В цикле вызывается touch! То есть для каждого файла будет запущен новый процесс! Это нереально неэффективно. Код на любом другом языке программирования будет работать быстрее этого. Просто на момент появления UNIX shell он был одним из немногих языков, которые позволяют написать это действие в одну строчку. Короче говоря, вместо UNIX shell нужно использовать любой другой скриптовый язык программирования. Который подходит не только для скриптинга, но и для реального программирования. Который не запускает новый процесс каждый раз, когда нужно «touch'нуть» файл. Возможно, понадобится «доложить» в этот скриптовый язык средства для простого выполнения вещей, которые есть в shell, скажем, для создания пайпов. Простота. Здесь я говорю не конкретно про shell и про связывание кучи простых утилит из shell'а (про это был предыдущий пункт), а про простоту вообще. Использование простых инструментов. Скажем, редактирование картинки sed'ом. Да, да. Конвертим jpg в ppm при помощи командной строки. Затем при помощи графического редактора, grep, sed и такой-то матери редактируем картинку. А потом обратно в jpg. Да, так можно. Но часто photoshop'ом или gimp'ом всё-таки лучше. Хоть это и большие, интегрированные программы. Не в стиле UNIX. На этом я закончу эти пункты. Да, хватит. Есть идеи в UNIX, которые мне реально нравятся. Скажем, «программа должна делать одну вещь и делать её хорошо». Но не в контексте shell. Вы уже поняли, что я не люблю shell. (Ещё раз повторю, я считаю, что в приведённом выше интервью Пайка он воспринял принцип «программа должна делать одну вещь и делать её хорошо» именно в контексте shell и потому отверг его). Нет, я говорю про этот принцип в своей сути. Скажем, консольный почтовый клиент не должен иметь встроенный текстовый редактор, он должен просто запустить некий внешний редактор. Или вот принцип, по которому нужно писать консольное ядро для программы и потом графическую оболочку для этого ядра. Теперь общая картина. Однажды появился UNIX. На момент появления он был прорывом. И он был во многом лучше своих конкурентов. UNIX имел много идей. И, как и любая ОС, UNIX требовал от программистов соблюдения некоторых принципов при написании прикладных программ. Идеи, лежащие в основе UNIX, стали называться «философией UNIX». Одним из тех людей, которые сформулировали философию UNIX, был уже упомянутый Роб Пайк. Он это сделал в своей презентации «UNIX Style, or cat -v Considered Harmful». После презентации он вместе с Керниганом опубликовал статью по мотивам презентации. В ней авторы рассказали о том, что, скажем, предназначение cat — это только конкатенация и ничего больше (ну то есть «склеивание» файлов, мы с вами помним, как расшифровывается cat, так ведь?). Возможно, что это Пайк как раз и придумал «философию UNIX». В честь этой презентации был назван сайт cat-v.org, почитайте его, очень интересный сайт. Но потом, через много лет, этот же Пайк сделал ещё две презентации, в которых, как я считаю, отменил свою философию обратно. Поняли, фанатики, да? Ваш кумир отказался от своей же философии. Можете расходиться по домам. В первой презентации «Systems Software Research is Irrelevant» Пайк сетует на то, что никто больше не пишет новых ОС. А даже если и пишут, то просто ещё один UNIX (который подразумевается в этой презентации уже чем-то неинтересным): «New operating systems today tend to be just ways of reimplementing Unix. If they have a novel architecture — and some do — the first thing to build is the Unix emulation layer. How can operating systems research be relevant when the resulting operating systems are all indistinguishable?» Вторую презентацию Пайк прямо называет: «The Good, the Bad, and the Ugly: The Unix Legacy». Пайк говорит, что простой текст не универсален, он хорош, но работает не всегда: «What makes the system good at what it's good at is also what makes it bad at what it's bad at. Its strengths are also its weaknesses. A simple example: flat text files. Amazing expressive power, huge convenience, but serious problems in pushing past a prototype level of performance or packaging. Compare the famous spell pipeline with an interactive spell-checker». Далее: «C hasn't changed much since the 1970s… And — let's face it — it's ugly». Дальше Пайк признаёт ограниченность пайпов, соединяющих простые утилиты, ограниченность регексов. UNIX был гениальным на момент своего появления. Особенно, если учесть, какие инструменты были в распоряжении у авторов UNIX. У них не было уже готового UNIX, чтобы на нём можно было разрабатывать UNIX. У них не было IDE. И программировали они вообще на ассемблере изначально. У них, видимо, был только ассемблер и текстовый редактор. Люди, стоящие у истоков UNIX, в определённый момент начали писать новую ОС: Plan 9. В том числе упомянутые Томпсон, Ритчи и Пайк. Учитывая многие ошибки UNIX. Но и Plan 9 никто не возводит в абсолют. В «Systems Software Research is Irrelevant» Пайк упоминает Plan 9, но несмотря на это всё равно призывает писать новые ОС. James Hague, ветеран программирования (занимается программированием с восьмидесятых) пишет: «What I was trying to get across is that if you romanticize Unix, if you view it as a thing of perfection, then you lose your ability to imagine better alternatives and become blind to potentially dramatic shifts in thinking» (ссылка). Прочитайте эту статью и его же статью «Free Your Technical Aesthetic from the 1970s», на которую он ссылается. (Вообще, если вам понравилась моя статья, то и его блог тоже, наверное, понравится, погуляйте там по ссылкам). Итак, я не хочу сказать, что UNIX — плохая система. Просто обращаю ваше внимание на то, что у неё есть полно недостатков, как и у других систем. И «философию UNIX» я не отменяю, просто обращаю внимание, что она не абсолют. Мой текст обращён скорее к фанатикам UNIX и GNU/Linux. Провокационный тон просто чтобы привлечь ваше внимание. UPD от 2017-02-14: комментаторы указывают, что сравнивать UNIX shell с PHP некорректно. Конечно, некорректно! Потому что UNIX shell не претендует на то, чтобы быть полноценным языком программирования, он предназначен для скриптинга системы. Вот только я в одно время этого не знал. И вдобавок считал UNIX shell прекрасным. Вот для людей в таком же положении я всё это и говорю. Ещё как минимум один комментатор говорит, что сравнивать UNIX shell нужно с cmd. Я бы сказал, что сравнивать надо с Windows Powershell. Последний, как я уже говорил, в чём-то превосходит UNIX shell. UPD от 2017-02-14: мне понравился вот этот коммент от sshikov: Но я скажу за автора — к сожалению, прямо сегодня можно найти сколько угодно восторженных статей типа «А вот есть такая замечательная фигня, как bash, щас я вам про нее расскажу...» — где unix way откровенно перехваливается неофитами. Это не помешает иногда компенсировать долей скепсиса. Да, в этом-то и всё дело! Достало, что хвалят UNIX way. Что считают UNIX красивым и ещё и других учат. А использовать-то UNIX можно. UPD от 2017-02-14: как минимум один комментатор сказал, что пересел с Windows на UNIX-подобные ОС и счастилив. Что поначалу он плевался от UNIX, но потом решил, что программировать под UNIX гораздо проще, чем под Windows. Так вот, я тоже сперва использовал и программировал на Windows. Потом пересел на UNIX. И сперва, конечно, было очень непривычно. Потом прочувствовал «философию UNIX», ощутил всю её мощь. Программировать под UNIX стало легко. Но позже пришло ещё одно озарение. Что UNIX неидеальна, а «философия UNIX» неабсолютна. Что программирование на «голом UNIX», с использованием C и Shell сильно уступает, скажем, Web-программированию. И далеко не только потому, что в Web-программировании используются языки, в которых трудно выстрелить себе в ногу, в отличие от C (тут языку C предъявить нечего, он намеренно является низкоуровневым). Но ещё и из-за всех этих quirks мейкфайлов, шела, языка C. Отсутствия удобных инструментов, систем сборки, менеджеров пакетов. Всё это, в принципе, можно было бы исправить. Вот я написал эту статью, чтобы открыть на это глаза тем, кто об этом не знает. У Windows тоже полно недостатков (я разве где-то говорил, что Windows лучше UNIX?). Но в чём-то Windows лучше UNIX (как минимум в некоторых особенностях Powershell). Сейчас я продолжаю использовать и программировать под UNIX. UNIX меня устраивает, мне достаточно удобно, хотя теперь уже я вижу многие его недостатки. Я не призываю бросать UNIX. Используйте UNIX дальше, просто не считайте его идеалом. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10070776. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10071096. UPD от 2017-02-15: habrahabr.ru/post/321652/#comment_10071714. UPD от 2017-02-16: понравился этот коммент: habrahabr.ru/post/321652/#comment_10066240. UPD от 2017-02-16: многие комментаторы рассказывают, как же полезны и удобны UNIX системы. Что они есть уже десятки лет, на них работает весь интернет. Что они стабильны и прекрасно справляются с возложенными на них задачами. И даже удалённо переустановить GNU/Linux можно :) А я и не спорю. Я не призываю отказываться от UNIX. Я просто хочу, чтобы вы видели недостатки UNIX. UNIX работает, используйте его. Процитирую James Hague, на которого я уже ссылался: Enough time has passed since the silly days of crazed Linux advocacy that I'm comfortable pointing out the three reasons Unix makes sense: 1. It works. 2. It's reliable. 3. It stays constant. But don't--do not--ever, make the mistake of those benefits being a reason to use Unix as a basis for your technical or design aesthetic. Yes, there are some textbook cases where pipelining commands together is impressive, but that's a minor point. Yes, having a small tool for a specific job sometimes works, but it just as often doesn't. Одно время я тоже, как и многие из вас, повёлся на эту «философию UNIX». Думал, что она прекрасна. А потом понял, что это не так. И вот этим своим открытием я хочу с вами поделиться. Мои мысли не новы. Они уже есть в приведённых мною ссылках. Я просто хочу сообщить эти мысли аудитории Хабра. Мой пост написан наскоро, ночью. Читайте скорее не его, а ссылки, которые я привожу. В первую очередь две презентации Пайка, в которых он «отменяет философию UNIX» и два поста от James Hague. Мой пост фактически написан, чтобы привлечь внимание к этим ссылкам. Как минимум один из комментаторов сказал, что многие из названных мной «недостатков» UNIX недостатками не являются. Например, слишком короткие имена команд. Ну да. Это не недостаток. Но это пример необдуманного решения. Сиюминутного решения, принятого под влиянием обстоятельств, имевших важность тогда. Как и с тем примером с /usr или make. Я показываю, что UNIX была непродумана. Да и вообще, вглядитесь в историю UNIX! Сотрудникам Bell Labs не понравилась сложность проекта Multics. Они сказали: «Да ну этот Multics, давайте по-быстрому напишем свою ОС, запростецкую». И написали. Понимаете? ОС получилась довольно хорошей. Но не идеальной. UNIX — это хак. Успешный хак, который выполнил свою миссию и продолжает её выполнять. В комментариях была мысль, что заголовок поста не соответствует содержанию, и что я критикую не самую суть, философию UNIX, а просто привожу некий список недостатков. Возможно даже не всего класса UNIX-подобных систем, а конкретных реализаций. Так вот, это не так. Да, статья начинается с перечисления мелких недостатков. Этим я обращаю внимание на то, что в UNIX полно костылей, как и в других системах. В том числе очень старых, оставшихся во всех UNIX системах и попавших во все стандарты. Но я критикую и саму философию UNIX. Основные принципы (но не все!). Язык C, UNIX shell, идею конвееров, «всё есть текст». Замечу, что компилятор C и make, хоть и являются по идее отдельными программами, всегда рассматриваются как неотъемлемая часть экосистемы UNIX. И входят в POSIX. Некоторые комментаторы пишут: «А я сижу в IDE и не использую этот ваш make». Ну окей, хорошо, мой пост предназначен скорее как раз для тех фанатиков, которые считают, что всякие IDE — это не труъ и что программировать нужно непременно используя голый C, make и shell. И я не говорю, что философия UNIX (даже в тех местах, которые мне не нравятся) всегда не верна. Часто конвееры и shell-скрипты — это именно то, что нужно. Но не всегда. Некоторые комментаторы указывают, что голый shell, make и прочее часто скрыты от глаз юзера всякими обёртками, всякими IDE, сложными системами сборки, GUI-интерфейсами и пр. Ну да. Так ведь это и есть признак кривости системы :) Когда что-то уродское покрывают слоем красоты. А ещё абстракции протекают. А потому использовать, скажем, autotools ещё сложнее, чем голый make. Потому что чтобы использовать autotools, нужно знать ещё и m4, make и shell. Да, да, всю эту цепочку языков, используемых при генерации окончательного мейкфайла. Один комментатор приводит следующие принципы UNIX: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. С первыми двумя я согласен при условии, что они понимаются в отрыве от UNIX shell и конвееров. Их можно перенести даже на новомодные микросервисы, общающиеся с помощью REST. С третьим я не согласен (как я понимаю, подразумевается именно придумываение простого кастомного текстового формата для каждого случая вместо единого формата наподобие JSON). Часто текст — это именно то, что нужно. Но пихать его везде как universal interface глупо. На эту роль скорее претендует JSON или XML. Или, может, какой-нибудь формат для структуированных данных, который ещё не изобрели. Многие указали на искусственность некоторых примеров на shell. Ну да, я знаю, что их можно было бы переписать на find -exec или xargs. Ну что вы хотите, наскоро написанная статья. Можно было привести примеры получше, просто мне не хотелось. Это не отменяет того, что в shell'е постоянно возникают проблемы со специальными символами. Которые нужно по-особому обходить. И вообще у shell'а полно quirks, которые нужно постоянно держать в голове. И он запускает новые программы на каждый чих. Я вам ещё покушать принёс. Вот вам цитата от безусловно ещё одного вашего кумира Линуса Торвальдса: iTWire: Systemd seems to depart to a large extent from the original idea of simplicity that was a hallmark of UNIX systems. Would you agree? And is this a good or a bad thing? Linus Torvalds: So I think many of the «original ideals» of UNIX are these days more of a mindset issue than necessarily reflecting reality of the situation. There's still value in understanding the traditional UNIX «do one thing and do it well» model where many workflows can be done as a pipeline of simple tools each adding their own value, but let's face it, it's not how complex systems really work, and it's not how major applications have been working or been designed for a long time. It's a useful simplification, and it's still true at *some* level, but I think it's also clear that it doesn't really describe most of reality. It might describe some particular case, though, and I do think it's a useful teaching tool. People obviously still do those traditional pipelines of processes and file descriptors that UNIX is perhaps associated with, but there's a *lot* of cases where you have big complex unified systems. And systemd is in no way the piece that breaks with old UNIX legacy. Graphical applications seldom worked that way (there are certainly _echoes_ of it in things like «LyX», but I think it's the exception rather than the rule), and then there's obviously the traditional counter-example of GNU emacs, where it really was not about the «simple UNIX model», but a whole new big infrastructure thing. Like systemd. Now, I'm still old-fashioned enough that I like my log-files in text, not binary, so I think sometimes systemd hasn't necessarily had the best of taste, but hey, details… UPD от 2017-02-18: ещё по поводу надуманных примеров на shell. Вы говорите, примеры надуманные, что можно сделать find -exec или xargs. Да, можно. Но как минимум сам факт того, что нужно постоянно держать в голове, что, мол, цикл нельзя и нужен -exec и xargs — это уже костыль. Проистекающий из принципа «всё есть текст», ну или из слишком тупой реализации этого принципа в UNIX shell. Итак, сейчас я приведу такую задачу, в которой любое решение будет уродским, даже с использованием find -exec и xargs. Вернёмся к моему примеру с touch'ем. «Как touch'нуть все файлы в папке foo (и во вложенных)?» Допустим, что нужно не touch'нуть их, а grep'нуть из них все строки со словом bar и положить результат туда же. Т. е. для каждого файла file сделать grep bar file > tmp; mv tmp file. Как быть? Если делать решение с циклом, то мы упираемся в те пять хаков, которые нужно сделать, чтобы не выстрелить себе в ногу. Результат будет таким, со всеми пятью хаками: find foo -print0 | while IFS=\"\" read -rd \"\" A; do grep -- bar \"$A\" > tmp mv -- tmp \"$A\" done Ладно, хорошо, мы знаем, что имя файла начинается на foo, а потому не может начинаться с дефиса и пробела. Но оно может заканчиваться на пробел, а потому тот трюк с IFS всё равно нужен. Так что единственный хак, от которого можно избавиться, зная, что имя начинается с foo — это написание --. Но даже от этого хака я бы не советовал избавляться, т. к. постоянное использование -- даёт понять читающему: «Да, я подумал об этом». Это как условия Йоды. Окей, можно ли этот пример написать проще с использованием xargs или find -exec? Если бы каждый файл нужно было всего лишь touch'нуть, то да, можно было бы написать существенно проще. Но если нужно выполнить два действия: grep и переименование, то существенного упрощения мы уже не получим. Два действия означают, что нам уже нужно запихивать эти два действия в вызов shell'а, в sh -c. Как будет выглядеть результат? Может быть, так? find foo -exec sh -c \"grep -- bar '{}' > tmp; mv -- tmp '{}'\" ';' Нет, неправильно! Это не будет работать, если имя содержит одинарную кавычку. Правильный вариант таков: find foo -exec sh -c 'grep -- bar \"$1\" > tmp; mv -- tmp \"$1\"' dummy '{}' ';' Видите? Опять хак. Нам пришлось передать имя файла через $1. И по-прежнему нужно помнить, что нам нужны двойные кавычки вокруг $1. То же самое было бы с xargs. Опять нужен sh -c и опять нужно передавать аргументы через $1. Всё это сделать можно, если надо, но сам факт того, что нужно постоянно держать это в голове и обходить грабли, говорит о том, что здесь что-то не то. Теперь по поводу другого примера. Где нужно удалить все файлы определённого размера. Да, всё это можно сделать одним вызовом find. Там есть опции и для проверки размера, и для удаления. Да. Вот только я вижу здесь хак. Хак в том, что find имеет фактически в себе целый sublanguage, подъязык. Язык вот этих вот опций. Почитайте хорошенько ман find'а. Вы узнаете, что, оказывается, порядок опций find'а имеет значение. Что каждая опция имеет truth value, т. е. булевское значение. Что можно по-хитрому комбинировать эти опции. Что в зависимости от порядка опции, от их truth value find принимает решение, в какой момент нужно остановить обработку опций для данного файла и нужно ли descend в данный каталог (т. е. нужно ли искать внутри этого этого каталога). Я помню, как однажды жутко оплошался, не зная этих тонкостей. Я набрал find -delete -name '*~' вместо find -name '*~' -delete или что-то такое. Ну подумаешь, думал я, опции не в том порядке. Смысл же тот же. И find удалил всё. Снёс мои важные файлы. Потом я восстановил из бекапа, так что всё ок. Это потом уже я понял, что -name имеет truth value true в случае, если файл соответствует маске. И если -name вернул true, то обработка опций продолжается. Что тут плохого? Плохо то, что find имеет свой sublanguage. Что это ещё один язык в дополнение к shell. (А sed, кстати говоря — это ещё один язык, а awk — это ещё один язык и так далее, авторы UNIX'а любили создавать по языку на каждый чих.) Нужно было вместо этого сделать так, чтобы find только умел искать файлы. А всю остальную функциональность нужно вынести из него. Проверки на размер файла должны быть снаружи. А если find'у нужно принять решение, нужно ли descend в данный каталог, то он должен вызывать внешний callback. Да, в UNIX shell так вряд ли получится. На то он и UNIX shell."], "hab": ["Разработка под Linux", "Open source", "C"]}{"url": "https://habrahabr.ru/post/322732/", "title": ["LED-it-GO — угроза безопасности данных"], "text": ["Блуждая по просторам интернета, наткнулся на любопытные исследования специалистов по компьютерной безопасности из израильского университета. Они создали вредоносную программу (Malware), назначение которой – передача данных на расположенную в зоне прямой видимости камеру с находящегося в офф-лайне компьютера. Цель состояла в моделировании хакерской атаки и возможности трансляции данных с зараженного компьютера с использованием светодиода активности жесткого диска. Технология, на мой взгляд, несколько голливудская, но, тем не менее, интересная. LED-it-GO. Использование световых сигналов для кражи данных Авторы назвали свое исследование «LED-it-GO: Leaking (a lot of) Data from Air-Gapped Computers via the (small) Hard Drive LED». Техническая сторона заключается в использовании светодиода, отражающего активность накопителя, установленного в компьютер, в качестве источника данных, которые он передает своим миганием. Он загорается при каждой операции чтения или записи, совершаемой накопителем. Таких светодиодов может быть несколько, например, по одному на каждый диск в сервере или сетевом хранилище. Десктопные компьютеры используют один общий источник светового сигнала на все накопители. Как правило, он расположен на лицевой панели, а у ноутбуков – на верхней панели рядом с клавиатурой или на передней грани. Сигнал генерируется включением и выключением светодиода. Управляет им материнская плата, и не было найдено какого-либо способа непосредственного управления им. Поэтому, разработчикам пришлось управлять длительностью и частотой включения светодиода, обращаясь к диску для чтения или записи блоков нужной длины. Для того, чтобы не оставлять в системе каких-либо следов, использовалась только операция чтения. Информация передавалась пакетами двух видов: постоянной длины и переменной. Каждый пакет предваряется 8-битным заголовком. В случае фиксированной длины пакета следовали 256 битов данных, заканчивающихся контрольной суммой. При переменной длине пакета между заголовком и собственно данными передавалась длина блока данных. Пакеты фиксированной длины предпочтительнее при передачи небольшого количества данных, например, пароля, ключа шифрования и т. п. Пакеты с переменной длиной позволяют передавать целые файлы. Результаты теста Исследователи использовали разные виды камер и светодиоды разного цвета – красные, белые, синие. Максимальная скорость передачи, которую мог обеспечить светодиод своим свечением, составляла 4000 бит в секунду. При этом было выявлено, что светодиоды синего цвета обеспечивают наиболее сильный световой сигнал. Для считывания сигналов светодиода использовались разные камеры: GoPro, зеркальные фотокамеры начального уровня, вебкамеры, смартфоны, очки Google Glass. Опытным путем было выяснено, что большинство этих камер могут обеспечивать распознавание сигналов со скоростью до 15 бит в секунду. При этом GoPro Hero5 позволяла принимать сигнал со скоростью до 120 бит в секунду. В проведенном тесте считываемый сигнал с системного блока, установленного в помещении, считывался при помощи камеры, смонтированной на дроне, который через окно записывал генерируемую информацию. Результаты можно увидеть в ролике, представленном разработчиками. LED-it-GO — атака, которую трудно распознать Разработчики заявляют, что, т. к. индикатор активности диска мигает часто и довольно хаотично, добавление к нему дополнительных миганий вряд ли будет заметным. При этом во время передачи данных светодиод мигает так часто, что для человеческого глаза кажется, что он горит постоянно. Сложность обнаружения такой атаки состоит в том, что в компьютере всегда мигает какой-либо светодиод, или даже несколько, и на это, как правило, никто не обращает никакого внимания. При скорости передачи до 4000 бит в секунду даже передача больших файлов, в принципе, становится возможной. Цель – компьютер, не подключенный к сети интернет Если компьютер подключен к всемирной сети, то гораздо проще использовать это подключение для кражи данных, нежели заморачиваться со светодиодами и камерами. Другое дело компьютеры, хранящие важные данные и не имеющие такого подключения. Правда, тут возникают 2 проблемы: • Обычно такие компьютеры устанавливаются в помещениях без окон, без размещения иных электронных устройств поблизости. • Необходимо каким-то образом инфицировать такой компьютер вирусом (Malware). Сделать обычным способом (через вложенный в почтовое сообщение файл, скачанный файл и т. п.) не представляется возможным. Если в помещении есть окна, то защититься от такого способа кражи информации можно, если заклеить или отключить светодиоды, установить систему мониторинга активности светодиодов, или сделать окна визуально непроницаемыми при взгляде снаружи в помещение, где установлены компьютеры. Также не следует использовать камеры видеонаблюдения, которые могут быть использованы для считывания информации таким образом. → Источник"], "hab": ["Антивирусная защита"]}{"url": "https://habrahabr.ru/post/277717/", "title": ["LLVM: компилятор своими руками. Введение"], "text": ["Представим себе, что в один прекрасный день вам пришла в голову идея процессора собственной, ни на что не похожей архитектуры, и вам очень захотелось эту идею реализовать «в железе». К счастью, в этом нет ничего невозможного. Немного верилога, и вот ваша идея реализована. Вам уже снятся прекрасные сны про то, как Intel разорилась, Microsoft спешно переписывает Windows под вашу архитектуру, а Linux-сообщество уже написало под ваш микропроцессор свежую версию системы с весьма нескучными обоями. Однако, для всего этого не хватает одной мелочи: компилятора! Да, я знаю, что многие не считают наличие компилятора чем-то важным, считая, что все должны программировать строго на ассемблере. Если вы тоже так считаете, я не буду с вами спорить, просто не читайте дальше. Если вы хотите, чтобы для вашей оригинальной архитектуры был доступен хотя бы язык С, прошу под кат. В статье будет рассматриваться применение инфраструктуры компиляторов LLVM для построения собственных решений на её основе. Область применения LLVM не ограничивается разработкой компиляторов для новых процессоров, инфраструктура компиляторов LLVM также может применяться для разработки компиляторов новых языков программирования, новых алгоритмов оптимизации и специфических инструментов статического анализа программного кода (поиск ошибок, сбор статистики и т.п.). Например, вы можете использовать какой-то стандартный процессор (например, ARM) в сочетании с специализированным сопроцессором (например, матричный FPU), в этом случае вам может понадобиться модифицировать существующий компилятор для ARM так, чтобы он мог генерировать код для вашего FPU. Также интересным применением LLVM может быть генерация исходных текстов на языке высокого уровня («перевод» с одного языка на другой). Например, можно написать генератор кода на Verilog по исходному коду на С. КДПВ Почему LLVM? На сегодняшний день существует только два реалистичных пути разработки компилятора для собственной архитектуры: использование GCC либо использование LLVM. Другие проекты компиляторов с открытым исходным кодом либо не достигли той степени развития, как GCC и LLVM, либо устарели и перестали развиваться, они не обладают развитыми алгоритмами оптимизации, и могут не обеспечивать полной совместимости даже со стандартом языка С, не говоря уже о поддержке других языков программирования. Разработка собственного компилятора “с нуля\", это весьма нерациональный путь, т.к. существующие опенсорсные решения уже реализуют фронтенд компилятора с множеством весьма нетривиальных алгоритмов оптимизации, которые, к тому же, хорошо протестированы и используются уже длительное время. Какой из этих двух open-source проектов выбрать в качестве основы для своего компилятора? GCC (GNU Compiler Collection) является более старым проектом, первый релиз которого состоялся в 1987 году, его автором является Ричард Столлман, известный деятель open-source движения [4]. Он поддерживает множество языков программирования: C, C++, Objective C, Fortran, Java, Ada, Go. Также существуют фронтенды для многих других языков программирования, не включенных в основную сборку. Компилятор GCC поддерживает большое количество процессорных архитектур и операционных систем, и является в настоящее время наиболее распространённым компилятором. Сам GCC написан на языке С (в комментариях меня поправили, что он уже по большей части переписан на С++). LLVM гораздо «моложе», его первый релиз состоялся в 2003 году, он (а точнее, его фронтенд Clang) поддерживает языки программирования C, C++, Objective-C and Objective-C++, и также имеет фронтенды для языков Common Lisp, ActionScript, Ada, D, Fortran, OpenGL Shading Language, Go, Haskell, Java bytecode, Julia, Swift, Python, Ruby, Rust, Scala, C# и Lua. Он разработан в университете Иллинойса, в США, и является основным компилятором для разработки под операционную систему OS X. LLVM написан на языке С++ (С++11 для последних релизов) [5]. Относительная «молодость» LLVM не является недостатком, он достаточно зрелый, чтобы в нём не было критических багов, и при этом он не несёт в себе огромного груза устаревших архитектурных решений, как GCC. Модульная структура компилятора позволяет использовать фронтенд LLVM-GCC, который обеспечивает полную поддержку стандартов GCC, при этом генерация кода целевой платформы будет осуществляться LLC (бэкенд LLVM). Также можно использовать Clang — оригинальный фронтенд LLVM. LLVM хорошо документирован, и для него большое количество примеров кода. Модульная архитектура компилятора Инфраструктура компиляторов LLVM состоит из различных инструментов, рассматривать их все в рамках данной статьи не имеет смысла. Ограничимся основными инструментами, которые образуют как таковой компилятор. Компилятор LLVM, как и некоторые другие компиляторы, состоит из фронтенда, оптимизатора (миддленда), и бэкенда. Такая структура позволяет разделить разработку компилятора нового языка программирования, разработку методов оптимизации и разработку кодогенератора для конкретного процессора (такие компиляторы называют «перенацеливаемыми», retargetable). Связующим звеном между ними является промежуточный язык LLVM, ассемблер «виртуальной машины». Фронтенд (например, Clang) преобразует текст программы на языке высокого уровня в текст на промежуточном языке, оптимизатор (opt) производит над ним различные оптимизации, а бэкенд (llc) генерирует код целевого процессора (на ассемблере или непосредственно в виде бинарного файла). Помимо этого, LLVM может работать в режиме JIT (just-in-time) компиляции, когда компиляция происходит непосредственно во время исполнения программы. Промежуточное представление программы может быть как в виде текстового файла на языке ассемблера LLVM, так и в виде двоичного формата, «биткода». По умолчанию clang генерирует именно биткод (файл .bc), но для отладки и изучения LLVM нам нужно будет генерировать текстовое промежуточное представление на ассемблере LLVM (он также называется IR-кодом, от слов Intermediate Representation, промежуточное представление). Рис. 1. Модульная архитектура компилятора Кроме вышеперечисленных программ, LLVM включает в себя и другие утилиты [6]. Итак, напишем простейшую программу на C. int foo(int x, int y) { return x + y; } И скомпилируем её: clang-3.5 -c add.c -O0 --target=xcore -emit-llvm -S -o add_o0.ll Некоторые пояснения: -c add.c — входной файл; -O0 — уровень оптимизации 0, оптимизация отсутствует; --target=xcore — ядро процессора xcore не имеет каких-либо сложных особенностей при компиляции в IR-код, поэтому является идеальным объектом для исследований. Это ядро имеет разрядность 32, и clang выравнивает все переменные по границам 32-битных слов; -emit-llvm -S — указание clang-у сгенерировать файл llvm в текстовом виде (на ассемблере LLVM); -o add_o0.ll — выходной файл Посмотрим на результат: ; ModuleID = 'add.c' target datalayout = \"e-m:e-p:32:32-i1:8:32-i8:8:32-i16:16:32-i64:32-f64:32-a:0:32-n32\" target triple = \"xcore\" ; Function Attrs: nounwind define i32 @foo(i32 %x, i32 %y) #0 { %1 = alloca i32, align 4 %2 = alloca i32, align 4 store i32 %x, i32* %1, align 4 store i32 %y, i32* %2, align 4 %3 = load i32* %1, align 4 %4 = load i32* %2, align 4 %5 = add nsw i32 %3, %4 ret i32 %5 } attributes #0 = { nounwind \"less-precise-fpmad\"=\"false\" \"no-frame-pointer-elim\"=\"false\" \"no-infs-fp-math\"=\"false\" \"no-nans-fp-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" } !llvm.ident = !{!0} !xcore.typestrings = !{!1} !0 = metadata !{metadata !\"Ubuntu clang version 3.5.0-4ubuntu2~trusty2 (tags/RELEASE_350/final) (based on LLVM 3.5.0)\"} !1 = metadata !{i32 (i32, i32)* @foo, metadata !\"f{si}(si,si)\"} Выглядит довольно сложно, не так ли? Однако давайте разберёмся, что тут написано. Итак: target datalayout = «e-m:e-p:32:32-i1:8:32-i8:8:32-i16:16:32-i64:32-f64:32-a:0:32-n32» описание разрядности переменных и самых основных особенностей архитектуры. e- little-endian архитектура. Для big-endian здесь была бы буква E. m:e — mangling, соглашение о преобразовании имён. Нам пока это не понадобится. p:32:32 — указатели имеют 32 бита и выровнены по границам 32-битных слов. i1:8:32 — булевые переменные (i1) выражаются 8-ибитными значениями и выровнены по границам 32-битных слов. Далее аналогично для целочисленных переменных i8 — i64 (от 8 до 64 бит соответственно), и f64 — для вещественных переменных двойной точности. a:0:32 — агрегатные переменные (т.е. массивы и структуры) имеют выравнивание 32 бита, n32 — разрядность чисел, обрабатываемых АЛУ процессора (native integer width). Далее следует название таргета (т.е. целевого процессора): target triple = «xcore». Хотя код IR часто называют «машинно-независимым», на самом деле мы видим, это не так. Он отражает некоторые особенности целевой архитектуры. Это является одной из причин, по которой мы должны указывать целевую архитектуру не только для бэкенда, но и для фронтенда. Далее следует код функции foo: define i32 @foo(i32 %x, i32 %y) #0 { %1 = alloca i32, align 4 %2 = alloca i32, align 4 store i32 %x, i32* %1, align 4 store i32 %y, i32* %2, align 4 %3 = load i32* %1, align 4 %4 = load i32* %2, align 4 %5 = add nsw i32 %3, %4 ret i32 %5 } Код довольно громоздкий, хотя исходная функция очень проста. Вот что он делает. Сразу отметим, что все имена переменных в LLVM имеют префикс либо % (для локальных переменных), либо @ — для глобальных. В данном примере все переменные локальные. %1 = alloca i32, align 4 — выделяет на стеке 4 байта для переменной, указателем на эту область является указатель %1. store i32 %x, i32* %1, align 4 — копирует в выделенную область один из аргументов функции (%x). %3 = load i32* %1, align 4 — извлекает значение в переменную %3. Теперь в %3 хранится локальная копия %x. Делает то же самое для аргумента %y %5 = add nsw i32 %3, %4 — складывает локальные копии %x и %y, помещает результат в %5. Есть ещё атрибут nsw, но он пока для нас не важен. Возвращает значение %5. Из приведённого примера видно, что при нулевом уровне оптимизации clang генерирует код, буквально следуя исходному коду, создаёт локальные копии всех аргументов и не делает каких-либо попыток удалить избыточные команды. Это может показаться плохим свойством компилятора, но на самом деле это очень полезная особенность при отладке программ и при отладке кода самого компилятора. Посмотрим, что произойдёт, если поменять уровень оптимизации на O1: define i32 @foo(i32 %x, i32 %y) #0 { %1 = add nsw i32 %y, %x ret i32 %1 } Мы видим, что ни одной лишней команды не осталось. Теперь программа складывает непосредственно аргументы функции и возвращает результат. Есть и более высокие уровни оптимизации, но в этом конкретном случае они не дадут лучшего результата, т.к. максимальный уровень оптимизации уже достигнут. Остальная часть LLVM-кода (атрибуты, метаданные), несёт в себе служебную информацию, которая пока нам неинтересна. Структура кода LLVM Структура кода LLVM очень проста. Код программы состоит из модулей, компилятор обрабатывает по одному модулю за раз. В модуле есть глобальные объявления (переменные, константы и объявления заголовков функций, находящихся в других модулях) и функции. Функции имеют аргументы и возвращаемые типы. Функции состоят из базовых блоков. Базовый блок — это последовательность команд ассемблера LLVM, имеющая одну точку входа и одну точку выхода. Базовый блок не содержит внутри себя никаких ветвлений и циклов, он выполняется строго последовательно от начала до конца и должен заканчиваться терминирующей командой (возвратом из функции или переходом на другой блок). И, наконец, базовый блок состоит из команд ассемблера. Список команд приведён в документации на LLVM [7]. Итак, ещё раз: базовый блок имеет одну точку входа, помеченную меткой, и обязательно должен заканчиваться командой безусловного перехода br или командой возврата ret. Перед ними может быть команда условного перехода, в этом случае она должна быть непосредственно перед терминирующей командой. Базовый блок имеет список predecessors — базовых блоков, из которых управление может приходить на него, и successors — базовых блоков, которым он может передавать управление. На основе этой информации строится CFG — Control Flow Graph, граф потока управления, важнейшая структура, представляющая программу в компиляторе. Рассмотрим тестовый пример на языке С: Пусть исходный код на языке С имеет цикл: //функция вычисляет сумму 10 элементов массива int for_loop(int x[]) { int sum = 0; for(int i = 0; i < 10; ++i) { sum += x[i]; } return sum; } Его CFG имеет вид: Ещё одним видом графов в LLVM является DAG — directed acyclic graph, направленный ациклический граф, представляющий собой базовый блок. Он представляет команды ассемблера и зависимости между ними. На рисунке ниже приведён DAG базового блока, представляющий тело цикла в примере выше, при уровне оптимизации -O1: SSA-форма Ключевой особенностью IR-кода, отличающей его от языков высокого уровня является то, что он представлен в так называемой SSA-форме (Static Single Assignment form). Эта особенность очень важна для понимания при разработке компилятора на платформе LLVM, поэтому уделим ей некоторое внимание. Если формулировать кратко, то в SSA-форме каждой переменной значение присваивается ровно один раз и только в одной точке программы. Все алгоритмы оптимизации и преобразования IR-кода работают только с такой формой. Однако, как преобразовать обычную программу на языке высокого уровня в такую форму? Ведь в обычных языках программирования значение переменной может присваиваться несколько раз в разных местах программы, или, например, в цикле. Для реализации такого поведения программы используется один из двух приемов. Первый прием заключается в использовании пар команд load/store, как в вышеприведённом коде. Ограничение единственного присваивания распространяется только на именованные переменные LLVM, и не распространяется на ячейки памяти, на которые ссылаются указатели. То есть, можно неограниченное количество раз производить запись в ячейку памяти командой store, и формальное правило SSA будет соблюдено, так как указатель на эту ячейку не меняется. Этот способ используется обычно при уровне оптимизации -O0, и мы не будем на нём подробно останавливаться. Второй прием использует φ-функции, ещё одну интересную концепцию IR-кода. Код в SSA-форме: load/store Воспользуемся тестовым примером из предыдущего раздела. Скомпилируем с уровнем оптимизации -O0: define i32 @for_loop(i32* %x) #0 { %1 = alloca i32*, align 4 %sum = alloca i32, align 4 %i = alloca i32, align 4 store i32* %x, i32** %1, align 4 store i32 0, i32* %sum, align 4 store i32 0, i32* %i, align 4 br label %2 ; :2 ; preds = %12, %0 %3 = load i32* %i, align 4 %4 = icmp slt i32 %3, 10 br i1 %4, label %5, label %15 ; :5 ; preds = %2 %6 = load i32* %i, align 4 %7 = load i32** %1, align 4 %8 = getelementptr inbounds i32* %7, i32 %6 %9 = load i32* %8, align 4 %10 = load i32* %sum, align 4 %11 = add nsw i32 %10, %9 store i32 %11, i32* %sum, align 4 br label %12 ; :12 ; preds = %5 %13 = load i32* %i, align 4 %14 = add nsw i32 %13, 1 store i32 %14, i32* %i, align 4 br label %2 ; :15 ; preds = %2 %16 = load i32* %sum, align 4 ret i32 %16 } Здесь мы видим то, о чем было написано выше: переменная цикла, это просто ячейка памяти, на которую ссылается указатель %i. Код в SSA-форме: φ-функции Теперь попробуем уровень оптимизации O1: define i32 @for_loop(i32* nocapture readonly %x) #0 { br label %1 ; :1 ; preds = %1, %0 %i.02 = phi i32 [ 0, %0 ], [ %5, %1 ] %sum.01 = phi i32 [ 0, %0 ], [ %4, %1 ] %2 = getelementptr inbounds i32* %x, i32 %i.02 %3 = load i32* %2, align 4, !tbaa !2 %4 = add nsw i32 %3, %sum.01 %5 = add nsw i32 %i.02, 1 %exitcond = icmp eq i32 %5, 10 br i1 %exitcond, label %6, label %1 ; :6 ; preds = %1 ret i32 %4 } Здесь мы видим, что переменной цикла является %i.02 (имена переменных в LLVM могут содержать точки), и это не указатель, а обычная 32-битная переменная, а присваивание значения происходит с помощью функции phi i32 [ 0, %0 ], [ %5, %1 ]. Это означает, что функция примет значение 0, если переход произошёл с базового блока %0 (первый базовый блок в функции), и значение переменной %5, если переход произошёл с базового блока %1 (т.е. с выходной точки этого же базового блока). Таким образом, генератор IR-кода избавился от двух присваиваний переменной, строго следуя формальным правилам SSA. Далее видно, что сходным образом происходит присваивание переменной %sum.01. Итак, смысл φ-функции состоит в том, что её значение зависит от того, из какого базового блока был произведён переход на неё. φ-функции могут находиться только в начале базового блока. Если их несколько, они должны следовать непрерывно, начиная с первой инструкции базового блока. Moar optimization! Что будет при уровне оптимизации -O2? define i32 @for_loop(i32* nocapture readonly %x) #0 { %1 = load i32* %x, align 4, !tbaa !2 %2 = getelementptr inbounds i32* %x, i32 1 %3 = load i32* %2, align 4, !tbaa !2 %4 = add nsw i32 %3, %1 %5 = getelementptr inbounds i32* %x, i32 2 %6 = load i32* %5, align 4, !tbaa !2 %7 = add nsw i32 %6, %4 %8 = getelementptr inbounds i32* %x, i32 3 %9 = load i32* %8, align 4, !tbaa !2 %10 = add nsw i32 %9, %7 %11 = getelementptr inbounds i32* %x, i32 4 %12 = load i32* %11, align 4, !tbaa !2 %13 = add nsw i32 %12, %10 %14 = getelementptr inbounds i32* %x, i32 5 %15 = load i32* %14, align 4, !tbaa !2 %16 = add nsw i32 %15, %13 %17 = getelementptr inbounds i32* %x, i32 6 %18 = load i32* %17, align 4, !tbaa !2 %19 = add nsw i32 %18, %16 %20 = getelementptr inbounds i32* %x, i32 7 %21 = load i32* %20, align 4, !tbaa !2 %22 = add nsw i32 %21, %19 %23 = getelementptr inbounds i32* %x, i32 8 %24 = load i32* %23, align 4, !tbaa !2 %25 = add nsw i32 %24, %22 %26 = getelementptr inbounds i32* %x, i32 9 %27 = load i32* %26, align 4, !tbaa !2 %28 = add nsw i32 %27, %25 ret i32 %28 } Оптимизатор развернул цикл. Вообще, оптимизатор IR-кода в LLVM очень интеллектуален, он умеет не только разворачивать циклы, но и упрощать нетривиальные конструкции, вычислять константные значения, даже если они не присутствуют в коде в явном виде, и делать другие сложные преобразования кода. Компоновка IR-кода Реальные программы состоят не из одного модуля. Традиционный компилятор компилирует модули по отдельности, превращая их в объектные файлы, а затем передаёт их компоновщику (линкеру), который объединяет их в один исполняемый файл. LLVM тоже позволяет так делать. Но LLVM имеет также возможность компоновки IR-кода. Проще всего рассмотреть это на примере. Пусть есть два исходных модуля: foo.c и bar.c: //bar.h #ifndef BAR_H #define BAR_H int bar(int x, int k); #endif //bar.c int bar(int x, int k) { return x * x * k; } //foo.c #include \"bar.h\" int foo(int x, int y) { return bar(x, 2) + bar(y, 3); } Если программа будет скомпилирована «традиционным» образом, то оптимизатор не сможет сделать с ней практически ничего: при компиляции foo.c компилятор не знает, что находится внутри функции bar, и может поступить единственным очевидным способом, вставить вызовы bar(). Но если мы скомпонуем IR-код, то мы получим один модуль, который после оптимизации с уровнем -O2 будет выглядеть так (для ясности заголовок модуля и метаданные опущены): define i32 @foo(i32 %x, i32 %y) #0 { %1 = shl i32 %x, 1 %2 = mul i32 %1, %x %3 = mul i32 %y, 3 %4 = mul i32 %3, %y %5 = add nsw i32 %4, %2 ret i32 %5 } ; Function Attrs: nounwind readnone define i32 @bar(i32 %x, i32 %k) #0 { %1 = mul nsw i32 %x, %x %2 = mul nsw i32 %1, %k ret i32 %2 } Здесь видно, что в функции foo не происходит никаких вызовов, компилятор перенёс в неё содержимое bar() полностью, попутно подставив константные значения k. Хотя функция bar() осталась в модуле, она будет исключена при компиляции исполняемого файла, при условии, что она не вызывается нигде больше в программе. Нужно отметить, что в GCC также есть возможность компоновки и оптимизации промежуточного кода (LTO, link-time optimization) [6]. Разумеется, оптимизация в LLVM не исчерпывается оптимизацией IR-кода. Внутри бэкенда также происходят различные оптимизации на разных стадиях преобразования IR-кода в машинное представление. Часть таких оптимизаций LLVM производит самостоятельно, но разработчик бэкенда может (и должен) разработать собственные алгоритмы оптимизации, которые позволят в полной мере использовать особенности архитектуры процессора. Генерация кода целевой платформы Разработка компилятора для оригинальной архитектуры процессора, это, в основном, разработка бэкенда. Вмешательство в алгоритмы фронтенда, как правило, не является необходимым, или, во всяком случае, требует весьма веских оснований. Если проанализировать исходный код Clang, можно увидеть, что большая часть «особенных» алгоритмов приходится на процессоры x86 и PowerPC с их нестандартными форматами вещественных чисел. Для большинства других процессоров нужно указать только размеры базовых типов и endianness (big-endian или little-endian). Чаще всего можно просто найти аналогичный (в плане разрядности) процессор среди множества поддерживаемых. Генерация кода для целевой платформы происходит в бэкенде LLVM, LLC. LLC поддерживает множество различных процессоров, и вы можете на его основе сделать генератор кода для вашего собственного оригинального процессора. Эта задача упрощается ещё и благодаря тому, что весь исходный код, включая модули для каждой поддерживаемой архитектуры, полностью открыт и доступен для изучения. Именно генератор кода для целевой платформы (таргета) является наиболее трудоёмкой задачей при разработке компилятора на основе инфрастуктуры LLVM. Я решил не останавливаться подробно здесь на особенностях реализации бэкенда, так как они существенным образом зависят от архитектуры целевого процессора. Впрочем, если у уважаемой аудитории хабра возникнет интерес к данной теме, я готов описать ключевые моменты разработки бэкенда в следующей статье. Заключение В рамках небольшой статьи нельзя рассмотреть подробно ни архитектуру LLVM, ни синтаксис языка LLVM IR, ни процесс разработки бэкенда. Однако эти вопросы подробно освещаются в документации. Автор скорее старался дать общее представление об инфраструктуре компиляторов LLVM, сделав упор на то, что эта платформа является современной, мощной, универсальной, и независимой ни от входного языка, ни от целевой архитектуры процессора, позволяя реализовать и то, и другое по желанию разработчика. Помимо рассмотренных, LLVM содержит и другие утилиты, однако их рассмотрение выходит за рамки статьи. LLVM позволяет реализовать бэкенд для любой архитектуры (см. примечание), включая архитектуры с конвейеризацией, с внеочередным выполнением команд, с различными вариантами параллелизации, VLIW, для классических и стековых архитектур, в общем, для любых вариантов. Вне зависимости от того, насколько нестандартные решения лежат в основе процессорной архитектуры, это всего лишь вопрос написания большего или меньшего объёма кода. примечаниедля любой, в пределах разумного. Вряд ли возможно реализовать компилятор языка С для 4-хбитной архитектуры, т.к. стандарт языка явно требует, чтобы разрядность была не меньше 8. Литература Компиляторы [1] Книга дракона [2] Вирт Н. Построение компиляторов GCC [3] gcc.gnu.org — сайт проекта GCC [4] Richard M. Stallman and the GCC Developer Community. GNU Compiler Collection Internals LLVM [5] http://llvm.org/ — сайт проекта LLVM [6] http://llvm.org/docs/GettingStarted.html Getting Started with the LLVM System [7] http://llvm.org/docs/LangRef.html LLVM Language Reference Manual [8] http://llvm.org/docs/WritingAnLLVMBackend.html Writing An LLVM Backend [9] http://llvm.org/docs/WritingAnLLVMPass.html Writing An LLVM Pass [10] Chen Chung-Shu. Creating an LLVM Backend for the Cpu0 Architecture [11] Mayur Pandey, Suyog Sarda. LLVM Cookbook [12] Bruno Cardoso Lopes. Getting Started with LLVM Core Libraries [13] Suyog Sarda, Mayur Pandey. LLVM Essentials Автор будет рад ответить на ваши вопросы в комментариях и в личке. Просьба обо всех замеченных опечатках сообщать в личку. Заранее спасибо."], "hab": ["Компиляторы", "Open source"]}{"url": "https://habrahabr.ru/post/189046/", "title": ["GitHub Flow: рабочий процесс Гитхаба", "перевод"], "text": ["Краткое предисловие переводчика.Захватывающе интересная статья одного из разработчиков «GitHub Inc.» о принятом в компании рабочем процессе потребовала употребить пару специальных терминов при переводе. То понятие, для которого на английском языке достаточно одного слóва «workflow», на русский приходится переводить словосочетанием — «рабочий процесс». Ничего лучше не знаю ни сам я, ни при помощи гуглоперевода — так что и мне, и читателям придётся с этим мириться, хотя бы и поневоле. Другое понятие, «deploy», на русский часто переводят словом «развёртывание», но в моём переводе я решил вспомнить оборот из советского делопроизводства — «внедрение инноваций на производстве» — и стану говорить именно о «внедрении» новых фич. Дело в том, что описанный ниже рабочий процесс не имеет «выпусков» (releases), что делает несколько неудобными и речи о каком-либо «развёртывании» их. К сожалению, некоторые переводчики бывают склонны грубо убивать сочную метафору «иньекции» (или даже «впрыскивания», если угодно), содержающуюся в термине «code injection», так что и его также переводят словосочетанием «внедрение кода». Эта путаница огорчает меня, но ничего не могу поделать. Просто имейте в виду, что здесь «внедрением кода» я стану назвать внедрение его именно в производство (на продакшен), а не в чей-нибудь чужой код. Я стремился употреблять словосочетание «в Гитхабе» в значении «в компании GitHub Inc.», а «на Гитхабе» — в значении «на сайте GitHub.com». Правда, иногда разделять их сложновато. Проблемы git-flow Повсюду путешествую, преподавая Git людям — и почти на каждом уроке и семинаре, недавно мною проведённом, меня спрашивали, что я думаю о git-flow. Я всегда отвечал, что думаю, что этот подход великолепен — он взял систему (Git), для которой могут существовать мириады возможных рабочих процессов, и задокументировал один проверенный и гибкий процесс, который для многих разработчиков годится при довольно простом употреблении. Подход этот также становится чем-то вроде стандарта, так что разработчики могут переходить от проекта к проекту и из компании в компанию, оставаясь знакомыми с этим стандартизированным рабочим процессом. Однако и у git-flow есть проблемы. Я не раз слыхал мнения людей, выражавших неприязнь к тому, что ветви фич отходят от develop вместо master, или к манере обращения с хотфиксами, но эти проблемы сравнительно невелики. Для меня одной из более крупных проблем git-flow стала его сложность — бóльшая, чем на самом деле требуется большинству разработчиков и рабочих групп. Его сложность ужé привела к появлению скрипта-помощника для поддержания рабочего процесса. Само по себе это круто, но проблема в том, что помощник работает не из GUI Git, а из командной строки, и получается, что те самые люди, которым необходимо действительно хорошо выучить сложный рабочий процесс, потому что им вручную придётся пройти все шаги его — для этих-то людей система и недостаточно удобна для того, чтобы использовать её из командной строки. Вот что становится крупною проблемою. Все эти проблемы можно без труда преодолеть, следуя гораздо более простому рабочему процессу. Мы не пользуемся git-flow в Гитхабе. Наш рабочий процесс основан (и всегда был основан) на более простом подходе к Git. Простота его имеет несколько достоинств. Во-первых, людям проще понять его, так что они быстрее начинают использовать его, реже (или вовсе никогда не) допускают ошибки, требующие отката. Кроме того, не требуется скрипт-обёртка, помогающий следовать процессу, так что употребление GUI (и т. п.) не создаёт проблем. Рабочий процесс Гитхаба Итак, почему мы в Гитхабе не используем git-flow? Главная проблема в том, что у нас принято беспрестанное внедрение изменений. Рабочий процесс git-flow создавался в основном в помощь «выпускам» нового кода. А у нас нет «выпусков», потому что код поступает на продакшен (основной рабочий сервер) ежедневно — иногда по нескольку раз в день. Мы можем подавать для этого команды боту в той же чат-комнате, в которой отображаются итоги CI (интеграционного тестирования). Мы стремимся сделать процесс тестирования кода и его внедрения как можно проще, чтобы каждому сотруднику он был удобен в работе. У такого частого внедрения новинок есть ряд достоинств. Если оно случается каждые несколько часов, то почти невозможно возникнуть большому количеству крупных багов. Небольшие недочёты случаются, но они могут быть исправлены (а исправления, в свою очередь, внедрены) очень быстро. Обычно пришлось бы делать «хотфикс» или как-то ещё отступать от нормального процесса, но для нас это становится просто частью нормального процесса: в рабочем процессе Гитхаба нет разницы между хотфиксом и небольшою фичею. Другим достоинством беспрерывного внедрения изменений становится возможность быстро отреагивать на проблемы любого рода. Мы можем откликаться на сообщения о проблемах с безопасностью или исполнять небольшие (но интересные) просьбы о новых фичах — но тот же самый процесс работает и при внесении изменений, связанных с разработкою фичи нормального (или даже крупного) размера. Процесс один и тот же и он очень прост. Как мы это делаем Итак, каков рабочий процесс Гитхаба? Содержимое ветви master всегда работоспособно (deployable).   Начиная работу над чем-то новым, ответвляйте от ветви master новую ветвь, имя которой соответствует её предназначению (например, «new-oauth2-scopes»).   Закоммитив в эту ветвь локально, отправляйте регулярно свою работу и в одноимённую ветвь на сервере.   Когда вам понадобится отзыв, или помощь, или когда вы сочтёте ветвь готовою ко слиянию, отправьте запрос на слияние.   После того, как кто-то другой просмотрел и одобрил фичу, вы можете слить вашу ветвь в ветвь master.   После того, как ветвь master пополнилась новым кодом, вы можете немедленно внедрить его на продакшен и вам следует сделать это. Вот и весь рабочий процесс. Он очень прост и результативен, он работает для довольно крупных рабочих групп — в Гитхабе сейчас работает 35 человек, из которых, может быть, пятнадцать или двадцать одновременно работают над одним и тем же проектом (github.com). Думаю, что большинство команд разработчиков (групп, одновременно работающих с логикою одного и того же кода, что может порождать конфликты) имеют такой же размер — или меньше такого. Особенно группы, достаточно прогрессивные для того, чтобы заниматься быстрым и последовательным внедрением. Итак, давайте по порядку рассмотрим каждый шаг. Содержимое ветви master всегда работоспособно (deployable) В общем-то это единственное жёсткое правило во всей системе. Есть только одна ветвь, имеющая всегда некоторое особенное значение, и её мы назвали master. Для нас это значит, что код этой ветви либо внедрён на продакшен, либо, в худшем случае, окажется внедрён в течение нескольких часов. Эта ветвь очень редко подвергается откручиванию на несколько коммитов назад (для отмены работы): если возникает проблема, то отменяются изменения из коммитов или совершенно новые коммиты исправляют проблему, но сама ветвь почти никогда назад не откручивается. Ветвь master стабильна. Внедрение её кода на продакшен или создание новых ветвей на её основе — это всегда, всегда безопасно. Если от вас в master поступает неоттестированный код или он ломает сборку, то вы нарушили «общественный договор» команды разработчиков и у вас на душé должны кошки поскрести по этому поводу. Каждая ветвь подвергается у нас тестированию, а итоги поступают в чат-комнату — так что, если вы не тестировали её локально, то можете запушить ветвь (даже с единственным коммитом) на сервер и подождать, пока Jenkins не сообщит, все ли тесты успешно пройдены. Ответвляйте от ветви master новые ветви, имена которых соответствуют предназначению Когда хотите поработать над чем-то новым, ответвляйте от стабильной ветви master новую ветвь, имя которой соответствует предназначению. (Например, в коде Гитхаба прямо сейчас есть ветви «user-content-cache-key», «submodules-init-task», «redis2-transition».) Такое наименование имеет несколько достоинств. Например, достаточно подать команду fetch, чтобы увидеть, над какими темами работают остальные. Кроме того, оставив ветвь на какое-то время и возвратившись к ней позднее, по имени проще припомнить, о чём она была. И это приятно, потому что, когда на Гитхабе заходим на страницу со списком ветвей, то легко видеть, над какими ветвями недавно поработали (и, приблизительно, каков был объём работы). Это почти как список будущих фич с грубою оценкою нынешнего состояния их. Если не пользуетесь этой страницею, то знайте — у ней классные возможности: она вам показывает только те ветви, в которых была проделана работа, уникальная по отношению к выбранной вами в настоящий момент ветви, да ещё и сортирует таким способом, чтобы ветви с наиболее недавнею работою были сверху. Если захочется полюбопытствовать, то я могу нажать на кнопку «Compare» и поглядеть на точный объединённый diff и на список коммитов, уникальных для этой ветви. Сейчас, когда я это пишу, у нас в репозитории 44 ветви с невоссоединённым кодом, но также видно, что из них только в девять или в десять поступал код за последнюю неделю. Постоянно отправляйте код именованных ветвей на сервер Другое крупное отличие от git-flow: мы беспрерывно делаем push ветвей на сервер. С точки зрения внедрения приходится по-настоящему беспокоиться только о ветви master, так что push никого не озадачит и ничего не поломает: всё, что не master — это просто код, над которым идёт работа. Этим создаётся страховочная копия на случай утраты ноутбука или выхода жёсткого диска из строя. Этим поддерживается, что ещё важнее, постоянный обмен сведениями между разработчиками. Простой командою «git fetch» можно получить список тех TODO, над которыми все сейчас работают. $ git fetch remote: Counting objects: 3032, done. remote: Compressing objects: 100% (947/947), done. remote: Total 2672 (delta 1993), reused 2328 (delta 1689) Receiving objects: 100% (2672/2672), 16.45 MiB | 1.04 MiB/s, done. Resolving deltas: 100% (1993/1993), completed with 213 local objects. From github.com:github/github * [new branch] charlock-linguist -> origin/charlock-linguist * [new branch] enterprise-non-config -> origin/enterprise-non-config * [new branch] fi-signup -> origin/fi-signup 2647a42..4d6d2c2 git-http-server -> origin/git-http-server * [new branch] knyle-style-commits -> origin/knyle-style-commits 157d2b0..d33e00d master -> origin/master * [new branch] menu-behavior-act-i -> origin/menu-behavior-act-i ea1c5e2..dfd315a no-inline-js-config -> origin/no-inline-js-config * [new branch] svg-tests -> origin/svg-tests 87bb870..9da23f3 view-modes -> origin/view-modes * [new branch] wild-renaming -> origin/wild-renaming Также это позволяет всем видеть (на гитхабовской странице списка ветвей), над чем работают все остальные — можно проанализировать код и решить, не желаешь ли в чём-нибудь помочь разработчику. В любое время создавайте запрос на слияние GitHub снабжён поразительною системою обзора кода, которая называется запросами на слияние; боюсь, недостаточно разработчиков вполне знают о ней. Многие пользуются ею в обыкновенной работе над открытым исходным кодом: форкнул проект, обновил код, отправил запрос на слияние к хозяину проекта. Однако эта система также может употребляться как средство внутрикорпоративной проверки кода, и так её используем мы. Её мы, собственно, скорее используем как средство просмотра и обсуждения ветвей, чем как запрос на слияние. GitHub поддерживает отсылку запроса на слияние из одной ветви в другую в одном и том же проекте (открытом или приватном), так что в запросе можно сказать «мне нужна подсказка или обзор этого кода», а не только «прошу принять этот код». На этой иллюстрации вы можете видеть, как Джош просит Брайана взглянуть на код, и тот является с советом по поводу одной из строк кода. Ниже можно видеть, как Джош соглашается с соображениями Брайана и пополняет код, чтобы отреагировать на них. Можно, наконец, видеть и то, что код находится ещё на стадии испытаний: это ещё не ветвь, подготовленная ко внедрению, мы используем запросы на слияние для рассмотрения кода задолго до того, как захотим действительно слить его в master и отправить на внедрение. Если ваша работа над фичею или ветвью застревает и нужна помощь или совет, или если вы — разработчик, а на вашу работу надо бы посмотреть и дизайнеру (или наоборот), или даже если кода у вас мало (или вовсе нет), но есть какая-то композиция скриншотов и общих идей, то вы открываете запрос на слияние. Система Гитхаба позволяет добавлять людей к обсуждению @-упоминанием их, так что если обзор или отклик нужен от конкретного человека, то можно в запросе упомянуть его (вы видели выше, как Джош сделал это). И это круто, потому что в запросах на слияние можно комментировать отдельные строки объединённого диффа, или отдельные коммиты, или весь запрос в целом — и копии реплик сложатся в единое обсуждение. Также можно продолжать пополнение ветви кодом, так что если кто-нибудь укажет на ошибку или на позабытую возможность в коде, то можно поместить исправление в ту же ветвь, и GitHub покажет новые коммиты в обсуждении, так что и вот так можно трудиться над ветвью. Если ветвь существует слишком долго и вы ощущаете, что код в ней рассогласовывается с кодом ветви master, то можно код из master влить в вашу ветвь и продолжить работу. В обсуждении запроса на слияние или в списке коммитов без труда видно, когда ветвь последний раз обновляли кодом, взятым из master. Когда работа над ветвью целиком и полностью окончена и вы ощущаете её готовою ко внедрению, тогда можете переходить к следующему шагу. Слияние только после обзора запроса Мы не работаем непосредственно в ветви master, но и работу из именованной ветви мы не подвергаем слиянию сразу после того, как сочтём её оконченною — сперва мы стараемся получить одобрение от других сотрудников компании. Оно обычно имеет форму «+1», или эмоджи, или комментария «:shipit:», но кого-то ещё нам надо привести поглядеть на ветвь. Когда одобрение получено, и ветвь прошла CI, мы можем слить её в master и на внедрение; в этот момент запрос на слияние будет закрыт автоматически. Внедрение непосредственно после обзора Наконец, ваш труд окончен, а плоды его — на ветви master. Это означает, что, даже если прямо сейчас вы не станете внедрять их, то они всё равно станут основою для ветвей других сотрудников, и что следующее внедрение (которое, скорее всего, случится через несколько часов) запустит новинку в дело. И так как бывает очень неприятно обнаружить, что кто-то ещё запустил ваш код и пострадал от этого (если код что-то поломал), то людям свойственно самим заботливо проверять стабильность итогов совершённого ими слияния, самостоятельно внедрять результаты. Наш campfire-бот, по имени hubot, может внедрять код по указанию от любого из сотрудников. Достаточно подать в чате команду hubot deploy github to production, и код поступит на продакшен, где начнётся перезапуск (с нулевым даунтаймом) всех необходимых процессов. Можете сами судить о том, как часто это случается на Гитхабе: Как видите, шесть разных человек (среди которых один саппорт и один дизайнер) внедряли код более двух дюжин раз за сутки. Всё вышеописанное я совершал для ветвей с одним коммитом, содержащим однострочное изменение. Процесс прост, бесхитростен, масштабируем и силён. То же самое можно делать и с ветвью фичи, содержащей полсотни коммитов, потребовавших двухнедельной работы, и с одним коммитом, изготовленным минут за десять. Процесс настолько прост и настолько не тяготит, что его необходимость не раздражает даже в однокоммитном случае, так что люди редко пропускают или обходят отдельные шаги его — разве что речь идёт об изменении настолько малом и незначительном, что это никакого значения не имеет. Наш рабочий процесс обладает и силою, и невероятною простотою. Думаю, многие согласятся, что GitHub — очень стабильная платформа, что на её проблемы мы реагируем быстро (если они вообще возникают), что новые фичи внедряются в быстром темпе. Там нет таких компромиссов в отношении качества или стабильности, которые могли бы увеличить скорость и простоту рабочего процесса или уменьшить число его шагов. Заключение Git сам по себе довольно сложен для понимания. Если его ещё и употреблять в рабочем процессе более сложном, чем это необходимо, то дело кончится ежедневным чрезмерным усилием рассудка. Я всегда буду отстаивать употребление простейшей из возможных систем, пригодных для работы вашей группы, и до тех пор, пока система эта не перестанет работать; только тогда добавляйте сложность, когда никак не удаётся избежать этого. Для тех рабочих групп, которым необходимо подготавливать официальные выпуски кода через продолжительные интервалы (от нескольких недель до нескольких месяцев между выпусками), и создавать хотфиксы, и ветви поддержки прежних версий, и совершать другие дела, необходимость которых вызывается такими нечастыми выпусками кода, имеет смысл git-flow, и я весьма рекомендовал бы его употребление. Для групп, труд которых строится вокруг доставки кода, которые ежедневно обновляют продакшен, беспрерывно тестируют и внедряют фичи, я рекомендовал бы более простой рабочий процесс — такой, как GitHub Flow."], "hab": ["GitHub", "Git"]}{"url": "https://habrahabr.ru/post/322706/", "title": ["Социальное ботоводство: кто, как и зачем использует ботов?"], "text": ["Хочу начать статью сразу с сути, чтобы было понятно о чем я хочу рассказать. Для этого мне нужно, чтобы вы ответили для себя на ряд вопросов честно: Откуда вы черпаете информацию? Как вы ее проверяете? Как информация влияет на ваше мнение или действия? Сколько времени вы проводите в социальных сетях? Лично я с удивлением для себя обнаружил, что 90% информации я получаю из интернета и где-то 30% от этого — из социальных сетей. Я отношусь к тому поколению, которое застало DialUP модемы и скорость подключения в 22 кб/с. В то время, чтобы “захостить” сайт и выкинуть какую-то статью нужны были знания, опыт и необходимо было вложить в это хоть чуть-чуть денег (на хостинг). Поэтому я знал, что какой-либо материал из интернета должен был представлять хоть какую-то ценность, а его автор должен был быть не совсем глупым человеком. Тогда не было новостей в стиле “ШОК”, SEO только зарождалось, а про SMM никто вообще не слышал. Новостям и информации из интернета можно было относительно доверять. Читая обзорную статью про “рейтинг” самых популярных для программистов книг, у меня не возникало сомнений в его объективности и я не задумывался, что автор может иметь выгоду от продажи книг по модели CPA или просто продвигать какой-то бренд. Все это прошло… Настала эра социальных сетей, лайков, репостов и тонн бессмысленной информации, подталкивающей вас к тому или иному выводу или действию. Я хочу рассказать и попытаться классифицировать социальных ботов, как инструмент воздействия на человека. Социальные боты в электронной коммерции. Боты-продажники Сегодня любой “топ” или “рейтинг”, как и любую информацию из интернета нужно подвергать сомнению и искать заинтересованные стороны. Выбор товара нужно осуществлять не по “отзывам”, опубликованным на сайте продавца, а по отзывам именитых экспертов. В этом свете первой и наиболее очевидной категорией ботов в социальных сетях являются боты-продажники, имеющие единственную цель — впарить вам товар или услугу. Как правило, управляют ими живые люди — продавцы, а наиболее простая и популярная схема их использования выглядит следующим образом. К примеру, вы зашли на сайт компании, которая продает услуги информационной безопасности, прочли пару статей и вышли. Но вот “фокус”, уже через пару минут после того, как вы закрыли сайт, к вам в друзьях в социальной сети ломится аккаунт той же компании, и в личных сообщениях вы видите что-то вроде “Вы посещали наш сайт, но ничего не заказали, могу ли я Вам чем-то помочь?”. Все давно знают, что подобные “фишки” реализованы через “кликджекинг” (статья на Хабр) и уже существуют целые сервисы по отслеживанию пользователей из социальных сетей, посетивших ваш сайт. Рекламировать я их не буду. Идентифицирующий признак: стилизация под сервис своего владельца. SEO боты, лайкатели, накрутчики и репостеры На мой взгляд, еще одна очевидная категория социальных ботов — боты, предназначением которых является накрутка базовых показателей на сторонних сайтах, внутри групп в социальных сетях и даже на сторонних сервисах голосований. Аккаунты для такого рода ботов легко купить через специальные магазины BuyAccs или на форумах вроде Antichat. Как правило, боты управляются простенькими программами (не сочтите за рекламу): Викинг Ботовод, Plusmo, Olike и прочий ширпотреб. Не имеют функций ответа на сообщения. Идентифицирующий признак: огромное количество групп (подписок) и репостов при практически полном отсутствии личных публикаций. CPA боты-многодневки Чуть более сложный тип социальных ботов, созданный для классической цели “впарить”, но уже по более сложной схеме. Эти боты имеют SMS активацию и автор дорожит ими, наполняя фотографиями и информацией на первом этапе в ручную или с использованием чуть более дорогих программ: Xrumer + Social Plagin, ZennoPoster или другие. Такие боты разделены под необходимые классы продвигаемых продуктов. К примеру, боты CPA, боты красивых девочек будут наполнены большим количеством сексуальных фотографий и фото в стиле ню, если бот предназначен для продвижения какого-либо сайта знакомств, сайта с интимными товарами или даже услугами индивидуалок. Бот будет подписан на большое количество тематических пабликов знакомств, флирта и даже групп с эротическим контентом. Такие боты постят в сообществах сообщения в стиле “ищу знакомств”, “мне так одиноко” и так далее, желая привлечь внимание к своему профилю. Такие же боты “красивые девочки” или “красивые мальчики” могут продвигать по этой же схеме все что угодно, к примеру “ставки на спорт” или “казино”, тогда автор “подпишет” такого бота на паблики по азартным играм и схема та же, что описана выше. Таких ботов можно легко перепрофилировать на любые CPA офферы, товары или услуги. Идентифицирующий признак: активная ссылка на сторонний сервис прямо в профиле или на стене пользователя с припиской “тут редко бываю, ищите меня (ссылка)”. От очевидно коммерчески выгодных ботов мы плавно переходим к “элите” ботоводства: Политические социальный боты-тролли и лидеры мнений Наиболее сложная категория социальных ботов — политические. Этих ботов обычно “взращивают” месяцами, а иногда и годами. Боты используются в интересах политических сил, правительств и представляют из себя “лидеров мнений” определенного социального слоя (политического электората). Тут можно встретить бота, замаскированного под простого таксиста, регулярно публикующего свое личное мнение вперемешку с личными фотографиями и публикациями. Бывают учительницы младших классов, которые высказываются о простых и всем знакомым проблемах среднего образования, апеллируя к самому важному, что есть в жизни у каждого — детям, а как результат и их родителям. Такие боты имеют большое количество друзей (подписчиков), их посты “разгоняются” SEO ботами и “лайкателями” и несут скрытый посыл или “слегка замаскированный”. Идентифицирующий признак: явно не выражен, максимальное соответствие реальному аккаунту. Как правило, такие боты подключены к системам ИИ общения и даже могут односложно отвечать на ваши сообщения. Из личного опыта: иногда владельцы таких ботов используют фотографии “под открытой лицензией” для “главарей” своих ботов, боясь жалоб за использование чужих фото. В разных странах подобные боты имеют даже свое собственное название. В РФ, согласно легендам, существуют “Тролли из Ольгино”, якобы созданные для того, чтобы создавать иллюзию сверхвысокой поддержки власти народом. Получившие свое название за место своей первичной локации. (статья об этом) В Украине, согласно легендам, существуют “Порохоботы”, направленные на поддержку президента. Происхождение названия очевидно. (упоминание об этом) В США, авторитетный журнал motherboard и другие издания рассказывали, что социальные боты используются в лагерях обоих (тогда еще) кандидатов на пост президента Дональда Трампа и Хиллари Клинтон. (статья об этом) Для объективности и честности стоит сказать, что практически все значимые мировые лидеры прибегают к помощи высокотехнологичных компаний, занимающихся ботоводством. Фразу “согласно легендам” я добавил для той массы населения, которая опровергает вмешательство государства в частную жизнь своих граждан, социальные сети и прямое воздействие на мнение людей. Эти технологии используются политтехнологами всех стран. А обслуживающие такие “акции” компании получают солидные чеки. Эта статья не претендует на роль полного руководства по классификации социального ботоводства, а является лишь отражением моего личного мнения. За время работы в сфере информационной безопасности я сталкивался, да и чего греха таить, пробовал использовать почти все вышеописанные категории ботов, как по отдельности, так и в связке, тестируя их на количество привлеченного трафика и отзывов (лайки, репосты), но это уже совсем другая история."], "hab": ["Информационная безопасность"]}{"url": "https://habrahabr.ru/post/280498/", "title": ["Как накрутить счетчик Google Analytics или Google ненавидит Казахстан"], "text": ["Добрый день. Думаю многие читали историю про Катю, я решил написать в том же стиле, т.е. информация о том почему Google ненавидит Казахстан будет в спойлерах. И так. Предыстория. Однажды изучаю статистику в Google Search Console, я заметил что в разделе «Вид в поиске» появился еще один пункт, этот пункт вел на отчет о проиндексированных AMP страницах. Зайдя на эту страницу я увидел надпись Мне конечно же сразу захотелось узнать что это за ускоренные мобильные страницы. Пройдя по ссылке я увидел гайдлайн по созданию AMP страниц. Почитав гайдлайн я приступил к созданию amp страниц на своем сайте. История Прочитав документацию по созданию AMP, я сразу же приступил к реализации. Google ненавидит КазахстанПосле того как я подготовил все свои страницы, я стал ждать официального запуска AMP, он должен был быть 24 февраля, и он запустился 24 февраля, НО Казахстан не входил в список тех стран у которых будут отображаться AMP версии страниц в поиске. И нигде об этом не было написано. И нигде не написано когда будет доступен AMP в Казахстане. Наверное потому что... Google ненавидит Казахстан Через час у меня уже был готовы AMP версий моих страниц. Но возникла проблема, я не мог вставить Google Analytics, т.к. вставлять JS скрипты в AMP страницы нельзя, а компонента amp-analytics еще не существовало, но был компонент amp-pixel. Погуглив немного я наткнулся на вот этот вопрос на Stackoverflow. Тут предлагали вставлять pixel с следующим url который передавал нужные значение в Google Analytics <amp-pixel src=\"https://ssl.google-analytics.com/collect?v=1&tid=UA-12345678-1&t=pageview&cid=$RANDOM&dt=$TITLE&dl=$CANONICAL_URL&z=$RANDOM\"></amp-pixel> Это картинка передавала уникальный ID пользователя, название статьи и ссылку указанную в О передаваемых параметрах можно почитать тут, честно говоря я еще сам не читал Google ненавидит КазахстанТ.к. я работаю в редакции онлайн СМИ, для нас важны все методы привлечения трафика. Одним из основных каналов для других СМИ, это Google News. Но в Google News нет возможности выбрать новости с Казахстана. Черт, там есть даже новости с Эфиопии и Кении!!! А с Казахстана нету!!! У меня бомбит. Это все потому что... Google ненавидит Казахстан Добавив этот пиксель на свой сайт я заметил что при каждом обновлении страницы, он считал меня как нового пользователя, посмотрев еще раз на ссылку я понял что $RANDOM в amp-pixel каждый раз создавал новый ID для меня и GA считал меня уником. Первое что мне пришло в голову, это написания цикла который будет выводить кучу amp-pixel. Для начала я решил создать 100 amp-pixel чтобы не нагружать браузер. Вот сам цикл p.s. я использую laravel, поэтому разметка blade @for($i=0;$i<100;$i++) <amp-pixel src=\"https://www.google-analytics.com/collect?v=1&t=pageview&z={{rand(100000,500000)}}&dt=$TITLE&dl=$CANONICAL_URL&tid=UA-59188XXX-1&cid=$RANDOM\" > </amp-pixel> @endfor Я открыл страницу, дождался прогрузки и решил посмотреть на Google Analytics. И вот что я там увидел… , Google Analytics показывал что у меня на сайте 100 активных пользователей и все они читают ту статью. Я решил попробовать увеличить цикл до 1000 и у меня стало 1000 активных пользователей. Google ненавидит КазахстанИз прошлого спойлера вы знаете что нет выпуска «Казахстан» в Google News, но можно добавить свой сайт в новости, и они будут отображаться в секции Казахстан в Российском выпуске. В Google Новости для издателей можно добавить особый RSS «Выбор редакции» и он должен отображаться в правой части на главной news.google.ru, я благополучно добавил этот rss фид и стал ждать. Прождав неделю я решил написать в саппорт гугла и там я узнал что «Выбор редакции» не будет отображаться, т.к. СМИ казахстанский, а выпуск Российский (так какого х**, казахстанский СМИ вообще выходит в Российском выпуске???) Определенно Google ненавидит Казахстан Но показатель отказов был 100%, глубина 1 да и рендеринг amp-pixel сильно грузил процессов. Поэтому я сначала решил создать обычную версию страницы(Обычный HTML, вместо AMP) и вставить туда сотню изображении с этой ссылкой. Как оказалось это работало и в обычном HTML. Теперь надо было решать проблему глубины и отказов, поэтому за место рандомной генерации ID пользователя я решил создать массив. $users = range(1,2000); Дальше создаем 2 страницы с разными ссылками и названиями, где есть ссылка на другую страницу. Заходим на первую страницу ждем пока загрузятся картинки, и переходим на 2-ую страницу и так с десяток раз. Показательная гифка В итоге мы имеем приличную глубину, большое кол-во просмотров, низкий показатель отказов и т.д. Думаю что можно накрутить и другие показатели, так что экспериментируете :) P.S. Чуть не забыл, вы можете накрутить статистику кому угодно, главное знать идентификатор отслеживания."], "hab": ["Программирование", "Информационная безопасность"]}{"url": "https://habrahabr.ru/post/175685/", "title": ["GitHub Pages переезжают на github.io"], "text": ["Начиная с сегодняшнего дня все сайты GitHub Pages переходят на новый домен: github.io. Это мера безопасности нацеленна на предотвращение CSRF атак на главный сервер — github.com. Если ваш сайт настроен, как «yoursite.com» вместо «yoursite.github.com» — изменения вас никак не затронут. Если ваш сайт раньше располагался на домене «username.github.com», последующие запросы будут редиректиться на новый домен: «username.github.io». C этого момента все сайты, размещенные на субдоменах github.com могут и должны расцениваться как официальные продукты GitHub. Технические детали Изменения в сайтах и пользовательских доменах: Все пользователи, организации, и сайты проектов настроенные на работу с github.io, вместо github.com. Все сайты *.github.com редиректятся c кодом 301 на *.github.io. Сайты, с пользовательскими доменами изменения не затронули. IP адреса не изменились, существующие A-записи указывают на старые IP. Изменения в GitHub репозиториях: Пользовательские сайты можно называть используя новое, или старое соглашение: username/username.github.[io/com]. Существующие репозитории сайтов, названные по старому соглашению так и будут продолжать свою работу. Если существует два репозитория, названные по старому и новому соглашению — *.github.io победит. Уязвимости безопасности. Существует две основные категории потенциальных уязвимостей, которые привели к этим переменам. Изменение сессий и CSRF. Так как пользовательские сайты могут подключать JavaScript, который хранится на субдоменах github.com, становится возможным записывать(но не считывать) куки с домена github.com, что позволяет злоумышленнику получить доступ к github.com и осуществить атаку Фишинговые атаки, при которых злоумышленник создает похожий сайт и запрашивает у пользователя конфиденциальную информацию. У нас нет доказательств, что такие атаки были, однако мы постарались их заранее не допустить. Источник: github.com/blog/1452-new-github-pages-domain-github-io UPD: В комментариях s01o указал на пост Егора Хомакова, там рассказывается как всетаки можно провести атаку. Как оказалось — возможно это в WebKit браузерах, так как они особым образом работают с куками. Так это устроенно в нормальных браузерах: Cookie:_gh_sess=ORIGINAL; _gh_sess=HACKED; Надо понимать, что _gh_sess — это две разные куки, не смотря на название и отсылаются они в одно и то же время. В WebKit дело же обстоит по другому, куки отсылаются не по домену (первой записью должно быть Domain=github.com) ровно тоже самое и с httpOnly(очевидно, что они также должны быть в начале). В действительности они упорядочиваются по времени создания (В этом месте я должен быть не прав, но это то, как есть на самом деле)"], "hab": ["Системы управления версиями", "Git"]}{"url": "https://habrahabr.ru/post/319080/", "title": ["Основы компьютерных сетей. Тема №6. Понятие VLAN, Trunk и протоколы VTP и DTP", "tutorial"], "text": ["Всех с наступившим новым годом! Продолжаем разговор о сетях и сегодня затронем такую важную тему в мире коммутации, как VLAN. Посмотрим, что он из себя представляет и как с ним работать. А также разберем работающие с ним протоколы VTP и DTP. Содержание1) Основные сетевые термины, сетевая модель OSI и стек протоколов TCP/IP. 2) Протоколы верхнего уровня. 3) Протоколы нижних уровней (транспортного, сетевого и канального). 4) Сетевые устройства и виды применяемых кабелей. 5) Понятие IP адресации, масок подсетей и их расчет. 6) Понятие VLAN, Trunk и протоколы VTP и DTP. 7) Протокол связующего дерева: STP. 8) Протокол агрегирования каналов: Etherchannel. 9) Маршрутизация: статическая и динамическая на примере RIP, OSPF и EIGRP. 10) Трансляция сетевых адресов: NAT и PAT. 11) Протоколы резервирования первого перехода: FHRP. 12) Безопасность компьютерных сетей и виртуальные частные сети: VPN. 13) Глобальные сети и используемые протоколы: PPP, HDLC, Frame Relay. 14) Введение в IPv6, конфигурация и маршрутизация. 15) Сетевое управление и мониторинг сети. P.S. Возможно, со временем список дополнится. В предыдущих статьях мы уже работали с многими сетевыми устройствами, поняли, чем они друг от друга отличаются и рассмотрели из чего состоят кадры, пакеты и прочие PDU. В принципе с этими знаниями можно организовать простейшую локальную сеть и работать в ней. Но мир не стоит на месте. Появляется все больше устройств, которые нагружают сеть или что еще хуже — создают угрозу в безопасности. А, как правило, «опасность» появляется раньше «безопасности». Сейчас я на самом простом примере покажу это. Мы пока не будем затрагивать маршрутизаторы и разные подсети. Допустим все узлы находятся в одной подсети. Сразу приведу список IP-адресов: PC1 – 192.168.1.2/24 PC2 – 192.168.1.3/24 PC3 – 192.168.1.4/24 PC4 – 192.168.1.5/24 PC5 – 192.168.1.6/24 PC6 – 192.168.1.7/24 У нас 3 отдела: дирекция, бухгалтерия, отдел кадров. У каждого отдела свой коммутатор и соединены они через центральный верхний. И вот PC1 отправляет ping на компьютер PC2. Кто хочет увидеть это в виде анимации, открывайте спойлер (там показан ping от PC1 до PC5). Работа сети в одном широковещательном домене Красиво да? Мы в прошлых статьях уже не раз говорили о работе протокола ARP, но это было еще в прошлом году, поэтому вкратце объясню. Так как PC1 не знает MAC-адрес (или адрес канального уровня) PC2, то он отправляет в разведку ARP, чтобы тот ему сообщил. Он приходит на коммутатор, откуда ретранслируется на все активные порты, то есть к PC2 и на центральный коммутатор. Из центрального коммутатора вылетит на соседние коммутаторы и так далее, пока не дойдет до всех. Вот такой не маленький трафик вызвало одно ARP-сообщение. Его получили все участники сети. Большой и не нужный трафик — это первая проблема. Вторая проблема — это безопасность. Думаю, заметили, что сообщение дошло даже до бухгалтерии, компьютеры которой вообще не участвовали в этом. Любой злоумышленник, подключившись к любому из коммутаторов, будет иметь доступ ко всей сети. В принципе сети раньше так и работали. Компьютеры находились в одной канальной среде и разделялись только при помощи маршрутизаторов. Но время шло и нужно было решать эту проблему на канальном уровне. Cisco, как пионер, придумала свой протокол, который тегировал кадры и определял принадлежность к определенной канальной среде. Назывался он ISL (Inter-Switch Link). Идея эта понравилась всем и IEEE решили разработать аналогичный открытый стандарт. Стандарт получил название 802.1q. Получил он огромное распространение и Cisco решила тоже перейти на него. И вот как раз технология VLAN основывается на работе протокола 802.1q. Давайте уже начнем говорить про нее. В 3-ей части я показал, как выглядит ethernet-кадр. Посмотрите на него и освежите в памяти. Вот так выглядит не тегированный кадр. Теперь взглянем на тегированный. Как видим, отличие в том, что появляется некий Тег. Это то, что нам и интересно. Копнем глубже. Состоит он из 4-х частей. 1) TPID (англ. Tag Protocol ID) или Идентификатор тегированного протокола — состоит из 2-х байт и для VLAN всегда равен 0x8100. 2) PCP (англ. Priority Code Point) или значение приоритета — состоит из 3-х бит. Используется для приоритезации трафика. Крутые и бородатые сисадмины знают, как правильно им управлять и оперирует им, когда в сети гуляет разный трафик (голос, видео, данные и т.д.) 3) CFI (англ. Canonical Format Indicator) или индикатор каноничного формата — простое поле, состоящее из одного бита. Если стоит 0, то это стандартный формат MAC-адреса. 4) VID (англ. VLAN ID) или идентификатор VLAN — состоит из 12 бит и показывает, в каком VLAN находится кадр. Хочу заострить внимание на том, что тегирование кадров осуществляется между сетевыми устройствами (коммутаторы, маршрутизаторы и т.д.), а между конечным узлом (компьютер, ноутбук) и сетевым устройством кадры не тегируются. Поэтому порт сетевого устройства может находиться в 2-х состояниях: access или trunk. Access port или порт доступа — порт, находящийся в определенном VLAN и передающий не тегированные кадры. Как правило, это порт, смотрящий на пользовательское устройство. Trunk port или магистральный порт — порт, передающий тегированный трафик. Как правило, этот порт поднимается между сетевыми устройствами. Сейчас я покажу это на практике. Открываю ту же лабу. Картинку повторять не буду, а сразу открою коммутатор и посмотрю, что у него с VLAN. Набираю команду show vlan. Выстраиваются несколько таблиц. Нам по сути важна только самая первая. Теперь покажу как ее читать. 1 столбец — это номер VLAN. Здесь изначально присутствует номер 1 — это стандартный VLAN, который изначально есть на каждом коммутаторе. Он выполняет еще одну функцию, о которой чуть ниже напишу. Также присутствуют зарезервированные с 1002-1005. Это для других канальных сред, которые вряд ли сейчас используются. Удалить их тоже нельзя. Switch(config)#no vlan 1005 Default VLAN 1005 may not be deleted. При удалении Cisco выводит сообщение, что этот VLAN удалить нельзя. Поэтому живем и эти 4 VLANа не трогаем. 2 столбец — это имя VLAN. При создании VLAN, вы можете на свое усмотрение придумывать им осмысленные имена, чтобы потом их идентифицировать. Тут уже есть default, fddi-default, token-ring-default, fddinet-default, trnet-default. 3 столбец — статус. Здесь показывается в каком состоянии находится VLAN. На данный момент VLAN 1 или default в состоянии active, а 4 следующих act/unsup (хоть и активные, но не поддерживаются). 4 столбец — порты. Здесь показано к каким VLAN-ам принадлежат порты. Сейчас, когда мы еще ничего не трогали, они находятся в default. Приступаем к настройке коммутаторов. Правилом хорошего тона будет дать коммутаторам осмысленные имена. Чем и займемся. Привожу команду. Switch(config)#hostname CentrSW CentrSW(config)# Остальные настраиваются аналогично, поэтому покажу обновленную схему топологии. Начнем настройку с коммутатора SW1. Для начала создадим VLAN на коммутаторе. SW1(config)#vlan 2 - создаем VLAN 2 (VLAN 1 по умолчанию зарезервирован, поэтому берем следующий). SW1(config-vlan)#name Dir-ya - попадаем в настройки VLAN и задаем ему имя. VLAN создан. Теперь переходим к портам. Интерфейс FastEthernet0/1 смотрит на PC1, а FastEthernet0/2 на PC2. Как говорилось ранее, кадры между ними должны передаваться не тегированными, поэтому переведем их в состояние Access. SW1(config)#interface fastEthernet 0/1 - переходим к настройке 1-ого порта. SW1(config-if)#switchport mode access - переводим порт в режим access. SW1(config-if)#switchport access vlan 2 - закрепляем за портом 2-ой VLAN. SW1(config)#interface fastEthernet 0/2 - переходим к настройке 2-ого порта. SW1(config-if)#switchport mode access - переводим порт в режим access. SW1(config-if)#switchport access vlan 2 - закрепляем за портом 2-ой VLAN. Так как оба порта закрепляются под одинаковым VLAN-ом, то их еще можно было настроить группой. SW1(config)#interface range fastEthernet 0/1-2 - то есть выбираем пул и далее настройка аналогичная. SW1(config-if-range)#switchport mode access SW1(config-if-range)#switchport access vlan 2 Настроили access порты. Теперь настроим trunk между SW1 и CentrSW. SW1(config)#interface fastEthernet 0/24 - переходим к настройке 24-ого порта. SW1(config-if)#switchport mode trunk - переводим порт в режим trunk. %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/24, changed state to down %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/24, changed state to up Сразу видим, что порт перенастроился. В принципе для работы этого достаточно. Но с точки зрения безопасности разрешать для передачи нужно только те VLAN, которые действительно нужны. Приступим. SW1(config-if)#switchport trunk allowed vlan 2 - разрешаем передавать только 2-ой VLAN. Без этой команды передаваться будут все имеющиеся VLAN. Посмотрим, как изменилась таблица командой show vlan. Появился 2-ой VLAN с именем Dir-ya и видим принадлежащие ему порты fa0/1 и fa0/2. Чтобы вывести только верхнюю таблицу, можно воспользоваться командой show vlan brief. Можно еще укоротить вывод, если указать определенный ID VLANа. Или его имя. Вся информациях о VLAN хранится в flash памяти в файле vlan.dat. Как вы заметили, ни в одной из команд, нет информации о trunk. Ее можно посмотреть другой командой show interfaces trunk. Здесь есть информация и о trunk портах, и о том какие VLAN они передают. Еще тут есть столбец Native vlan. Это как раз тот трафик, который не должен тегироваться. Если на коммутатор приходит не тегированный кадр, то он автоматически причисляется к Native Vlan (по умолчанию и в нашем случае это VLAN 1). Native VLAN можно, а многие говорят, что нужно менять в целях безопасности. Для этого в режиме настройки транкового порта нужно применить команду — switchport trunk native vlan X, где X — номер присваиваемого VLAN. В этой топологии мы менять не будем, но знать, как это делать полезно. Осталось настроить остальные устройства. CentrSW: Центральный коммутатор является связующим звеном, а значит он должен знать обо всех VLAN-ах. Поэтому сначала создаем их, а потом переводим все интерфейсы в транковый режим. CentrSW(config)#vlan 2 CentrSW(config-vlan)# name Dir-ya CentrSW(config)#vlan 3 CentrSW(config-vlan)# name buhgalter CentrSW(config)#vlan 4 CentrSW(config-vlan)# name otdel-kadrov CentrSW(config)#interface range fastEthernet 0/1-3 CentrSW(config-if-range)#switchport mode trunk Не забываем сохранять конфиг. Команда copy running-config startup-config. SW2: SW2(config)#vlan 3 SW2(config-vlan)#name buhgalter SW2(config)#interface range fastEthernet 0/1-2 SW2(config-if-range)#switchport mode access SW2(config-if-range)#switchport access vlan 3 SW2(config)#interface fastEthernet 0/24 SW2(config-if)#switchport mode trunk SW2(config-if)#switchport trunk allowed vlan 3 SW3: SW3(config)#vlan 4 SW3(config-vlan)#name otdel kadrov SW3(config)#interface range fastEthernet 0/1-2 SW3(config-if-range)#switchport mode access SW3(config-if-range)#switchport access vlan 4 SW3(config)#interface fastEthernet 0/24 SW3(config-if)#switchport mode trunk SW3(config-if)#switchport trunk allowed vlan 4 Обратите внимание на то, что мы подняли и настроили VLAN, но адресацию узлов оставили такой же. То есть, фактически все узлы в одной подсети, но разделены VLAN-ами. Так делать нельзя. Каждому VLAN надо выделять отдельную подсеть. Я это сделал исключительно в учебных целях. Если бы каждый отдел сидел в своей подсети, то они бы априори были ограничены, так как коммутатор не умеет маршрутизировать трафик из одной подсети в другую (плюс это уже ограничение на сетевом уровне). А нам нужно ограничить отделы на канальном уровне. Снова отправляю ping с PC1 к PC3. Идет в ход ARP, который нам и нужен сейчас. Откроем его. Пока что ничего нового. ARP инкапсулирован в ethernet. Кадр прилетает на коммутатор и тегируется. Теперь там не обычный ethernet, а 802.1q. Добавились поля, о которых я писал ранее. Это TPID, который равен 8100 и показывающий, что это 802.1q. И TCI, которое объединяет 3 поля PCP, CFI и VID. Число, которое в этом поле — это номер VLAN. Двигаемся дальше. После тега он отправляет кадр на PC2 (т.к. он в том же VLAN) и на центральный коммутатор по транковому порту. Так как жестко не было прописано какие типы VLAN пропускать по каким портам, то он отправит на оба коммутатора. И вот здесь коммутаторы, увидев номер VLAN, понимают, что устройств с таким VLAN-ом у них нет и смело его отбрасывают. PC1 ожидает ответ, который так и не приходит. Можно под спойлером посмотреть в виде анимации. Анимация Теперь следующая ситуация. В состав дирекции нанимают еще одного человека, но в кабинете дирекции нет места и на время просят разместить человека в отделе бухгалтерии. Решаем эту проблему. Подключили компьютер к порту FastEthernet 0/3 коммутатора и присвою IP-адрес 192.168.1.8/24. Теперь настрою коммутатор SW2. Так как компьютер должен находиться во 2-ом VLAN, о котором коммутатор не знает, то создам его на коммутаторе. SW2(config)#vlan 2 SW2(config-vlan)#name Dir-ya Дальше настраиваем порт FastEthernet 0/3, который смотрит на PC7. SW2(config)#interface fastEthernet 0/3 SW2(config-if)#switchport mode access SW2(config-if)#switchport access vlan 2 И последнее — настроить транковый порт. SW2(config)#interface fastEthernet 0/24 SW2(config-if)#switchport trunk allowed vlan add 2 - обратите внимание на эту команду. А именно на ключевое слово \"add\". Если не дописать это слово, то вы сотрете все остальные разрешенные к передаче VLAN на этом порту. Поэтому если у вас уже был поднят транк на порту и передавались другие VLAN, то добавлять надо именно так. Чтобы кадры ходили красиво, подкорректирую центральный коммутатор CentrSW. CentrSW(config)#interface fastEthernet 0/1 CentrSW(config-if)#switchport trunk allowed vlan 2 CentrSW(config)#interface fastEthernet 0/2 CentrSW(config-if)#switchport trunk allowed vlan 2,3 CentrSW(config)#interface fastEthernet 0/3 CentrSW(config-if)#switchport trunk allowed vlan 4 Время проверки. Отправляю ping с PC1 на PC7. Пока что весь путь аналогичен предыдущему. Но вот с этого момента (с картинки ниже) центральный коммутатор примет другое решение. Он получает кадр и видит, что тот протегирован 2-ым VLAN-ом. Значит отправлять его надо только туда, где это разрешено, то есть на порт fa0/2. И вот он приходит на SW2. Открываем и видим, что он еще тегированный. Но следующим узлом стоит компьютер и тег надо снимать. Нажимаем на «Outbound PDU Details», чтобы посмотреть в каком виде кадр вылетит из коммутатора. И действительно. Коммутатор отправит кадр в «чистом» виде, то есть без тегов. Доходит ARP до PC7. Открываем его и убеждаемся, что кадр не тегированный PC7 узнал себя и отправляет ответ. Открываем кадр на коммутаторе и видим, что на отправку он уйдет тегированным. Дальше кадр будет путешествовать тем же путем, что и пришел. ARP доходит до PC1, о чем свидетельствует галочка на конверте. Теперь ему известен MAC-адрес и он пускает в ход ICMP. Открываем пакет на коммутаторе и наблюдаем такую же картину. На канальном уровне кадр тегируется коммутатором. Так будет с каждым сообщением. Видим, что пакет успешно доходит до PC7. Обратный путь я показывать не буду, так как он аналогичен. Если кому интересно, можно весь путь увидеть на анимации под спойлером ниже. А если охота самому поковырять эту топологию, прикладываю ссылку на лабораторку. Логика работы VLAN Вот в принципе самое популярное применение VLAN-ов. Независимо от физического расположения, можно логически объединять узлы в группы, там самым изолируя их от других. Очень удобно, когда сотрудники физически работают в разных местах, но должны быть объединены. И конечно с точки зрения безопасности VLAN не заменимы. Главное, чтобы к сетевым устройствам имели доступ ограниченный круг лиц, но это уже отдельная тема. Добились ограничения на канальном уровне. Трафик теперь не гуляет где попало, а ходит строго по назначению. Но теперь встает вопрос в том, что отделам между собой нужно общаться. А так как они в разных канальных средах, то в дело вступает маршрутизация. Но перед началом, приведем топологию в порядок. Самое первое к чему приложим руку — это адресация узлов. Повторюсь, что каждый отдел должен находиться в своей подсети. Итого получаем: Дирекция — 192.168.1.0/24 Бухгалтерия — 192.168.2.0/24 Отдел кадров — 192.168.3.0/24 Раз подсети определены, то сразу адресуем узлы. PC1: IP: 192.168.1.2 Маска: 255.255.255.0 или /24 Шлюз: 192.168.1.1 PC2: IP: 192.168.1.3 Маска: 255.255.255.0 или /24 Шлюз: 192.168.1.1 PC3: IP: 192.168.2.2 Маска: 255.255.255.0 или /24 Шлюз: 192.168.2.1 PC4: IP: 192.168.2.3 Маска: 255.255.255.0 или /24 Шлюз: 192.168.2.1 PC5: IP: 192.168.3.2 Маска: 255.255.255.0 или /24 Шлюз: 192.168.3.1 PC6: IP: 192.168.3.3 Маска: 255.255.255.0 или /24 Шлюз: 192.168.3.1 PC7: IP: 192.168.1.4 Маска: 255.255.255.0 или /24 Шлюз: 192.168.1.1 Теперь про изменения в топологии. Видим, что добавился маршрутизатор. Он как раз и будет перекидывать трафик с одного VLAN на другой (иными словами маршрутизировать). Изначально соединения между ним и коммутатором нет, так как интерфейсы выключены. У узлов добавился такой параметр, как адрес шлюза. Этот адрес они используют, когда надо отправить сообщение узлу, находящемуся в другой подсети. Соответственно у каждой подсети свой шлюз. Осталось настроить маршрутизатор, и я открываю его CLI. По традиции дам осмысленное имя. Router(config)#hostname Gateway Gateway(config)# Далее переходим к настройке интерфейсов. Gateway(config)#interface fastEthernet 0/0 - переходим к требуемому интерфейсу. Gateway(config-if)#no shutdown - включаем его. %LINK-5-CHANGED: Interface FastEthernet0/0, changed state to up %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/0, changed state to up Теперь внимание! Мы включили интерфейс, но не повесили на него IP-адрес. Дело в том, что от физического интерфейса (fastethernet 0/0) нужен только линк или канал. Роль шлюзов будут выполнять виртуальные интерфейсы или сабинтерфейсы (англ. subinterface). На данный момент 3 типа VLAN. Значит и сабинтерфейсов будет 3. Приступаем к настройке. Gateway(config)#interface fastEthernet 0/0.2 Gateway(config-if)#encapsulation dot1Q 2 Gateway(config-if)#ip address 192.168.1.1 255.255.255.0 Gateway(config)#interface fastEthernet 0/0.3 Gateway(config-if)#encapsulation dot1Q 3 Gateway(config-if)#ip address 192.168.2.1 255.255.255.0 Gateway(config)#interface fastEthernet 0/0.4 Gateway(config-if)#encapsulation dot1Q 4 Gateway(config-if)#ip address 192.168.3.1 255.255.255.0 Маршрутизатор настроен. Переходим к центральному коммутатору и настроим на нем транковый порт, чтобы он пропускал тегированные кадры на маршрутизатор. CentrSW(config)#interface fastEthernet 0/24 CentrSW(config-if)#switchport mode trunk CentrSW(config-if)#switchport trunk allowed vlan 2,3,4 Конфигурация закончена и переходим к практике. Отправляю ping с PC1 на PC6 (то есть на 192.168.3.3). PC1 понятия не имеет, кто такой PC6 или 192.168.3.3, но знает, что они находятся в разных подсетях (как он это понимает описано в предыдущей статье). Поэтому он отправит сообщение через основной шлюз, адрес которого указан в его настройках. И хоть PC1 знает IP-адрес основного шлюза, для полного счастья не хватает MAC-адреса. И он пускает в ход ARP. Обратите внимание. Как только кадр прибывает на CentrSW, коммутатор не рассылает его кому попало. Он рассылает только на те порты, где разрешен пропуск 2-го VLAN. То есть на маршрутизатор и на SW2 (там есть пользователь, сидящий во 2-ом VLAN). Маршрутизатор узнает себя и отправляет ответ (показан стрелочкой). И обратите внимание на нижний кадр. Когда SW2 получил ARP от центрального коммутатора, он аналогично не стал рассылать его на все компьютеры, а отправил только PC7, который сидит во 2-ом VLAN. Но PC7 его отбрасывает, так как он не для него. Смотрим дальше. ARP дошел до PC1. Теперь ему все известно и можно отправлять ICMP. Еще раз обращу внимание на то, что в качестве MAC-адреса назначения (канальный уровень), будет адрес маршрутизатора, а в качестве IP-адреса назначения (сетевой уровень), адрес PC6. Доходит ICMP до маршрутизатора. Он смотрит в свою таблицу и понимает, что не знает никого под адресом 192.168.3.3. Отбрасывает прибывший ICMP и пускает разведать ARP. PC6 узнает себя и отправляет ответ. Доходит до маршрутизатора ответ и он добавляет запись в своей таблице. Посмотреть таблицу ARP можно командой show arp. Двигаемся дальше. PC1 недоволен, что ему никто не отвечает и отправляет следующее ICMP-сообщение. На этот раз ICMP доходит без проблем. Обратно он проследует тем же маршрутом. Я лишь покажу конечный результат. Первый пакет затерялся (в результате работы ARP), а второй дошел без проблем. Кому интересно увидеть в анимации, добро пожаловать под спойлер. InterVLAN Routing Итак. Мы добились того, что если узлы находятся в одной подсети и в одном VLAN, то ходить они будут напрямую через коммутаторы. В случае, когда нужно передать сообщение в другую подсеть и VLAN, то передавать будут через роутер Gateway, который осуществляет «межвлановую» маршрутизацию. Данная топология получила название «router on a stick» или «роутер на палочке». Как вы поняли она очень удобна. Мы создали 3 виртуальных интерфейса и по одному проводу гоняли разные тегированные кадры. Без использования сабинтерфейсов и VLAN-ов, пришлось бы для каждой подсети задействовать отдельный физический интерфейс, что совсем не выгодно. Кстати очень хорошо этот вопрос разобран в этом видео (видео идет около 3-х часов, поэтому ссылка с привязкой именно к тому моменту времени). Если после прочтения и просмотра видео захочется добить все собственными руками, прикладываю ссылку на скачивание. Разобрались с VLAN-ами и переходим к одному из протоколов, работающего с ним. DTP (англ. Dynamic Trunking Protocol) или на русском динамический транковый протокол — проприетарный протокол компании Cisco, служащий для реализации trunk режима между коммутаторами. Хотя в зависимости от состояния, они могут согласоваться и в режим access. В DTP есть 4 режима: Dynamic auto, Dynamic desirable, Trunk, Access. Рассмотрим как они согласуются. Режимы Dynamic auto Dynamic desirable Trunk Access Dynamic auto Access Trunk Trunk Access Dynamic desirable Trunk Trunk Trunk Access Trunk Trunk Trunk Trunk Отсутствие соединения Access Access Access Отсутствие соединения Access То есть левая колонка это 1-ое устройство, а верхняя строка 2-ое устройство. По-умолчанию коммутаторы находятся в режиме «dynamic auto». Если посмотреть таблицу сопоставления, то два коммутатора в режиме «dynamic auto» согласуются в режим «access». Давайте это и проверим. Создаю я новую лабораторную работу и добавлю 2 коммутатора. Соединять их пока не буду. Мне надо убедиться, что оба коммутатора в режиме «dynamic auto». Проверять буду командой show interfaces switchport. Результат этой команды очень большой, поэтому я его обрезал и выделил интересующие пункты. Начнем с Administrative Mode. Эта строка показывает, в каком из 4-режимов работает данный порт на коммутаторе. Убеждаемся, что на обоих коммутаторах порты в режиме «Dynamic auto». А строка Operational Mode показывает, в каком режиме работы они согласовали работу. Мы пока их не соединяли, поэтому они в состоянии «down». Сразу дам вам хороший совет. При тестировании какого либо протокола, пользуйтесь фильтрами. Отключайте показ работы всех ненужных вам протоколов. Перевожу CPT в режим simulation и отфильтрую все протоколы, кроме DTP. Думаю здесь все понятно. Соединяю коммутаторы кабелем и, при поднятии линков, один из коммутаторов генерирует DTP-сообщение. Открываю и вижу, что это DTP инкапсулированный в Ethernet-кадр. Отправляет он его на мультикастовый адрес «0100.0ccc.cccc», который относится к протоколам DTP, VTP, CDP. И обращу внимание на 2 поля в заголовке DTP. 1) DTP Type — сюда отправляющий вставляет предложение. То есть в какой режим он хочет согласоваться. В нашем случае он предлагает согласовать «access». 2) Neighbor MAC-address — в это поле он записывает MAC-адрес своего порта. Отправляет он и ждет реакции от соседа. Доходит до SW1 сообщение и он генерирует ответный. Где также согласует режим «access», вставляет свой MAC-адрес и отправляет в путь до SW2. Успешно доходит DTP. По идее они должны были согласоваться в режиме «access». Проверю. Как и предполагалось, согласовались они в режим «access». Кто то говорит, что технология удобная и пользуется ею. Но я крайне не рекомендую использовать этот протокол в своей сети. Рекомендую это не только я, и сейчас объясню почему. Смысл в том, что этот протокол открывает большую дыру в безопасности. Я открою лабораторку, в которой разбиралась работа «Router on a stick» и добавлю туда еще один коммутатор. Теперь зайду в настройки нового коммутатора и жестко пропишу на порту работу в режиме trunk. New_SW(config)#interface fastEthernet 0/1 New_SW(config-if)#switchport mode trunk Соединяю их и смотрю, как они согласовались. Все верно. Режимы «dynamic auto» и «trunk» согласуются в режим trunk. Теперь ждем, когда кто- то начнет проявлять активность. Допустим PC1 решил кому то отправить сообщение. Формирует ARP и выпускает в сеть. Пропустим его путь до того момента, когда он попадет на SW2. И вот самое интересное. Он отправляет его на вновь подключенный коммутатор. Объясняю, что произошло. Как только мы согласовали с ним trunk, он начинает отправлять ему все пришедшие кадры. Хоть на схеме и показано, что коммутатор отбрасывает кадры, это ничего не значит. К коммутатору или вместо коммутатора можно подключить любое перехватывающее устройство (sniffer) и спокойно просматривать, что творится в сети. Вроде перехватил он безобидный ARP. Но если взглянуть глубже, то можно увидеть, что уже известен MAC-адрес «0000.0C1C.05DD» и IP-адрес «192.168.1.2». То есть PC1 не думая выдал себя. Теперь злоумышленник знает о таком компьютере. Вдобавок он знает, что он сидит во 2-ом VLAN. Дальше он может натворить многого. Самое банальное — это подменить свой MAC-адрес, IP-адрес, согласоваться быстро в Access и и выдавать себя за PC1. Но самое интересное. Ведь сразу можно этого не понять. Обычно, когда мы прописываем режим работы порта, он сразу отображается в конфигурации. Ввожу show running-config . Но здесь настройки порта пустые. Ввожу show interfaces switchport и проматываю до fa0/4. А вот здесь видим, что согласован trunk. Не всегда show running-config дает исчерпывающую информацию. Поэтому запоминайте и другие команды. Думаю понятно почему нельзя доверять этому протоколу. Он вроде облегчает жизнь, но в то же время может создать огромную проблему. Поэтому полагайтесь на ручной метод. При настройке сразу же обозначьте себе какие порты будут работать в режиме trunk, а какие в access. И самое главное — всегда отключайте согласование. Чтобы коммутаторы не пытались ни с кем согласоваться. Делается это командой «switchport nonegotiate». Переходим к следующему протоколу. VTP (англ. VLAN Trunking Protocol) — проприетарный протокол компании Cisco, служащий для обмена информацией о VLAN-ах. Представьте ситуацию, что у вас 40 коммутаторов и 70 VLAN-ов. По хорошему нужно вручную на каждом коммутаторе их создать и прописать на каких trunk-ых портах разрешать передачу. Дело это муторное и долгое. Поэтому эту задачу может взвалить на себя VTP. Вы создаете VLAN на одном коммутаторе, а все остальные синхронизируются с его базой. Взгляните на следующую топологию. Здесь присутствуют 4 коммутатора. Один из них является VTP-сервером, а 3 остальных клиентами. Те VLAN, которые будут созданы на сервере, автоматически синхронизируются на клиентах. Объясню как работает VTP и что он умеет. Итак. VTP может создавать, изменять и удалять VLAN. Каждое такое действие влечет к тому, что увеличивается номер ревизии (каждое действие увеличивает номер на +1). После он рассылает объявления, где указан номер ревизии. Клиенты, получившие это объявление, сравнивают свой номер ревизии с пришедшим. И если пришедший номер выше, они синхронизируют свою базу с ней. В противном случае объявление игнорируется. Но это еще не все. У VTP есть роли. По-умолчанию все коммутаторы работают в роли сервера. Расскажу про них. VTP Server. Умеет все. То есть создает, изменяет, удаляет VLAN. Если получает объявление, в которых ревизия старше его, то синхронизируется. Постоянно рассылает объявления и ретранслирует от соседей. VTP Client — Эта роль уже ограничена. Создавать, изменять и удалять VLAN нельзя. Все VLAN получает и синхронизирует от сервера. Периодически сообщает соседям о своей базе VLAN-ов. VTP Transparent — эта такая независимая роль. Может создавать, изменять и удалять VLAN только в своей базе. Никому ничего не навязывает и ни от кого не принимает. Если получает какое то объявление, передает дальше, но со своей базой не синхронизирует. Если в предыдущих ролях, при каждом изменении увеличивался номер ревизии, то в этом режиме номер ревизии всегда равен 0. Это все, что касается VTP версии 2. В VTP 3-ей версии добавилась еще одна роль — VTP Off. Он не передает никакие объявления. В остальном работа аналогична режиму Transparent. Начитались теории и переходим к практике. Проверим, что центральный коммутатор в режиме Server. Вводим команду show vtp status. Видим, что VTP Operating Mode: Server. Также можно заметить, что версия VTP 2-ая. К сожалению, в CPT 3-ья версия не поддерживается. Версия ревизии нулевая. Теперь настроим нижние коммутаторы. SW1(config)#vtp mode client Setting device to VTP CLIENT mode. Видим сообщение, что устройство перешло в клиентский режим. Остальные настраиваются точно также. Чтобы устройства смогли обмениваться объявлениями, они должны находиться в одном домене. Причем тут есть особенность. Если устройство (в режиме Server или Client) не состоит ни в одном домене, то при первом полученном объявлении, перейдет в объявленный домен. Если же клиент состоит в каком то домене, то принимать объявления от других доменов не будет. Откроем SW1 и убедимся, что он не состоит ни в одном домене. Убеждаемся, что тут пусто. Теперь переходим центральному коммутатору и переведем его в домен. CentrSW(config)#vtp domain cisadmin.ru Changing VTP domain name from NULL to cisadmin.ru Видим сообщение, что он перевелся в домен cisadmin.ru. Проверим статус. И действительно. Имя домена изменилось. Обратите внимание, что номер ревизии пока что нулевой. Он изменится, как только мы создадим на нем VLAN. Но перед созданием надо перевести симулятор в режим simulation, чтобы посмотреть как он сгенерирует объявления. Создаем 20-ый VLAN и видим следующую картинку. Как только создан VLAN и увеличился номер ревизии, сервер генерирует объявления. У него их два. Сначала откроем тот, что левее. Это объявление называется «Summary Advertisement» или на русском «сводное объявление». Это объявление генерируется коммутатором раз в 5 минут, где он рассказывает о имени домена и текущей ревизии. Смотрим как выглядит. В Ethernet-кадре обратите внимание на Destination MAC-адрес. Он такой же, как и выше, когда генерировался DTP. То есть, в нашем случае на него отреагируют только те, у кого запущен VTP. Теперь посмотрим на следующее поле. Здесь как раз вся информация. Пройдусь по самым важным полям. Management Domain Name — имя самого домена (в данном случае cisadmin.ru). Updater Identity — идентификатор того, кто обновляет. Здесь, как правило, записывается IP-адрес. Но так как адрес коммутатору не присваивали, то поле пустое Update Timestamp — время обновления. Время на коммутаторе не менялось, поэтому там стоит заводское. MD5 Digest — хеш MD5. Оно используется для проверки полномочий. То есть, если на VTP стоит пароль. Мы пароль не меняли, поэтому хэш по-умолчанию. Теперь посмотрим на следующее генерируемое сообщение (то, что справа). Оно называется «Subset Advertisement» или «подробное объявление». Это такая подробная информация о каждом передаваемом VLAN. Думаю здесь понятно. Отдельный заголовок для каждого типа VLAN. Список настолько длинный, что не поместился в экран. Но они точно такие, за исключением названий. Заморачивать голову, что означает каждый код не буду. Да и в CPT они тут больше условность. Смотрим, что происходит дальше. Получают клиенты объявления. Видят, что номер ревизии выше, чем у них и синхронизируют базу. И отправляют сообщение серверу о том, что база VLAN-ов изменилась. Принцип работы протокола VTP Вот так в принципе работает протокол VTP. Но у него есть очень большие минусы. И минусы эти в плане безопасности. Объясню на примере этой же лабораторки. У нас есть центральный коммутатор, на котором создаются VLAN, а потом по мультикасту он их синхронизирует со всеми коммутаторами. В нашем случае он рассказывает про VLAN 20. Предлагаю еще раз глянуть на его конфигурацию. И тут в сеть мы добавляем новый коммутатор. У него нет новых VLAN-ов, кроме стандартных и он не состоит ни в одном VTP-домене, но подкручен номер ревизии. И перед тем как его воткнуть в сеть, переводим порт в режим trunk. Теперь переключаю CPT в «Simulation Mode» и отфильтровываю все, кроме VTP. Подключаюсь и смотрю, что происходит. Через какое то время до NewSW доходит VTP сообщение, откуда он узнает, что в сети есть VTP-домен «cisadmin.ru». Так как он не состоял до этого в другом домене, он автоматически в него переходит. Проверим. Теперь он в том же домене, но с номером ревизии выше. Он формирует VTP-сообщение, где рассказывает об этом. Первым под раздачу попадет SW1. Заметьте, что на SW1 приходят сразу 2 VTP-сообщения (от NewSW и от CentrSW). В сообщении от NewSW он видит, что номер ревизии выше, чем его и синхронизирует свою базу. А вот сообщение от CentrSW для него уже устарело, и он отбрасывает его. Проверим, что изменилось на SW1. Обновился номер ревизии и, что самое интересное, база VLAN. Теперь она пустая. Смотрим дальше. Обратите внимание. До сервера доходит VTP-сообщение, где номер ревизии выше, чем у него. Он понимает, что сеть изменилась и надо под нее подстроиться. Проверим конфигурацию. Конфигурация центрального сервера изменилась и теперь он будет вещать именно ее. А теперь представьте, что у нас не один VLAN, а сотни. Вот таким простым способом можно положить сеть. Конечно домен может быть запаролен и злоумышленнику будет тяжелее нанести вред. А представьте ситуацию, что у вас сломался коммутатор и срочно надо его заменить. Вы или ваш коллега бежите на склад за старым коммутатором и забываете проверить номер ревизии. Он оказывается выше чем у остальных. Что произойдет дальше, вы уже видели. Поэтому я рекомендую не использовать этот протокол. Особенно в больших корпоративных сетях. Если используете VTP 3-ей версии, то смело переводите коммутаторы в режим «Off». Если же используется 2-ая версия, то переводите в режим «Transparent». Кому интересно посмотреть это в виде анимации, открывайте спойлер. Подключение коммутатора с большей ревизией Для желающих поработать с этой лабораторкой, прикладываю ссылку. Ну вот статья про VLAN подошла к концу. Если остались какие то вопросы, смело задавайте. Спасибо за прочтение."], "hab": ["Системное администрирование", "Сетевые технологии", "IT-инфраструктура", "Cisco"]}{"url": "https://habrahabr.ru/post/60030/", "title": ["Git Workflow"], "text": ["1 Вступление В топике освещаются не столько подробности работы с git, сколько его отличия от схемы разработки других систем контроля версий, и общий подход (выработанный по большей части личным опытом и Git Community Book) к работе. Современная разработка программного обеспечения стала чем-то большим, чем просто набором исходного кода программы в текстовом редакторе; она обросла целым комплексом дополнительного инструментария, вроде багтреккеров, систем для управления проектами и систем контроля версий (СКВ). Последние, пожалуй, играют особенную роль в проекте, поскольку определяют сам ход работ (или workflow). 2 Централизованные системы контроля версий Классическим примером подобных программ в мире открытого софта являются CVS и ее потомок — Subversion (лозунг проекта — «CVS the right way»); проприетарные аналоги: Perforce или Clearcase. Эти системы строятся вокруг централизованной модели разработки, в которой существует единственный удаленный репозитарий, в который вносят изменения все разработчики проекта. Ветвление (branching) проекта возможно, но не желательно и приносит, как правило, только дополнительные сложности в проект. Стандартный ход разработки выглядит примерно следующим образом: выкачивание из репозитария последней версии; разработка новой функциональности или исправление ошибок; повторное обращение к репозитарию для разрешения возможных конфликтов с работой других разработчиков; закачивание очередной ревизии на сервер. Соответствующие команды: svn checkout (забрать последнюю версию), svn resolve (показать, что конфликт в исходном коде разрешен) и svn commit (создать в репозитарии очередную ревизию). Подобный линейный подход к разработке прост и очевиден, но здорово ограничивает программиста. А что, если на любой из стадий цикла требуется отвлечься на другой функционал? Или срочно исправить какой-либо баг в предыдущей работе? Существуют разные выходы из ситуации. Можно проявить твердость, и закончить текущую работу, чтобы потом обратиться к следующим этапам; или, как вариант, нагружать текущий коммит большим количеством изменений. В первом случае не обеспечивается достаточная гибкость; во втором — усложняется поиск ошибки в коммите, нарушается логическая цельность действия. Откатиться к начальному состоянию невозможно — значит потерять уже проведенную работу. Ну или, если совсем усложнить, завести отдельную копию проекта, в ней исправить ошибку (внести функционал), закоммитить, затем стереть копию, и вернуться к прежней работе… Сложно? Сложно. Не выход, иными словам. Кроме того, иногда требуется работать без доступа к центральному репозитарию (удаленная работа, поездка и т.д. и т.п.). Что делать? Лишаться всякой гибкости разработки и заливать монстроидальный коммит весом в неделю? 3 Распределенный подход Решением подобных проблем явилась альтернативная схема разработки, предлагаемая так называемыми распределенными системами контроля версий (Distributed Version Control System). Среди открытых разработок на данную тему можно вспомнить git, Mercurial и Bazaar. Первый проект особенно интересен, он используется в некоторых из сложнейших современных программных систем(Linux Kernel, Qt, Wine, Gnome, Samba и многие другие), крайне быстро работает с любым объемом кода и сейчас набирает популярность в открытом мире. Какое-то время на распространении этой программы негативно сказывался недостаток документации; но сейчас этот недостаток можно считать устраненным. Итак, в чем заключается глобальное отличие git (и DVCS вообще) от централизованных аналогов? Во-первых, как следует из самого названия, не существует главного (в том смысле, который его понимают разработчики, привыкшие к SVN) репозитария. У каждого разработчика имеется собственный полноценный репозитарий, с которым и ведется работа; периодически проводится синхронизация работы с (чисто условно!) центральным репозитарием. Во вторых, операции ветвления и слияния веток (merging) ставятся во главу угла при работе программиста, и поэтому они очень легковесны. Кстати говоря, привычных ревизий также не существует; но об этом — чуть позже. 4 workflow в одну морду Итак, рассмотрим простейший случай: личный проект, в котором участвует единственное одно лицо — вы. Для создания нового репозитария достаточно просто зайти в папку проекта и набрать: git init Был создан пустой репозитарий — папка .git в корне проекта, в которой и будет собираться вся информация о дальнейшей работе (и никаких некрасивых .svn, разбросанных по дереву проекта!). Предположим, уже существует несколько файлов, и их требуется проиндексировать командой git add: git add . Внесем изменения в репозитарий: git commit -m \"Первоначальный коммит\" Готово! Имеется готовый репозитарий с единственной веткой. Допустим, потребовалось разработать какой-то новый функционал. Для этого создадим новую ветку: git branch new-feature И переключимся на нее (обратите внимание на отличие в терминологии по сравнению с SVN): git checkout new-feature Вносим необходимые изменения, после чего смотрим на них, индексируем и коммитимся: git status git add . git commit -m \"new feature added\" Теперь у нас есть две ветки, одна из которых (master) является условно (технически же ничем не отличается) основной. Переключаемся на нее и включаем изменения: git checkout master git merge new-feature Легко и быстро, не находите? Веток может быть неограниченное количество, из них можно создавать патчи, определять diff с любым из совершенных коммитов. Теперь предположим, что во время работы выясняется: нашелся небольшой баг, требующий срочного внимания. Есть два варианта действий в таком случае. Первый состоит из создания новой ветки, переключения в нее, слияния с основой… Второй — команда git stash. Она сохраняет все изменения по сравнению с последним коммитом во временной ветке и сбрасывает состояние кода до исходного: git stash Исправляем баг и накладываем поверх произведенные до того действия (проводим слияние с веткой stash): git stash apply Вот и все. Очень удобно. На самом деле таких «заначек» (stash) может быть сколько угодно; они просто нумеруются. При такой работе появляется необычная гибкость; но среди всех этих веточек теряется понятие ревизии, характерное для линейных моделей разработки. Вместо этого каждый из коммитов (строго говоря, каждый из объектов в репозитарии) однозначно определяется хэшем. Естественно, это несколько неудобно для восприятия, поэтому разумно использовать механизм тэгов для того, чтобы выделять ключевые коммиты: git tag просто именует последний коммит; git tag -a также дает имя коммиту, и добавляет возможность оставить какие-либо комментарии (аннотацию). По этим тегам можно будет в дальнейшем обращаться к истории разработки. Плюсы такой системы очевидны! Вы получаете возможность колдовать с кодом как душе угодно, а не как диктует система контроля версий: разрабатывать параллельно несколько «фишек» в собственных веточках, исправлять баги, чтобы затем все это дело сливать в единую кашу главной ветки. Замечательно быстро создаются, удаляются или копируются куда угодно папочки .git с репозитарием, не в пример SVN. Гораздо удобней такую легковесную систему использовать для хранения версий документов, файлов настроек и т.д, и т.п. К примеру, настройки и плагины для Емакса я храню в директории ~/site-lisp, и держу в том же месте репозитарий; и у меня есть две ветки: work и home; иногда бывает удобно похожим образом управлять настройками в /etc. Естественно, что каждый из моих личных проектов тоже находит под управлением git. 5 Общественные репозитарии Общественный репозитарий — способ обмениваться кодом в проектах, где участвует больше двух человек. Лично я использую сайт github.com, настолько удобный, что многие начинают из-за него пользоваться git. Итак, создаем у себя копию удаленного репозитария: git clone git://github.com/username/project.git master Команда создала у вас репозитарий, и внесла туда копию ветки master проекта project. Теперь можно начинать работу. Создадим новую ветку, внесем в нее изменения, закоммитимся: git branch new-feature edit README git add . git commit -m \"Added a super feature\" Перейдем в основную ветку, заберем последние изменения в проекте, и попробуем добавить новую фишку в проект: git checkout master git pull git merge new-feature Если не было неразрешенных конфликтов, то коммит слияния готов. Команда git pull использует так называемую удаленную ветку (remote branch), создаваемую при клонировании удаленного репозитария. Из нее она извлекает последние изменения и проводит слияние с активной веткой. Теперь остается только занести изменения в центральный (условно) репозитарий: git push Нельзя не оценить всю гибкость, предоставляемую таким средством. Можно вести несколько веток, отсылать только определенную, жонглировать коммитами как угодно. В принципе, никто не мешает разработать альтернативную модель разработки. Например, использовать иерархическую систему репозитариев, когда «младшие» разработчики делают коммиты в промежуточные репозитарии, где те проходят проверку у «старших» программистов и только потом попадают в главную ветку центрального репозитария проекта. При работе в парах возможно использовать симметричную схему работы. Каждый разработчик ведет по два репозитария: рабочий и общественный. Первый используется в работе непосредственно, второй же, доступный извне, только для обмена уже законченным кодом. 6 Заключение Я старался не акцентировать внимание на всех тонкостях использования главных команд, тем более что почти наверняка не знаю их все, да активно использовать git начал сравнительно недавно; но хотел продемонстрировать именно простоту и гибкость этого замечательного средства разработки. Также не стремился объяснить особенности внутренней механики проекта; хотя бы даже потому, что изящность и красота реализации заслуживает как минимум еще одного хабратопика. Естественно, приветствуются указания на фактические ошибки, подсказки насчет использования системы и общие замечания. :) UPD: разжился кармой и перенес в блог Git UPD2: продолжение заметки, несколько неформатное для Хабра :)"], "hab": ["Git"]}{"url": "https://habrahabr.ru/post/321446/", "title": ["Каково это — быть разработчиком в России, когда тебе сорок"], "text": ["Привет всем, я — сорокалетний программист-самоучка, а это моя история. Пару недель назад я наткнулся на график распределения людей, интересующихся технологиями, ИТ и программированием. И он заставил меня задуматься о моей карьере. Через каких-то 20 лет мне стукнет 60. И вероятность того, что я еще смогу заниматься тем, для чего был создан, составляет очень крошечную величину. Эти размышления привели меня туда, откуда все начиналось. Я дебютировал в роли разработчика программного обеспечения в 1990 году, через год после того, как мне на 14-тилетие родители подарили ПЭВМ «Микроша». Мир в 1990 году Тогда он был немного другим. Слова «Интернет» в СССР мы тогда еще не знали. У родителей на производстве использовались ЕС ЭВМ, которые выглядели так: А жесткие диски выглядели вот так: Карманные компьютеры (КПК) Palm тогда еще не существовали в природе, и их место занимали микрокалькуляторы «Электроника» всевозможных моделей. О программируемом калькуляторе можно было только мечтать, цены на них были заоблачными: В прокате отечественного кинематографа крутили «Кин-Дза-Дза», Алиса Селезнёва регулярно боролась с космическими пиратами на каналах центрального телевидения, и в этот год вышел стильный трех-серийный фантастический фильм «Посредник»: Холодная война завершалась, в космосе царил мир, а за посадкой в беспилотном режиме космического челнока «Буран» я годом ранее наблюдал по новостям в прямом эфире: Да, беспилотная посадка многотонного объекта с аэродинамикой утюга — это было реально круто! Мой первый опыт в качестве разработчика Как утверждает А. В. Столяров в своей книге «Программирование: введение в профессию», программистом человек становится тогда, когда его программа становится нужна другим людям, и они готовы заплатить за нее деньги. Благодаря прекрасной документации на компьютер «Микроша», написанной в конструкторском бюро «Лианозово» (которую я храню до сих пор как образец составления руководства пользователя), я за полгода освоил Бейсик, успел в нем разочароваться с точки зрения скорости выполнения программ, и начал писать свою первую игру на Ассемблере. И рядом со мной никого не было, кто разбирался бы в программировании и компьютерах. В соседнем дворе жил школьный товарищ, которому родители подарили компьютер «Партнер 01.01». Компьютер у него лежал в коробке, а товарищ не знал как подключить его к магнитофону, чтоб загружать программы. Я пришел к нему, наладил, и мы стали поочередно ходить друг к другу в гости, чтобы поиграть. Игр было немного, и к тому же между нашими ЭВМ была небольшая аппаратно-программная несовместимость вплоть до формата записи на магнитную ленту. Поэтому загрузить программы от одного компьютера в другой мы не могли. Товарища в компьютерах интересовали только игры, и вот когда все игры были переиграны, я ему показал, как я пишу игру для себя. Это была вертикальная скроллер-гонка на Ассемблере. Он загорелся этой игрой, и попросил сделать ему такую же. Я объяснил, что дело будет долгим: придется вручную перенабирать на его компе длинный листинг Ассемблерной программы, или вбивать килобайты HEX-кода, поправляя в них адреса процедур и портов, которые на наших компьютерах различались. В конце концов, я пришел к нему вечером, и написал за один присест аналог своей игры, но только на Бейсике. В игре была дорога, машинка из буквы «Ж» и бревна из символов решетки #####. Чем дольше шла игра, тем уже становилась дорога, длиннее бревна и выше скорость. В углу щелкал счетчик километров. Сейчас такую игру назвали бы аркадой со случайно генерируемым ландшафтом. Написав игру, я ушел домой, сказав, что 100 км в ней проехать невозможно. Через три-четыре дня позвонил довольный товарищ, и сказал, что смог проехать больше. Оказывается, он круглыми сутками рубался в эту нехитрую игру, и в какой-то момент бревна на дороге разместились так, что можно было проехать. Потом он позвал к себе еще пацанов, и они рубались в нее не прекращая. Родителям это не нравилось, и они обмолвились, что надо брать деньги за игру на компьютере, как это делали кооперативщики в игровом Спектрум-клубе. Поток желающих резко снизился, но какие-то деньги он успел заработать. Половину денег он отдал мне, сказав, что если бы не моя игра, он бы вообще не заработал ничего. Так я стал разработчиком, удовлетворяя всем условиям критерия Столярова. Высшая школа В институт, с точки зрения программирования, я шел неплохо подготовленным. Не у каждого за плечами было несколько лет программирования на языке высокого и низкого уровня плюс сгоревший компьютер, который не выдержал своего превращения в голосовой автоответчик в связке с магнитофоном «Ореанда 204-C». «Программистских» специальностей в нашем ВУЗе не было, но была кафедра микроэлектроники и полупроводниковых приборов. Кафедра создавалась для радиозавода, который был построен на окраине города. В год, когда я начал учиться, персоналу перестали выплачивать заработную плату и завод закрыли. В двух группах моей специальности оказалось пятеро человек, у которых на момент поступления был домашний компьютер, включая меня. После окончания института только эти люди и еще пара человек из нашего потока связала свою жизнь с информационными технологиями. Я считаю, что такой выхлоп — это ужасно. В институте нам преподавали язык Си. С этим императивным языком у меня не было проблем, за исключением указателей. Зная Ассемблер КР580ВМ80А, и успев поковыряться с i8086, я недоумевал, как можно было так уродливо сделать такую простую вещь, и почему эту уродскую реализацию еще и приняли в стандарт. Но я заставил себя воспринять это как должное. В последний семестр курса по программированию нам номинально прочитали какой-то сумбур по Си++. Этот язык «преподавал» человек, который вообще в нем не разбирался. Лекции он читал по бумажке. Такое положение дел было нормой тогда, и, насколько понимаю, остается нормой и по сей день. «Концепция объектно-ориентированной парадигмы предельно проста. Каждый объект в системе представляет собой инкапсулированную абстракцию, наделенную свойствами наследования и полиморфизма» — вот что мы заучивали на лекциях. В итоге, на практическом экзамене по С++ я был единственным человеком, который писал решение на этом языке. Остальные писали на C. Я подготавливался дома, днями мучая плюсовую часть компилятора TurboC, чтобы заставить компилироваться одну-единственную программу в ООП-стиле на 300 строк. Это было дело принципа: если я учу Си++, то я должен понять, как написать и скомпилировать хотя бы одну программу. Это было трудно, неудобно, и вызывало недоумение: зачем придуман такой извращенный язык? Зачем защищать данные внутри программы от самой себя? В конце концов я механически запомнил конструкции языка и их совокупность, образующие костяк программы, которые «хотя бы работали». А на экзамене просто это повторил. Преподаватель, как и я, был в трансе, когда увидел скомпилированную на экзамене Си++ программу (это были часы, в которых циферблат и стрелки были отдельными объектами). Я тогда дал себе слово, что никогда больше не притронусь к Си++. И рядом со мной никого не было, кто разбирался бы в ООП и в программировании на плюсах, и сказал бы: товарищ, ничего тут сложного нет. Учась в институте, я стал подрабатывать, чтобы помогать родителям. Шли девяностые годы, и каждая копейка была на счету. Первый серьезный заказ был от сокурсника, отец которого был директором собственного предприятия. Им требовалась программа для учета горюче-смазочных материалов. Идея заработать на своих умениях программировать была заманчива, и я согласился. Хотя, положа руку на сердце, какое мне дело до горюче-смазочных материалов? Мне хотелось разобраться как кодить 3D на голом процессоре и играться с вокселями. Но я понимал, что за это платят где-то далеко, но не у нас. Поэтому я краем глаза смотрел на базы данных, и к тому моменту уже полгода экспериментировал с FoxPro. Я пришел в контору предприятия, и стал выяснять, что же им таки нужно. Говоря языком разработчика, я собирал историю пользователя. Мне показали какую-то бухгалтерскую систему, и рассказали свои проблемы. Проблема была в том, что они вели суммовой (денежный) учет, а им нужен был количественный, с детализацией по потребителям и местам хранения. Это сейчас я так точно говорю, в чем была проблема. А тогда на меня вылили полную кучу взаимоисключающих параграфов и выразили надежду, что я смогу в этой каше разобраться. Я записал себе все, что успел понять, взял месяц на разработку, и сделал эту небольшую учетную систему. За день до передачи заказчику, у меня щелкнула мысль: а что, если мне за этот вынос мозга не заплатят? Скажут, например, что программа не подошла, и откажутся платить. А потом будут использовать. Или не будут. Но я-то ночи не спал, измотался вконец. Я сидел, думал-думал, и наконец решил: вставлю защиту по времени. Если мне не заплатят в течение месяца, программа перестанет работать. Если ее никто не будет использовать, то об этом никто не узнает. А если заплатят и станут использовать, то я после получения денег скажу, что в каталоге с программой надо просто удалить файлик с определенным именем. И я сделал защиту. На FoxPro сделать защиту — это та еще проблема. На тот момент уже существовало два декомпилятора, которые практически один-в-один восстанавливали исходный код. Но я, рассудив, что вряд ли заказчик станет искать человека, который сломает защиту, немного обфусцировал код, скрыв от посторонних глаз вызов системной ошибки. На следующий день я отдал программу, показал как в ней работать, но обещанных денег не получил. Сумма была небольшая — уговор был о деньгах в размере месячной стипендии. Но мне сказали, что выплатят через неделю, когда на предприятии будут выдавать зарплату. Что произошло через неделю? Правильно, денег мне не выплатили, сославшись на то, что в этом месяце фирма мало заработала, и вот именно на меня не хватило. Я внутренне похвалил себя за свою предусмотрительность. Даже если я останусь без денег, то хотя бы буду удовлетворен тем, что в один прекрасный момент программа (без моего вмешательства!) откажется работать с нехорошими людьми. И я больше не стал о себе напоминать. Нет — значит нет. А дальше все произошло как по нотам. Окунувшись в учебу, я уже и забыл про свою программу. Я был дома, и вдруг зазвонил телефон. Звонил мой сокурсник, отец которого заказал программу. Он сказал, что моя программа перестала работать, а в ней уже накопились данные и недельные отчеты за целый месяц. Я скромно напомнил, что мне за программу ничего не заплатили. И спокойно сообщил, что делать категорически ничего не буду, пока не увижу денег. Сокурсник очень обиделся, сказав что не ожидал от меня такой подставы. Но через два часа привез деньги, а я ему сообщил, какой файл надо удалить. Больше я с ним дел не имел. Этот эпизод глубоко убедил меня в том, что лучшие друзья девушек — это алмазы лучшие друзья программиста — это его собственные программы. Они не подведут, и всегда будут работать так, как задумано. И единственное, на что я могу в жизни опереться (помимо родителей, конечно) — это мои собственные программы. Они меня — как люди — не подведут. А для того, чтобы они не подводили, программы надо писать хорошо. Завершение учебы В конце учебы, на пятом курсе, я одновременно подрабатывал на трех работах. Вообще, с работой тогда было туго: даже взрослые люди сидели на мели. Родители мои были из тех, о ком упоминалось в смешном анекдоте: мама — врач, а папа — инженер. Устроить они меня могли только в своей сфере деятельности. У отца на заводе с ЗП было полностью глухо: завод стоял и разворовывался. Поэтому в трудоустройстве мне помогала мать. Работа раз Три раза в неделю я из города ездил в деревню, в центральную районную больницу, где работал на должности «тыжпрограммиста». Да, уже тогда существовал мем «тыжпрограммист». Поддержка бухгалтерии, расчет зарплаты, подача отчетности в налоговую и пенсионнный фонд, медицинская статистика и социальное страхование. Что-то досталось по-наследству от предыдущего работника. Расчет зарплаты приходилось дописывать в соответствии с меняющимся законодательством. Медстатистика была написана с нуля. FoxPro, Clarion, Си для мелких утилит. Локальных компьютерных сетей мы тогда еще в глаза не видели, и я делал распайку навесного LPT-порта, чтобы соединить компьютеры на двух этажах через LPT-линк в Norton Commander. Хотя, какое мне дело было до бухгалтерии, статистики и отчетности в налоговую? Я ночами разбирался с CORDIC-алгоритмами, адаптируя чудом найденный листинг мотороловского ассемблера на i8086. Я добрался до BUMP-маппинга в реальном времени на микропроцессоре i386. Я открыл для себя FIDO, а через него и и демосцену. Ассемблер, сплошной Ассемблер, и микроскопические размеры программ. Первые свои intro я делал для своих фидошных нод. А рядом со мной не было никого, кто бы разбирался в этом низкоуровневом программировании графики и звука. Работа два Еще два раза в неделю я работал в мастерской «Медтехника». Выработка там считалась по починенным приборам. Мне отдавали всякую рухлядь, с которой никто не хотел возиться. Есть такие типы поломок, когда прибор то работает, то нет. Диагностировать проблему в таких условиях очень сложно. Лучше бы он просто сгорел и не работал, тогда было бы сразу ясно что чинить. Опыта у меня не было, и вообще я себя считаю рукожопом в электронике, посему мучился я изрядно. В какой-то момент владельцу мастерской пришла на ум бизнес-идея закупать оптом фотопленку для рентгенографии большого формата, и нарезать из нее малые форматы. С малыми форматами почему-то везде была проблема, медучреждениям их негде было достать. А чтобы продавать пленку, нужна была упаковка. Были куплены два километровых рулона светонепроницаемой бумаги для внутреннего и внешнего конверта. Сварили опоры для рулонов, поставили металлический стол. Получился импровизированный станок. В этом деле я не участвовал, только наблюдал со стороны. Я видел, как директор со своим замом сами резали бумагу, пытаясь придумать ручную технологию быстрого изготовления конверта. Они стояли с секундомером, и замеряли, сколько времени у них уходит на один конверт. Но как они делали этот конверт? Меня поразило, что они не стали разбивать технологические операции резки, сворачивания и склейки на этапы, а делали разную работу последовательно для каждого конверта. Резку они делали канцелярским ножом под металлическую линейку. При разметке листа они крутили линейку и так и эдак, и на это уходило много времени. Я не выдержал, подошел, и поинтересовался, почему они не нанесут разметку прямо на стол? Это предложение их поразило. Как просто! Они поинтересовались, что еще можно сделать. Я рассказал им выжимку об организации технологических процессов, и внес несколько изменений в станок. И кого, в итоге, поставили резать бумагу? Нет, не таджика с улицы. Я резал бумагу месяц, постепенно понимая, что тупею от такой работы. Через месяц кончилась фотопленка, и я снова начал чинить приборы. Один раз я ошибся, и включил трансформатор без сердечника. Трансформатор сгорел, и его пришлось перематывать. Стоимость потраченной медной проволоки вычли из моей и так мизерной зарплаты. Меня это доконало окончательно, и я с медтехникой завязал. Работа три Третье рабочее место. Сутки-через-три сторожем в детской поликлинике. Прекрасная работа, чтобы почитать книжки. Если бы у меня была возможность притащить компьютер на эту работу, я хотя бы там писал свои программы. «Так взял бы ноутбук!» — скажет современный читатель. Но о ноутбуке я не мог и мечтать. У меня была еще свежа в памяти поездка с отцом в командировку в Москву на ВДНХ, где я увидел в торговом павильоне цены на ноутбуки, которые были выше цены автомобиля. Я тогда ходил потрясенный, и все никак не мог понять: неужели находятся люди, которые готовы купить ноутбук за ТАКУЮ цену? И рядом со мной не было человека, который бы сказал: чувак, в нормальных конторах такие ноутбуки выдают для работы бесплатно! Свободный полет В 1998 году я закончил институт. Диплом с отличием на руках. У меня две смешные работы со смешными зарплатами. На Кавказе вторая чеченская война, и я призывник. В аспирантуру не взяли — туда только по великому блату, потому что бронь от армейки. Очень хорошая перспектива — 15 лет учиться, получить диплом с отличием, и со всем багажом знаний отправляться на реальную войну. Но благодаря дырявому законодательству, я стал официальным единственным опекуном, и получил отсрочку. Одна проблема была решена. Однако была вторая проблема: отсутствие приличной работы. И была третья проблема: в стране случился дефолт. То есть, до дефолта я думал, что хуже быть не может. А при дефолте узнал, что может. На работу ходить не имело никакого смысла: заработанных денег хватало только на проезд до работы. Оставался только один вариант: понаехать в Нерезиновую, а там — как кривая выведет. Некоторые родственники новоиспеченных инженеров уже перебрались в Москву: работали на стройках, на рынках, в электричках. В тот момент устроиться по специальности в Москве не было никакой возможности: без прописки Москва-Подмосковье на работу не брали. По сути, человек был нелегалом в столице своей родины. Кусочек лета, с осени до весны я бегал по московским электричкам, торгуя копеешным товаром: отрава для тараканов, стельки, некондиционные фломастеры, книги, чай из пыли с чайных фабрик, просроченное кофе, ножи для мясорубок, прочая неимоверная дребедень. По выходным торговал на рынке радиотелефонами. Параллельно пытался найти настоящую работу, но везде был отказ по причине отсутствия прописки. Один раз, казалось, нашел компанию, которая могла бы меня взять на работу. Во всяком случае, в требованиях не было упоминаний о прописке, а при звонке менеджер ничего не знал о том, нужна прописка или нет. Контора занималась организацией бухгалтерского/складского учета, внедряла 1С, и им нужны были расторопные люди. Прием на конкурсной основе. Я пришел на собеседование, написал тест, и из толпы в сколько-то десятков человек осталось только двое. Далее было личное собеседование, на котором состоялся эпичный диалог: — Вы нам подходите. И последний вопрос: вы где живете? — В Пушкино. (я снимал там угол). — Прописка Подмосковье? — Нет. Юг России. — Ну что ж вы так… Конкурс вы не прошли! Конкурс, мать, не прошел. Кровь сдал, мочу сдал, кал сдал, на математике завалился. Один мимолетный положительный момент пребывания в Первопрестольной все же был: я смог попасть на фестиваль компьютерного искусства ByteFall-99. Он тоже по причине кризиса переносился, но в конце концов был проведен. Я выставил там свое интро, которое написал еще в институте. Доступа к компьютерным сетям Интернет/Fido у меня не было, поэтому заветную дискетку пришлось вести организаторам ногами. Организатор работал админом в Христианской миссии — какие-то католики или протестанты, я в них не разбираюсь. Он фанател по Линуксу, и тогда я на полчаса залип — я впервые увидел линукс в реальной работе. Весной я и мой напарник поняли, что в Москве ловить нечего. Вечная беготня от милиции, взятки, обезьянники, терки с конкурирующими «фирмами» в вагонах электричек. Жизнь в общежитии с тараканами и общим душем. Мы планировали вернуться домой и заняться натуральным хозяйством — тема пасеки была актуальной. Но по возвращению напарника забрали в армию, и с пасекой не сложилось. Легальная работа 1999 г. Начало лета. Я сидел, и с удивлением смотрел на свои руки. Пока я был в Москве они стали деревянными. Пальцы забыли, где находятся буквы на клавиатуре. Куда девались мои неторопливые 250 симв./мин.? Странная игра мозга: иногда вспоминаю раскладку JCUKEN своего первого компьютера, хотя за годы учебы и работы с PC уже мог бы привыкнуть к QWERTY. Чем может заняться инженер кафедры микроэлектроники в городе, где он никому не нужен? По сути, только сопровождением компьютерных учетных систем. И я пошел работать в государственные налоговые органы. Мне сразу дали должность ведущего специалиста. Но зарплата… Она рассчитывалась по каким-то древним нормам, и была чисто символической. И честно говоря, какое мне дело было до тонкостей расчета подоходного налога на физических лиц с поправками от ДД.ММ.ГГГГ? Я бредил искусственным интеллектом и экспертными системами. Чтобы совсем не закиснуть на работе, я ночами писал виртуального игрока на основе дихотомайзера для игры, теории которой я нигде не мог найти. Экспертная система на то и экспертная, что ей можно формализовать те закономерности, которые разработчик сам не знает, и для этого достаточно обучить её на образцах «правильной» игры. И когда мой электронный игрок обыграл меня, я был просто счастлив. Игра называлась «Акционер». Я и мой товарищ, писавший GUI, планировали продавать игру. Но из этого ничего не получилось. В конце концов мы выложили игру в свободный доступ. Спустя 15 лет, со мной связался автор, который публиковал правила этой игры в журнале «Наука и жизнь». Он написал, что он нашел нашу игру, и несколько раз играл «в ничью». Это было круто. Прошел год. Страна стала оправляться от дефолта. Стал восстанавливаться наш градообразующий машиностроительный завод. Ну как восстанавливаться… Гости с туманного Альбиона, купившие по дешевке советский завод атомного машиностроения, решили начать извлекать из него прибыль не только путем продажи оборудования на металлолом. Через знакомых я случайно узнал, что на заводе формируется инжиниринговый центр, куда набираются специалисты. Плевать, что не по моему профилю, но хотя бы зарплата будет взрослая. В конце концов, в приложении к диплому, в графе инженерного конструирования стояло «отлично». Я схватил документы, прошел собеседование, устроился в штат, и через неделю уже ехал в командировку в Питер. Зарплата, командировочные — можно копить на жилье. Хотя, какое мне дело было до теплообменного оборудования, трубных решеток, фланцев и компенсаторов? Нейронные сети, обучающие выборки, распознавание образов — вот интересная тема! А что, если нейросетку применить к кодированию видео? Черт, MPEG-4 именно это и делает… Из инженера-конструктора я переквалифицировался в инженера-расчетчика с уклоном на расчеты потоков жидкостей и газов. Основные инструменты — MathCad для аналитических решений, UniGraphics для геометрии, AnSys и StarCd — для конечно-элементного анализа. В MathCad можно программировать — ура! В AnSys можно писать скрипты — прекрасно, будет параметрическая геометрия! Для StarCd нужен STEP-конвертор, спецификация есть — напишем! Ах да, ведь в мире еще есть Интернет, и кто-то пишет сайты. И все это крутится на Linux! Надо его поизучать. А что такое PHP 4-й версии? О боже, это скриптовый мультипарадигменный язык с классами! Неужели ООП может быть таким простым? В это время вернулся из армии мой товарищ, с которым я работал на электричках в Москве. Он хороший схемотехник, и мы решили вместо пасеки, параллельно с основной работой, пощупать бизнес по изготовлению светодиодных табло типа «бегущая строка». Лучше б мы занялись пасекой. Мы наивно полагали, что если сможем сделать недорогое и простое (в нашем понимании) изделие, то сможем обеспечить свое существование. Для начала решили сделать тестовую модель. Бегущая строка состояла из самодельной светодиодной матрицы и самодельной платы с микросхемой КР580ВВ55А, которая по LPT-порту вставлялась в материнку i386. Расчет был на то, что материнки с i386 к тому времени настолько устарели, что их можно было закупать килограммами за копейки. А нашему ПО большего и не требовалось. Я сделал несложный язык разметки для вывода сообщений, в котором были подгружаемые шрифты, анимированные спрайты, эффекты, мета-символы даты-времени, значения датчиков. И написал драйвер для вывода динамической картинки на наше устройство. Когда прототип был полностью собран и заработал, мы стали подсчитывать, сколько будет стоить конечное изделие. Оказалось, что самое дорогое — это изготовление матрицы с диодами и корпус. Те цены, которые озвучили местные предприятия за сверление ровных отверстий в пластике и изгиб металла, были какими-то неадекватными. Они в разы превышали цену готовой бегущей строки, которую можно было достать в Китае. Мы продали опытный образец по цене комплектующих, и на этом наше высокотехнологичное предпринимательство закончилось. Три года я занимался расчетами. Не сказать, что это дело из легких: большая ответственность, и каждый день как экзамен по физике. Задачи все время новые: то расчет напряжений, то тепло-массо перенос, то профилирование аэродинамики, то трубные пучки. Везде своя методика, везде свои особенности. В какой-то момент я понял, что меня не хватает на всё. Добил меня заказ на масляный теплообменник для корабельного оборудования. Морская тематика — это вообще отдельная песня. У неё свои нормы и требования, сильно отличающиеся от классического, наземного машиностроения. Я «куснул» задачку, но существующая методика расчета давала погрешность больше, чем степень оптимизации характеристик. Гуру расчетной группы пожали плечами: выкручивайся как хочешь, мы не знаем как считать. Вместо аналитического решения решил применить конечно-элементное. Но была загвоздка: масло сильно охлаждалось, и изменением вязкости нельзя было пренебрегать. Требовались расчеты с изменяемыми параметрами среды. В пиратском StarCd был требуемый функционал (да, тогда все ПО на производстве было пиратским), но в нем была ошибка: терялись коэффициенты расчетной формулы. И обойти это было никак нельзя. Я даже пытался вывести коэффициенты для эквивалентной формулы, где были бы скомпенсированы потерянные коэффициенты, но нет. По очень дальним связям, в каком-то московском институте я нашел человека, который занимался расчетами в этой программе. И он сказал, что проблема существует, и никак не решается в той версии ПО, которое у меня. Зато в более новых версиях проблема решена, вопрос только в покупке. Я сообщил об этом главному инженеру, но он только покрутил пальцем у виска: где это видано, чтобы программы покупали для производственных нужд? А что касается расчета — надо писать фиктивный, и закрывать сумму за выполненную работу. Я отказался, и после странного разговора «либо пишешь левый наукообразный расчет и ставишь подпись, либо валишь», уволился по собственному желанию. Куда идти теперь? Бывший институтский товарищ работал у самого крупного в городе владельца продовольственных магазинов. Тогда только появилась тема магазинов самообслуживания, забытая с советских времен: всю перестройку людей боялись пускать к товару, и вся торговля в городе была прилавочной, и, соответственно, медленной. Владелец очень боялся, что в город придет сеть «Магнит» и «Пятерочка». И было принято решение переводить магазины на формат торговых залов. Требовался человек для реализации этой идеи. За неимением лучшего предложения от работодателей, этим стал заниматься я. Хотя, какое мне дело было до электронных весов, сканеров штрих-кодов, проводок в 1С «Рарус» и кассовых модулей? Компьютерная графика и анимация! Что есть из самого доступного? Flash! VirtualDub и потоковый скриптинг на AVISynth! Что сделать, чтобы разобраться в технологии? Конечно, собственный клип! Днем — работа чтоб покушать. Ночью — творчество. За год, — страшно вспомнить, — я запустил четыре магазина самообслуживания. Они были первые в городе. На остальных отставали строительные работы. Наступило относительное спокойствие: приход Пятерочки с Магнитом откладывался. Что бы еще навешать на ИТ-специалиста, который сопровождает три маркета и четвертый с приставкой «супер»? Конечно, ревизию! Тыжпрограммист, считать умеешь, и пока работы немного (магазины ведь волшебные, и работают сами по себе), надо ночами считать товар. А если недостача? Ну давай посмотрим: магазины вверены тебе, учет ведется на компьютерах, значит кто отвечает за недостачи? Понятно? Проведя несколько ревизий, и видя, какие дела творятся в торговой сети, я твердо решил съезжать с этой темы. Но надо понимать, что лихие девяностые в глубинке продолжались и в двухтысячных. Владельцы крупных городских бизнесов — люди не простые, а многие вещи решаются по понятиям. Поэтому просто так с темы не съедешь. И именно в этот момент произошло невероятное: одному моему товарищу мама подарила квартиру в Москве, а сама вышла за муж и уехала в Италию. Он перебрался в Москву, и каким-то образом устроился на работу бета-тестером в контору, которая писала игры под Linux. Он рассказал про меня, и меня пригласили на собеседование. Я собрал все свои свободные деньги, и поехал в Москву. Устроился на работу, нашел квартиру, и сразу заплатил тройную сумму месячного проживания: агентству, залог владельцу, оплату первого месяца. До первой зарплаты жил на бутербродах. Работа по специальности 2005 год. Москва была совсем другой! Лужковские законы отменили, и можно было ходить в метро и по улицам спокойно, не боясь что товарищ милиционер вдруг заинтересуется твоей личностью. У работодателей наблюдается кадровый голод, и фирмы с удовольствием устраивают специалистов, не взирая на прописку. И трудоустройство, что удивительно, по КЗОТ! Съем жилья в Москве по-прежнему дорогое удовольствие, но шестикратный рост зарплаты делает это удовольствие позволительным. Фирма, в которую я устроился, делала игры под Linux. Звучит невероятно? Кому нужны игры под Linux? Оказывается, нужны. Например, если это игровые автоматы с азартными играми. Сфера такого бизнеса называется буржуйским словом «геймблинг». Помните «столбики» и залы игровых автоматов на деньги? Вот это оно. Для снижения издержек, и чтобы не заморачиваться с лицензированием, фирма вела разработку на Linux. SDL, OpenGl, png для графики, ogg для звука. Фирма занималась полным циклом производства. Отдел разработки (куда я и устроился), отдел проектирования электроники (к PC-материнкам подключались свои платы защиты и контроллеры оборудования), отдел контроля качества, производство корпусов, сборка готовой продукции, и прочие непроизводственные отделы. Когда я устроился в эту фирму, пик сверхдоходов уже прошел. В воздухе летали слухи о запрете игровых автоматов. Поэтому фирма потихоньку переориентировалась на заказы из-за рубежа. Я проработал в этой фирме год «на постоянке». Вывел в продакшен два проекта, которые делал практически «с нуля», но используя и дорабатывая фирменные библиотеки. Со мной, помимо C, C++ и Java программистов работали художники, аниматоры, музыканты, шрифтовики, тестеры. Мне это очень нравилось. Там действительно был коллектив, и я в кои-то веки не был специалистом-одиночкой. Исторически, игры собирались из Си и Си++ кода, потихоньку перебираясь полностью на плюсы. Приходилось писать и низкоуровневые вещи, и фронтэнд на OpenGL, и финансово-денежную часть, и тесты математики. Внешне все должно было выглядеть броско и красиво, целились на «верхний» сегмент. Трехмерные объекты вылизывались до идеала: качественные текстуры, полутени, движущиеся блики, размытие для эмуляции быстрого движения, сплайны для мягкой анимации, etc. Я в работе использовал принципы написания кода «от НАСА»: единоразовое размещение объекта/ресурса в памяти при старте, никакой динамической работы с кучей в процессе выполнения программы. Это давало свои плоды: ничего не тормозило, все работало плавно, так как все данные при старте были подготовлены и доступны. Конечно, без оптимизации самих данных это было бы невозможно. За счет такого подхода, игры работали месяцами не выключаясь. Были и другие методики: со мной работал студент, который писал для своих проектов свой собственный аллокатор с тройным уровнем указателей. На его код я смотрел с благоговейным страхом. Его проекты то работали, то сегфолтились, но он быстро все исправлял. Когда он уволился, то код этого специалиста пришлось переписывать с нуля, так как никто не мог потянуть его сопровождение. За все время работы я локализировал свои проекты под двенадцать стран: Страны бывшего СССР, Европа и латино-американские страны. Для некоторых стран приходилось проходить сертификацию, чтобы пустили на рынок. Требования были нетривиальные, больше всего извозился с Чехией и Перу — одной условной компиляцией было не отделаться. Но работать было интересно! Хотя, положа руку на сердце, меня грызла совесть: все-таки делал я азартные игры. За азартом всегда стоял полукриминальный бизнес, а я делал для него услугу. И мне это очень не нравилось. Поэтому, чтобы хоть немного выправить карму, я стал подумывать о том, чтобы запустить свой собственный OpenSource проект. На благо общества, так сказать. Я ходил, и обдумывал возможные полезные проекты. И уже были некоторые намётки как вдруг приняли закон о регуляции игровой деятельности. И фирма, в которой я работал, стала делать резкое пике, т. к. большая часть доходов все-таки была с российского рынка. Производство стали оптимизировать, наметился переезд в более дешевый офис. Я договорился на приличную ЗП по меркам моего города, что для работодателя оказалось выгодно, и уехал обратно. И несколько лет продолжал работать удаленно. Личный опыт показывал, что Москва — это город для работы, но никак не для жизни. Удаленная работа — зеркало фриланса На удаленке я столкнулся с теми же самыми проблемами, с которыми сталкиваются фрилансеры. Самой большой проблемой, помимо нарушения суточного ритма, стало отсутствие общения с коллегами. Скайп, email, багтрекер, система контроля версий — это все хорошо, но не заменит живого общения с себе подобными. Невозможно просто так обсудить ту или иную идею или технологию. Невозможно узнать и понять что-нибудь новое просто так, с объяснениями по ходу дела на коленке. И это является самым большим тормозом в развитии специалиста. Чтобы оставаться «в теме», я учитывался Хабаром и Лором, при этом понимая, что это занятие не приведет меня к знаниям такого же уровня, которые я получил в «доинтернетовскую» эпоху. Интернет формирует очень мозаичную картину мира, и кажется, что уже исчезли люди, способные объяснять последовательно, подробно и доступно. С книгами тоже дела обстоят неважно: в силу многих причин я не могу «глубоко» читать с экрана. Поэтому всегда покупаю бумажные книжки. Но с ними проблема: большинство современных книг — это откровенный шлак. А те книги, которые действительно нужны, стали раритетом и в бумажном виде не продаются. В общем, человечество вступило в фазу, когда неоткуда взять глубокие знания, а вместо них подсовывается суррогат в виде бесконечных статеек людей, которые «наконец то все поняли», призывов обучиться на курсах «от какой-нибудь большой корпорации», рассуждений о прелестях удаленного обучения и прохождения открытых курсов в иностранных университетах на английском языке. Я решил, что только работа, опыт, и проекты с применением новых технологий удержат меня от выпадения из индустрии. И OpenSource казался тогда хорошим выходом. У меня было несколько проектов, которые я мог выставить на всеобщее обозрение, после приведения кода в более-менее приличный вид. Я выбрал один из них, написанный на C++ с Qt, и параллельно с удаленкой стал пилить проект для людей. Я очень рассчитывал, что появятся люди, которым проект интересен, и сформируется хотя бы небольшой костяк команды разработчиков. Однако, чуда не произошло: периодически появлялись люди (и я им очень благодарен), которые точечно помогали, если я не мог с чем-нибудь разобраться, но «на постоянку» не нашлось никого. Я тянул проект один, и продолжаю это делать сейчас. Соответственно, никакого обмена опыта в рамках команды я не получил, по причине отсутствия таковой. (Ремарка: после публикации на Хабре несколько человек стали коммитить в репозитарий проекта. Но теперь у меня нет времени чтобы разгрести эти коммиты и продолжить групповую разработку). Что касается основной работы по удаленке, то спустя пять лет произошло то, что и должно было произойти. В современном мире жизненный цикл разработки ПО примерно 5-6 лет в самом оптимистичном случае. Далее, без кардинального внедрения новых технологий (что ведет за собой кардинальную переделку всего проекта), проект будет постепенно распадаться, пока не загниет окончательно. На фирме это понимали, и затеяли переезд всей инфраструктуры на новые рельсы. Участвовать в такой крупной переделке удаленно не представляло никакой возможности. Требовалось либо личное присутствие, либо нужно было увольняться. Я только-только обзавелся жильем, и затевать новый переезд, причем на этот раз не одному а с семьей, не было ни возможности, ни желания. Просто работа 2011 год. Ну что же, вот я и приплыл. Теперь вариантов немного: либо фриланс, либо веб-студия, либо местное производственное предприятие. 1. Фриланс в области разработки — это очень специфичная вещь. Я периодически делал несколько заказов в режиме фриланса, и знаю, что это жуткий вынос мозга. Обычно, все происходит по одному и тому же сценарию: пытаясь сэкономить заказчик находит каких-то мутных исполнителей, которые что-то пилят примерно до момента «что-то начинает работать». Исполнители получают какие-то деньги, и исчезают по самым фантасмагорическим причинам. Заказчик до последнего пытается добиться от исполнителей завершения проекта, но нет. В результате все сроки вышли, бюджета нет, и заказчик начинает судорожно искать кого-нибудь на фриланс-биржах или по знакомым, кто разберется в том, что было сделано, и «немного допишет», ведь, по его мнению, гигантская часть работы уже выполнена, уже «почти все работает». Или другой вечный сюжет: сделайте мне аналог VK за месяц, плачу 8 тысяч рублей. В общем, об этом можно долго говорить, но по моему опыту, найти адекватного заказчика в России очень сложно. Самое лучшее, на что можно рассчитывать — это выполнение кучи мелких заказов за небольшие деньги. Адекватности в них больше, но их количество обычно ограничено. Как временный заработок такая работа может быть и имеет право на жизнь, но постоянно такими вещами заниматься несерьезно. 2. Веб-студия, по моему мнению, — это путь в никуда. Можно прокачать свои скиллы в нескольких CMS, разобраться с парой веб-фреймверков. Заточиться на PHP, присматриваясь к Питону и Ноде. Ну и дальше что? Бесконечное клепание сайтов, постоянный поиск заказчиков, ибо работа разовая. В нашем городе есть ровно одна веб-студия с приходящим веб-программистом. Есть пара предпринимателей-фрилансеров, работающих за еду. Кто-то скажет, что Интернет большой. Да, так и есть, но смотри пункт первый про фриланс. Кроме того, коммерческими сайтами заниматься весьма скучно. В общем, никаких перспектив в мире разработки для меня не осталось. Да, я очень, очень хотел развиваться как разработчик, но разработчики в моем окружении никому не нужны. По факту, они нужны в нескольких крупных городах: Москва, Питер, Новосибирск. А если посмотрим на более другие поселения, то там уже все гораздо грустнее. Вот, например, Пермь, 2016 г. (страшно подумать, какие зарплаты были в 2011 г.): Эти зарплаты — не насмешка, это всё на полном серьёзе. Но в вышеприведенном объявлении хотя бы есть работа. В поисках предложений я прошерстил hh.ru с фильтром по региону. Ближайшая работа разработчиком — через 250 км. При всех прикидках вырисовывалась следующая картина, которую мозг постоянно отпихивал как неприемлемую: хочешь быть разработчиком — уезжай в большой город или меняй страну. Не хочешь уезжать — ломай себя и меняй сферу интересов. Непривлекательная альтернатива. Я сидел, думал, и решил мыслить шире. Хорошо, с разработкой не сложилось. Но мы должны уважать себя, и не выбрасывать свои знания, а попытаться их трансформировать для других вещей. Желательно, чтобы эти вещи двигали человечество вперед, тогда в моей деятельности появится хоть какой-то смысл. Есть ли у нас в стране такие направления? То, что мы умеем делать на мировом уровне в промышленных масштабах, и составляем хоть какую-то конкуренцию на мировой арене? Похоже, что есть. Авиация, космос и атомная энергетика. Сейчас ни одна из таких сфер деятельности не может обойтись без ИТ-технологий. Если я пойду в такую сферу, то сделаю свой вклад, каким бы он ни был. Что для меня более реально? Атомная энергетика. В городе есть предприятия атомной отрасли. А космодромов и авиазаводов не наблюдается. Значит, выбора не остается. Разработка и ИТ с приставкой «гос» По знакомству я устроился в организацию атомной отрасли. Численность ~120 человек, с постепенным ростом количества персонала. Всё ИТ сводилось к серверу 1С и файловому серверу, управляемыми единственным админом, периодически жостко злоупотребляющий алкоголем. 80 компьютеров, одноранговая сеть без домена с черепашьей скоростью и постоянные пропадания компьютеров из сети. Я устроился инженером пуско-наладки: во втором человеке, который бы занимался ИТ, компания, по мнению руководства, не нуждалась. Полгода я занимался делами отдела пуско-наладки, и вдруг сверху спустили разнарядку на изменение структуры предприятия. В новой структуре был предусмотрен отдел информационных технологий. Правда, почему-то, помимо ИТ, в функциях отдела были закреплены работы по метрологическому обеспечению производства. Консультации с авторами структуры показали, что ошибки нет. Админ бухал, и, за неимением других кандидатур, я подготовил документы нового отдела, внутренне называя его химерой. В дальнейшем мне пришлось прикладывать усилия, чтобы за отделом информационных технологий не закрепили еще и функции экологического надзора. Так появился ИТ-отдел на моем предприятии, в таком виде он существует и по сей день. Какая связь между метрологией, экологическим надзором, и информационными технологиями? Самая прямая — мало кто понимает, чем занимаются специалисты этих направлений. Поэтому всякую «неведомую хрень» сваливают в одну кучу, не взирая на здравый смысл. И безумие продолжается: теперь эффективные менеджеры переводят отдел информационных технологий в сметно-договорное подразделение. Исторически, в отрасли сложилась парадоксальная ситуация: вроде бы отрасль высокотехнологичная, даже есть группа компаний, занимающаяся супервычислениями. В отрасли есть куча институтов, которые занимаются программными системами, промышленными контроллерами, АСУТП, автоматизацией производственных и непроизводственных процессов и прочими высокотехнологичными вещами. Вместе со всем этим существует когорта управленцев, которые воспринимают компьютерную технику и сети как досадное недоразумение, от которого хотелось бы отмахнуться. У этих управленцев отсутствует понимание того, что на дворе уже другой технологический уклад, и самая важная вещь в современном корпоративном мире — это информационные потоки, для которых нужна инфраструктура, квалифицированные пользователи, разработчики и обслуживающий персонал. Если же у управленца вдруг возникает понимание необходимости построения ИТ-инфраструктуры, то у него же появляются мечты о том, что все это возникнет само собой из ниоткуда. Другая крайность — что можно сделать любую информационную систему, просто выпустив распоряжение и выделив крупную сумму из бюджета. В купе с тем, что такие управленцы очень смутно представляют себе всю многогранность ИТ-мира, а познания об ИТ-профессиях у них ограничены названиями «сисадмин» и «тыжпрограммист», то с такими людьми очень трудно работать. Невозможно объяснить, что вчерашнего техника по документации нельзя сделать специалистом по базам данных, а очень хорошего мальчика, за которого ручается сам начальник управления, не научишь работе с сетями. И тем более невозможно объяснить, что хороший Linux/Win администратор не должен заниматься закупками, договорами и составлением бесконечных отчетов, а программист не сможет ничего внятного написать, параллельно сопровождая бухгалтерию/кадры/сметы/договора, в промежутках крутя АТС, заправляя принтеры и ведя складскую базу оборудования. Потребительское отношение к ИТ, помноженное на непонимание самого предмета, формирует в отрасли странные перекосы: достаточно простые и хорошо масштабируемые вещи, типа документооборота, стоят для предприятий очень дорого. В то же самое время целевые информационные системы не редко финансируются по остаточному принципу. Есть еще один тип управленцев, с которыми действительно можно плодотворно работать. Это бывшие инженеры еще советской закалки, которые ратуют за свое дело, не чураются инноваций, и вместо затягивания процесса продвигают дело вперед. При этом у них есть видение доступных ресурсов, и нет иллюзий по поводу сроков и затраченных усилий на реализацию проекта, если предоставлена адекватная аналитика. С такими людьми действительно можно работать. Беда таких управленцев в том, что рядом с ними могут оказаться заводные и бодрые исполнители, которые горят желанием заработать на сложном проекте, тщательно скрывая свою некомпетентность. Получив «добро» и административную поддержку, такие исполнители способны долго водить за нос заказчика, получив на выходе неудобовариваемую кашу. Ранее я зарекался, что никогда больше не буду работать с 1С. Жизнь внесла свои коррективы. Оказалось, что на предприятии в момент моего поступления не было единых и унифицированных информационных систем по основным направлениям деятельности. По сфере деятельности моего пуско-наладочного отдела, для каждого типового проекта было свое самобытное, уникальное учетное ПО, написанное залетными людьми на коленке. Под новый проект тоже необходимо было иметь пакет программ автоматизации деятельности. Можно было продолжить использовать старое ПО, к которому накопилось много претензий, исходники которого исчезли вместе с разработчиками. А можно было сделать все немного по-другому. Мне повезло, что рядом со мной работал человек, занимающийся регламентами. В момент моего прихода он как раз разрабатывал регламент на новый проект, и я смог скорректировать регламент так, чтобы в нем не было прописано названий программных продуктов и не было привязок на конкретные технологии. Я долго и методично убеждал его, что таким вещам не место в регламенте, что регламент должен быть написан безотносительно конкретных программных технологий. Только чистый протокол взаимодействия производственных служб, ничего больше. Это сработало, и был принят регламент, не привязывающий нас к старым информационным системам, и кроме того, на основе этого регламента уже можно было разрабатывать ТЗ, и при этом уберечься от буйной фантазии производственников. Если необходимость наличия регламента по старым традициям еще понимали, то необходимости в ТЗ на информационную систему (тем более, технического решения) под новый проект никто не видел. Для меня это было удивительно, но я знал, что палец о палец не ударю, пока у меня не будет подписанного ТЗ. Разработчики знают, что ТЗ — это надежный щит от последующих требований «всё переделать». Поэтому я написал и согласовал ТЗ, которое, по сути, никто не читал кроме меня, но которое несколько раз выручало в дальнейшем. Никакой другой вменяемой программной платформы, кроме 1С, я найти не смог. Копнул в сторону СПО-шнного OOBase, бесплатного Дебет Плюс, подумывал о Qt, потыкался в Лазарус, помедитировал над документаций ExtJs. Но ничего из этого набора не подходило. Да и где бы я нашел специалистов кроме себя, способных сопровождать такую экзотику? Дальше пришлось восстанавливать скиллы по 1С, разбираться с тонкостями написания конфигурации в режиме тонкого клиента. В итоге за несколько месяцев была написана конфигурация 1С, многопроектная и многопользовательская. Сервер был поднят на CentOs Linux + PostgreSQL. Было организовано подключение соответствующих отделов со всех филиалов предприятия к арендованному физическому серверу на магистральном кольце. Да, все это пришлось делать самому на чистом энтузиазме, как говорится, «в одно рыло». А дальше началось самое интересное. Когда система заработала, данные стали вноситься, отчеты стали формироваться, сразу появились люди, которые решили руководить процессом, и приложить свою, так сказать, руку, к развитию и к эксплуатации системы. Так же выяснилось, что система, объединяющая все проекты по направлению, которое она автоматизировала, требуется, ни много ни мало, а на самом верхнем корпоративном уровне для ведения аналитики. В общем, в результате всех блужданий и согласований, учетная система вдруг стала называться отраслевой. Потом из названия исчезло понятие «учет» и появилось понятие «управление», что явно преувеличивало реальность. У меня сложилось впечатление, что такие загадочные трансформации возникают из-за каких-то флуктуаций Вселенной, за ними не стоит какой-то вполне конкретный человек, а просто так работает система. В ходе всей этой деятельности мне пришлось вместо разработки разъезжать по командировкам, согласовывать свои бумажки и писать бумажки за моих «руководителей». Резко активировалась служба безопасности, и если ранее никому дела не было до того, где и как размещен сервер, а на мои запросы на правила размещения была отговорка «согласно техническим требованиям», то теперь последовательно выдвигались все новые и новые требования безопасности, так что мне и моему новому специалисту по сетям пришлось четыре раза переносить серверы на совершенно разные площадки. К моменту опытно-промышленной эксплуатации системы я смог убедить руководство в целесообразности принятия еще одного специалиста-разработчика помимо меня. Это было просто смешно: если бы мне сказали, что один человек разработал отраслевую информационную систему, я бы просто не поверил. Так не бывает. Проблема была в том, что этим человеком оказался я. Когда появился второй специалист, мне пришлось заниматься согласованием ввода системы в опытно-промышленную, а затем и в промышленную эксплуатацию. Оценивая сейчас количество бумажек и писем, я могу сказать, что их текст был сопоставим с объемом кода самой ИС. Основная разработка была сделана за несколько месяцев. Согласование же, до промышленной эксплуатации, длилось почти три года. Сумма, которая была заработана для предприятия, была очень смешной: примерно одна годовая зарплата тимлида в Москве. Мы очень рассчитывали на то, что основные суммы сможем заработать на ежегодных договорах по поддержке и развитию системы. Но нет. Традиционно, система была передана на сопровождение карманной ИТ-организации. Ребята не сразу осознали, что им придется столкнуться с Linux-виртуалками и сопровождать БД PostgreSQL, а не весело мышевозить связку 1С+Windows+Microsoft SQL Server. Руководитель стал названивать мне и требовать, чтобы все было переделано на продукты компании Microsoft, причем именно в тот момент, когда уже начались санкции и в стране был взят курс на импортозамещение. Я прикрылся ТЗ и сообщил ему пункты в отраслевых регламентах, в которых разрешалось использование дистрибутивов Linux для информационных систем. После чего попросил его подумать о стоимости, сроках согласования и сроках реализации такой переделки. Видимо, это взымело действие, и спустя пару недель молчания нам предложили заключить договор на техподдержку. На техподдержку третьей очереди. По факту это означало, что на нас будут сваливать все возникающие проблемы, что бы мы, как разработчики системы, их решали за весьма символическую плату. Мы понимали, что такие информационные системы не работают просто так, их нужно и сопровождать, и развивать. Тем более, что первичные данные в нее вводят наши сотрудники по всем площадкам. Поэтому даже на таких условиях согласились. Но дальше дело не пошло: договор на третью очередь должна была инициировать сторона заказчика, но никто этим не стал заниматься и дело спустили на тормозах. И это был один из немногих моментов, когда мой принцип «программы надо писать хорошо» меня подвел. Наша информационная система проработала в продакшене без поддержки три года на полном автопилоте, и продолжает работать сейчас. Трепещите, 1С-хайтеры! Сервера приложений этой платформы способны месяцами работать без перезагрузок, если их, конечно, правильно настроить. Единственный серьезный технический сбой — это когда спустя год закончилось свободное место на разделе хранилища из-за слишком большого числа сканов документов. Никто у владельцев системы за этим не следил, и когда все встало, мы сами начали названивать чтобы нашли хоть кого-то, кто смог бы увеличить квоту и поднять сервер. Самое большое, о чем я жалею во всей этой истории, так это о том, что я невольно сработал на управленцев, которых описал в начале главы. В результате моей деятельности я только подкрепил заблуждения таких людей в том, что информационные системы появляются сами собой из ниоткуда, они просто есть и они просто работают. Но ведь так не бывает. Обычно. Профессиональная деградация Не каждый разработчик способен работать в таких условиях, в которых пришлось работать мне: вместо разработки — согласования, феерическая бумажная безопасность, составление договоров на поставку и обслуживание, закупка ПО и лицензий… Как, я еще не сказал? Эффективные менеджеры методично выпускают документы, согласно которым каждый специалист должен заниматься всем спектром всех возможных производственных дел. Это называется вовлеченностью в техпроцесс. Забыт общероссийский классификатор профессий, забыт российский реестр профессиональных стандартов. По мнению менеджеров, каждый сотрудник производственного предприятия должен (пишу по памяти): Быть инициатором и исполнителем работ по подготовке, оформлению и регистрации проектов документов в отраслевых системах документооборота, согласовывать документы письменно и электронно, определять согласующих и обязательных согласующих проекта документа, определять сроки согласования проектов документов, вести учет карточек проектов документов как исполнитель. Кто знает что такое SAP, тот представляет себе этот адъ; Выступать инициатором по составлению соответствующего раздела плана мероприятий по заключению в течение планируемого календарного года расходных договоров на поставку требуемой продукции годовой программы закупок, обладать компетенциями по формированию закупочной документации, выраженными в оформлении комплектов документов: обоснование максимальной/минимальной цены; локальные сметы, выкопировки либо иные сметы или расчеты; техническое задание, приложения к ТЗ; проект договора с приложением графика поставки продукции; графики оплаты и поставки; заключение ПДТК и РИО; иные документы и приложения, предусмотренные отраслевыми стандартами. При формировании ТЗ необходимо: выставлять требования к качеству, техническим, функциональным характеристикам; выставлять требования к потребительским свойствам продукции; выставлять требования к безопасности продукции; выставлять требования к порядку приемки продукции и соблюдением иных показателей, связанных с определением соответствия продукции потребностям, требования стандартов, технических условий или иных нормативных документов; выставлять требования к подтверждающим документам которые должны быть предоставлены в составе заявки; контролировать требования к количеству, комплектации, месту, сроку, графику поставки. Как инициатор закупочной процедуры, каждый специалист должен запрашивать и обрабатывать технико-коммерческие предложения потенциальных поставщиков на основе которых составляется расчет начальных цен договоров. Инициатор несет ответственность за обоснованность определения плановой стоимости закупки, за обоснованность определения и правильность расчета НМЦ, за соблюдение положений методики расчета НМЦ при определении стоимости заключаемых договоров; Вести деятельность по отражению хозяйственных операций в бухгалтерском учете предприятия и самостоятельной регистрации закупочных документов по расходным договорам. Выполнять сценарии работы по расходному договору: создавать заявку на закупку для нужд производственного подразделения, вводить первичные бухгалтерские и финансовые документы в систему управления предприятием в бизнес-подразделениях владельцев договоров; Оформлять поступление товаров и прочих активов в подразделение (акты), участвовать в комиссиях по приемке и списанию товаров (акты), обеспечивать документальное оформление ввода/вывода в эксплуатацию оборудования и программного обеспечения (приказы и распоряжения); И т. д. про соблюдение безопасности, отраслевых политик и прочего. (Тяжело читать это канцелярит? Думаю, что в лучшем случае только каждый десятый смог дочитать эти пункты до середины). Суть в том, что такие службы как канцелярия, бухгалтерия, сметно-договорной отдел, административно-хозяйственный отдел и отдел закупок должны выполнять номинальную функцию. Вся основная же деятельность перечисленных подразделений должна быть возложена на плечи всех сотрудников предприятия. ИТ-отделам в этом смысле не повезло больше всех: как бы ни хотели закрыть глаза на такое непонятное и такое ненужное ИТ, а современная жизнь диктует, что всё завязано на ИТ-технологии. Поэтому на каждом предприятии ежегодно заключается около двух десятков договоров по ИТ направлению: различные виды телефонной связи (внутренняя, местная, междугородняя, международная, мобильная), аренда интернета, прочие защищенные/служебные линии, служба доставки (ведь оборудование надо возить), информационные системы типа правовых, ИТС, базы отраслевой технической документации, обслуживание печатной техники, заправка и ремонт картриджей, аттестации по различным видам безопасности, подача отчетности, криптография, банковские системы, закупочные договора по категорийным и мелким закупкам. Ни в каких других подразделениях нет такого количества заключаемых расходных договоров. Нам же еще «повезло» заниматься метрологией… И если прочитать все требования, выставляемые на сотрудников, и подумать сколько бумаги и подписей надо собрать по каждому договору, с соблюдением всех процедур, и учесть, что отчетные документы по договорам появляются каждый месяц, то станет ясно, что ИТ-отдел вынужден круглый год заниматься совсем не ИТ, а планированием, договорами, закупкой, бухотчетностью, обучением, аттестацией и прочим оформлением Очень Важных Документов. Кажется, речь раньше шла об ИТ и разработке? Перечитав эту главу, невозможно найти требований, которые действительно относились бы к информационным технологиям и/или разработке. Возможно, дело в том, что ИТ и разработка — это разные вещи, и если бы действительно существовал отдел разработки или отдел информационных систем, то в них все было бы лучше? Нет, конечно нет. Появившийся год назад отдел разработки программных систем имеет ровно полтора человека с «программистским» прошлым. Вечерами этот полтора человека пишет для заказчика ИС, а днем работает в «поле», производя замеры на оборудовании. Его тоже напрягают всей вышеописанной бюрократией, плюс его очень сильно касается бурная деятельность по безопасности труда. Нас — реже, мы «в поле» только в пики работ и при всяких изменениях сетевой инфраструктуры. Но тоже выполняем все нормы охраны труда: работа на высоте, работа внутри электроустановок, блюдем нормы энергетической отрасли с полным оформлением сопутствующих допусков и прочих документов… Безопасность — это очень хорошо, но когда на ее бумажное оформление уходит прорва рабочего времени работника ИТ-отдела, я считаю, что это перебор. В общем, у меня сложилось впечатление, что вся производственная деятельность регулируется людьми, которые ориентируются в какой-либо своей узкой области, и навязывают свои узкие ориентиры на подконтрольные предприятия и сотрудников. На нас сваливают требования по всему чему угодно: от закупок до энергетических политик, от экзаменов по электробезопасности до требований в области качества и экологии, от норм энергетики до информационного взаимодействия с представителями правоохранительных органов. Я честно пытаюсь все это барахло понимать и учить. Но я каждый раз с ужасом жду очередных экзаменов. Вдруг я не отвечу на вопрос о технологических системах нормальной эксплуатации и системах безопасности РО, или что-то не то скажу про технологические системы поддержания ВХР? А если я неправильно перечислю основные полномочия органов государственной власти РФ субъектов РФ и органов местного самоуправления в сфере отношений, связанных с охраной окружающей среды? Или, что хуже того, забуду требования к обязательному страхованию ответственности за причинение вреда при эксплуатации опасного производственного объекта? Я смотрю на весь объем документов, которые вписываются в должностные и производственные инструкции, и не понимаю: неужели никто не видит всего этого маразма в количестве требований? Я не знаю ни одного человека, который способен запомнить такой объем информации. Таким образом, я потихоньку стал деградировать. Я перестал читать, перестал воспринимать новое, стал забывать многие вещи, которые раньше знал. Я уткнулся в предел возможностей моего мозга: если раньше я без труда мог переварить всякую ненужную дребедень, которую на меня выливали учебные заведения, окружающие меня люди и работодатели, то теперь я стал пробуксовывать. Не имеющей ко мне отношения информации стало так много, что я даже дома часто не могу сразу понять, о чем мне говорят домочадцы. Соответственно, уже не могу читать ни художественную, ни, что важно, техническую литературу: вдумчивого чтения не получается. С опаской вожу машину: уже попадал в ДТП после феерического рабочего дня. Чтобы совсем не съехать с катушек, я пытаюсь снимать психологическое напряжение инстинктивной деятельностью. Немного помогает музицирование, стараюсь играть с подрастающим поколением в простые игры. Ну а чтобы совсем не потерять навыки, продолжаю вечерами и ночами клепать свои давнишние OpenSource проекты. Да, за счет собственного здоровья. Но это все уход от проблемы, а не решение самой проблемы. И теперь, когда мне сорок, я задаюсь вопросом: а сколько я еще смогу протянуть в таком режиме? В кого я превращусь через пять, десять лет? Мне скучно общаться со своими коллегами, как с молодыми так и в возрасте: мне с ними не о чем говорить, у меня другая сфера интересов. А то, что окружающие меня сотрудники называют информационными технологиями, к настоящему ИТ имеет очень отдаленное отношение. И, честно говоря, какое мне дело до сроков согласования основных видов отчетных документов, оформляемых по результатам готовности на объектах электро-энергетического производства? Послесловие Я долго думал, публиковать ли эту статью. Какая-то она получилась грустная и безнадежная, прям кризис среднего возраста. Но я надеюсь, что такое настроение больше получилось вследствие когнитивного диссонанса: во всех средствах массовой информации рассказывают о том, что развиваются информационные технологии, что как никогда востребованы ИТ-специалисты, как все это важно и сложно. Фактически же 95% рабочего времени от меня требуется только умение читать и писать канцелярит. Но сейчас я понял, что публикацию делать обязательно буду. Сегодня я сижу, и устраняю нарушения, обнаруженные в моем отделе. Вышли новые правила ведения производственных журналов. Журналы моего отдела, по недосмотру, прошиты синтетической нитью, а нужна грубая нить. И теперь журнал прошивается через три отверстия, а не через два. Это очень важно. А главное — понятно проверяющим органам. Я чувствую себя средневековым писарем: у меня те же амбарные книги, шило, суровая нитка, железные ножницы, перо для письма и указ Боярской думы. Это то, с чем должен уметь работать специалист отдела информационных технологий. А не с этими вашими компьютерами. Вот и все, что хотел я рассказать."], "hab": ["Программирование"]}{"url": "https://habrahabr.ru/post/322478/", "title": ["Первый способ генерации коллизий для SHA-1"], "text": ["Коллизии существуют для большинства хеш-функций, но для самых хороших из них количество коллизий близко к теоретическому минимуму. Например, за десять лет с момента изобретения SHA-1 не было известно ни об одном практическом способе генерации коллизий. Теперь такой есть. Сегодня первый алгоритм генерации коллизий для SHA-1 представили сотрудники компании Google и Центра математики и информатики в Амстердаме. Вот доказательство: два документа PDF с разным содержимым, но одинаковыми цифровыми подписями SHA-1. https://shattered.it/static/shattered-1.pdf https://shattered.it/static/shattered-2.pdf $ls -l sha*.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:01 shattered-1.pdf -rw-r--r--@ 1 amichal staff 422435 Feb 23 10:14 shattered-2.pdf $shasum -a 1 sha*.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-1.pdf 38762cf7f55934b34d179ae6a4c80cadccbb7f0a shattered-2.pdf На сайте shattered.it можно проверить любой файл на предмет того, входит ли он в пространство возможных коллизий. То есть можно ли подобрать другой набор данных (файл) с таким же хешем. Вектор атаки здесь понятен: злоумышленник может подменить «хороший» файл своим экземпляром с закладкой, вредоносным макросом или загрузчиком трояна. И этот «плохой» файл будет иметь такой же хеш или цифровую подпись. Криптографические хеш-функции вроде SHA-1 — это универсальный криптографический инструмент, который повсеместно используется в практических приложениях. Они нужны при построении ассоциативных массивов, при поиске дубликатов в наборах данных, при построении уникальных идентификаторов, при вычислении контрольных сумм для обнаружения ошибок. Например, на хеши SHA-1 полностью полагается система управлениями версиями программного обеспечения Git. Но ещё важнее, что хеширование критически важно в сфере информационной безопасности: оно используется при сохранении паролей, при выработке электронной подписи и т.д. В общем виде, хеш-функции преобразуют любой большой массив данных в небольшое сообщение. Учитывая повсеместное распространение хеш-функций очень важным требованием является минимальное количество коллизий, когда два различных блока входных данных преобразуются в два одинаковых хеша. В официальном сообщении авторы говорят, что эта находка стала результатом двухлетнего исследования, которая началась после вскоре публикации в 2013 году работы криптографа Марка Стивенса из Центра математики и информатики в Амстердаме о теоретическом подходе к созданию коллизии SHA-1. Он же в дальнейшем продолжил поиск практических методов взлома вместе с коллегами из Google. Компания Google давно выразила своё недоверие SHA-1, особенно в качестве использования этой функции для подписи сертификатов TLS. Ещё в 2014 году, вскоре после публикации работы Стивенса, группа разработчиков Chrome объявила о постепенном отказе от использования SHA-1. Теперь они надеются, что практическая атака на SHA-1 увеличит понимание у сообщества информационной безопасности, так что многие ускорят отказ от SHA-1. Специалисты начали поиск практического метода атаки с создания PDF-префикса, специально подобранного для генерации двух документов с разным контентом, но одинаковым хешем SHA-1. PDF-префикс Идентичный префикс для коллизии, рассчитанной на инфраструктуре Google Затем они использовали инфраструктуру Google, чтобы произвести вычисления и проверить теоретические выкладки. Разработчики говорят, что это было одно из самых крупных вычислений, которые когда-либо проводила компания Google. В общей сложности было произведено девять квинтиллионов вычислений SHA-1 (9 223 372 036 854 775 808), что потребовало 6500 процессорных лет на первой фазе и 110 лет GPU на второй фазе атаки. Числа кажутся большими, но на самом деле такая атака вполне практически реализуема для злоумышленника, у которого есть крупный компьютерный кластер или просто деньги на оплату процессорного времени в облаке. По оценке Google, атака проводится примерно в 100 000 быстрее, чем брутфорс, который можно считать непрактичным. Чтобы представить число хешей, которые обсчитала Google во время брутфорса, можно упомянуть, что примерно такое же количество хешей SHA-256 обсчитывается в сети Bitcoin каждые три секунды, так что в атаке нет ничего фантастического. Вполне можно предположить, что в криптографических отделах некоторых организаций с большими дата-центрами уже давно обсчитываются коллизии SHA-1. Правда, чтобы подобрать коллизию для конкретного сертификата TLS, нужен какой-то другой метод, потому что идентичный префикс из научной работы Google для PDF там не подойдёт. С другой стороны, содержимое сертификатов во многом совпадает, так что теоретически префикс для коллизии можно подобрать. Сейчас Марк Стивенс с соавторами опубликовали научную работу, в которой описывают общие принципы генерации документов с блоками сообщений, которые подвержены коллизии SHA-1. Блоки сообщений, которые подвержены коллизии SHA-1 В соответствии с принятыми правилами раскрытия уязвимостей Google обещает через 90 дней опубликовать в открытом доступе полный код для проведения атаки. Тогда кто угодно может создавать разные документы с одинаковыми цифровыми подписями SHA-1. Возможно, даже разные сертификаты, разные обновления программного обеспечения в Git, разные раздачи на торрентах (хеши DHT), разные старые ключи PGP/GPG и т.д. Впрочем, не стоит преувеличивать опасность таких атак, ведь далеко не каждый документ будет подвержен атаке на поиск коллизии. То есть злоумышленнику придётся изначально создавать два файла: один «хороший», а второй «плохой» с такой же подписью. Затем распространять «хороший» документ по нормальным каналам (например, через Git или торрент-трекер), а впоследствии пробовать подменить его «плохим» файлом с той же цифровой подписью. Впрочем, всё это чисто теоретические рассуждения. Защита от документов, подверженных коллизии хешей SHA-1 уже встроена в программное обеспечение Gmail и GSuite. Как уже упоминалось выше, детектор уязвимых документов работает в открытом доступе на сайте shattered.io. Кроме того, библиотека для обнаружения коллизий опубликована на Github. В качестве защиты от атаки на отыскание коллизий SHA-1 компания Google рекомендует перейти на более качественные криптографические хеш-функции SHA-256 и SHA-3."], "hab": ["Криптография", "Информационная безопасность", "Git"]}{"url": "https://habrahabr.ru/post/122060/", "title": ["Системы управления версиями. Пособие для инженеров, художников и писателей"], "text": ["Привет, Хабр. Решил затронуть измученную во многих статьях тему, конкретнее – описать во многом нестандартное (я бы сказал, несорцовое) использование систем контроля версий (далее – СКВ). Товарищи программисты, давайте спрячем тухлые помидоры и пройдем мимо, ибо данная статья – не для вас. Да, все вы уже изучили все тонкости работы Git, SVN, CVS и знаете много других умных слов. Позвольте же и нам, простым смертным, ознакомиться со всеми преимуществами использования СКВ. Приглашаю под кат всех желающих ознакомиться с СКВ, а также всех тех, кто, так или иначе, имеет дело с быстроменяющимися данными. Зачем это нужно Сам я являюсь студентом технического ВУЗа и практически постоянно работаю с документами (текстами, рисунками, чертежами), изменяя их по три (десять, сто) раз на дню. Порой получается так, что правки, сделанные в течение последней недели, необходимо отменить и вернуться к документам в состоянии недельной давности. Хорошо, если правок было сделано немного, в этом случае могут помочь полсотни ударов по Ctrl+Z. Однако если в течение этой недели шла более-менее активная работа с документом, просто так восстановить статус «до важной правки, сделанной неделю назад» не получится. Для этого необходима копия документа на момент «до важной правки», а также еще десяток копий «до другой важной правки», «до сомнительной правки» и «до правки, которую, скорее всего, придется отменить». В принципе, такой подход возможен и практикуется многими. До недавнего времени я и сам держал важные версии файлов, сохраняя их с префиксами «дата_время», и, вроде бы, был доволен. Преимуществом этого метода является простота, недостатком – «разбухание» рабочих папок и неудобство использования. И, если с первым из них можно как-то бороться (большими жесткими дисками и 7zip’ом), то с неудобством что-то нужно было делать. Что с этим можно сделать, или что такое СКВ Вырываем абзац из Википедии: «Система управления версиями (от англ. Version Control System, VCS или Revision Control System) – программное обеспечение для облегчения работы с изменяющейся информацией. Система управления версиями позволяет хранить несколько версий одного и того же документа, при необходимости, возвращаться к более ранним версиям, определять, кто и когда сделал то или иное изменение и многое другое». Похоже на принцип работы самой Википедии – все версии статей со всеми правками доступны для изучения. Таким образом, использование СКВ в ситуации, когда нужно хранить множество версий файлов – то, что надо. К преимуществам такого подхода относятся удобство использования и экономия свободного дискового пространства благодаря так называемому дельта-сжатию (когда сохраняются не сами файлы в различных версиях, а изменения от версии к версии, что уменьшает объем хранимых данных). Давайте попробуем. Какие бывают СКВ Та же Википедия подсказывает, что СКВ бывают централизованные и распределенные, большие и маленькие, с примочками и без. Нас это не особо интересует, так как мы будем пользоваться (по крайней мере, сначала) только частью функционала СКВ. Этот самый функционал и рассмотрим. Практически все СКВ представляют собой некое хранилище, в котором хранятся все версии файлов, с которыми мы работаем. Здесь необходимо уточнить, что версии хранимых файлов чаще всего определяет пользователь. Внесли мы, допустим, с десяток мелких правок и решили, что пора бы сохранить результаты нашей деятельности в хранилище. В голову приходит аналогия с периодическим нажатием Ctrl+S, с тем лишь отличием, что к данной версии файла можно будет обращаться в будущем. Естественно, что «одним махом» таким образом можно занести в хранилище версии сколь угодно большого количества файлов. Называется это действие «commit», или «фиксация изменений» по-простому. В любой момент в репозиторий (а именно так по-умному называется хранилище) можно добавить новый или удалить существующий файл, и СКВ будет «помнить» когда и что мы добавили/удалили. А благодаря комментариям при commit’ах можно еще и описать для чего собственно данный commit выполняется («добавили фенечку туда-то»/«удалили возможно нужный кусок оттуда-то»). Когда же мы, наконец, понимаем, что пора бы нам вернуться к версии недельной давности, у нас имеется вся история изменений. И тут мы можем выбирать, как поступить. Если необходимо скопировать из старого файла нужный кусочек и вставить в текущую версию – просто извлекаем из хранилища старый файл и копируем из него необходимое. Если же необходимо полностью откатиться назад и продолжить работу со старой версией нам на помощь снова приходит СКВ – можно вернуться к ранней версии и создать так называемую новую ветку («branch»), сохранив при этом все, от чего мы «отказались», откатившись в версиях на неделю назад. Таким образом, историю версий проекта графически можно представить в виде дерева – от «корней» (начала проекта) до «ветвей» (удачных и неудачных правок). Кроме того, «ветку» можно создать и искусственно, к примеру, в том случае, когда из одних исходных файлов мы решим развить две различные версии – в первой работаем над одними фенечками, во второй – над другими. Более того, в случае, если рабочие файлы представляют собой текстовые документы (и в некоторых других), возможно объединение различных веток в одну – так называемое слияние («merge»). Теперь представим, что над проектом работают несколько человек, и каждый занимается своей такой «фенечкой». Наличие общего репозитория в этом случае сильно упрощает разработку. От теории к практике, или начинаем использовать СКВ Итак, надеюсь, я убедил вас в том, что использование СКВ – это хорошо. Осталось лишь научиться использовать СКВ. Этим и займемся. Существуют различные системы контроля версий, отличающиеся друг от друга различными аспектами использования. Так как нас не интересуют (по крайней мере, сначала) тонкости работы различных систем, остановимся на самой простой и дружелюбной из них. По моему скромному мнению, такой системой, как ни странно, является Mercurial – «кроссплатформенная распределённая система управления версиями, разработанная для эффективной работы с очень большими репозиториями кода» с графической оболочкой TortoiseHg. Работа с системой возможна под Windows, Linux и Mac OS X. Сразу оговорюсь, что буду описывать работу с системой в Windows. Освоившим Linux не составит труда изучить все по аналогии. Кроме того, параллельно обучимся работать с бесплатным хостингом Mercurial репозиториев – bitbucket.org, необходимым в случае, если вы работаете над проектом не одни или же, что очень удобно, хотите иметь доступ ко всем версиям проекта через интернет. По сути, это удобная замена Dropbox, если вы использовали его ранее. Для начала устанавливаем Mercurial + TortoiseHg отсюда: tortoisehg.bitbucket.org. Эта система работает в консоли, поэтому для удобства использования позже напишем несколько *.bat файлов для типичных операций. Все операции производятся командой hg. Вызванная без параметров, она выводит список основных команд. В качестве репозитория выступает любая выбранная нами директория (я буду использовать папку “C:\\project\\”), в которой и должны храниться все файлы нашего будущего проекта. Разумеется, никто не запрещает иметь несколько репозиториев на одном компьютере. Чтобы система «поняла», что мы хотим создать репозиторий, выполняем команду: hg init c:\\project после которой будет создана папка “c:\\project\\”, если она не была создана ранее и папка “c:\\project\\.hg\\”, в которой Mercurial будет хранить всю служебную информацию. Тут же вспоминаем, что хотим получить не только локальный репозиторий на своем компьютере, но и удаленный репозиторий, в который будем отправлять все наши изменения (или, как говорят умники, «пушить» изменения в удаленный репозиторий, от англ. «push»). Для этого идем на bitbucket.org, регистрируемся, и создаем свой первый репозиторий (Repositories — Create new repository). Даем репозиторию имя (я для определенности назову его remote_project) и жмем на Create repository. Теперь у нас имеются два репозитория – локальный, находящийся в папке “c:\\project\\” и удаленный, расположенный по адресу “bitbucket.org/имя_вашей_учетки/remote_project/”, где имя_вашей_учетки – указанное при регистрации на bitbucket, remote_project – имя репозитория, выбранное при его создании. Для того, чтобы продолжить изучение, нам необходимо поместить что-нибудь в наш локальный репозиторий. Просто создайте в нем (в моем случае – в папке “c:\\project\\”) любой файл вашего будущего проекта либо скопируйте туда ваш текущий проект. Теперь, строго говоря, нам необходимо указать Mercurial: «мы добавили в папку проекта такой-то и такой-то файлы и пару новых папок», для этого предусмотрена команда “hg add”. Однако, более удобен другой подход – при очередном commit’е мы прикажем Mercurial подхватить все свежесозданные файлы из папки проекта и забыть про удаленные, это гораздо легче, чем каждый раз при создании нового документа выполнять “hg add c:\\project\\new_document.doc”. Итак, приступаем к нашему первому commit’у. Выполняется он следующей командой: hg commit –A –m “comment to commit” Разберем все по порядку. Команда должна вводиться тогда, когда мы находимся в репозитории (то есть предварительно необходимо выполнить “cd c:\\project”). Опция “-A” необходима для того, чтобы Mercurial «подхватил» свежесозданные файлы (см. выше), опция “-m” позволяет добавить к commit’у комментарий. Эти комментарии будут отображаться при просмотре версий (или changeset’ов – списков изменений) в TortoiseHg и на странице проекта в bitbucket.org. Очень важно давать осмысленные комментарии, чтобы потом не мучаться, вспоминая, когда же была сделана та или иная правка. Теперь в нашем репозитории хранится начальная версия нашего проекта. Все дальнейшие commit’ы выполняются аналогично после того, как мы решим, что пора бы сохранить текущую версию. Сделанный commit можно «втолкнуть» в удаленный репозиторий командой: hg push https://bitbucket.org/имя_вашей_учетки/remote_project При этом также необходимо находиться в папке, соответствующей репозиторию. После ввода команды будет запрошено имя и пароль нашей учетки на bitbucket.org, чтобы не вводить их при каждом push’е команду можно заменить на следующую: hg push hg push https://имя_вашей_учетки:пароль_вашей_учетки@bitbucket.org/имя_вашей_учетки/remote_project Так как все команды мы забьем в *.bat файл, в этом случае пароль будет храниться в открытом виде, что представляет собой некоторую угрозу безопасности, однако для меня это приемлемо. Итак, для удобства создаем в зоне прямой досягаемости файлы commit.bat, push.bat и commit&push.bat со следующим содержанием: [содержание файла commit.bat] IF !%1==! goto exit1 cd C:\\project hg commit -A -m \"%*\" goto exit0 :exit1 echo \"NO COMMAND-LINE ARG!\" :exit0 Этот файл, вызванный с аргументами, выполнит commit проекта с занесением аргументов в комментарии к commit’у. Пример: выполняем “commit.bat my first commit” и получаем commit с комментарием «my first commit». В FAR’е для этого удобно использовать сочетание Ctrl+Enter. [содержание файла push.bat] cd C:\\project hg push https://имя_вашей_учетки:пароль_вашей_учетки@bitbucket.org/имя_вашей_учетки/remote_project Этот файл произведет push в удаленный репозиторий. [содержание файла commit&push.bat] IF !%1==! goto exit1 cd C:\\project hg commit -A -m \"%*\" goto exit0 :exit1 echo \"NO COMMAND-LINE ARG!\" :exit0 call ./push.bat Этот файл, вызванный с аргументами, выполнит последовательный commit и push проекта с занесением аргументов в комментарии к commit’у. Кроме того, для мелких промежуточных commit’ов я рекомендую создать файл commit_date_time.bat: [содержание файла commit_date_time.bat] cd C:\\project hg commit -A -m \"%DATE% %TIME%\" Этот файл произведет commit с указанием текущей даты и времени в качестве комментария, что часто бывает удобно. Вопрос о частоте commit’ов и push’ей каждый решает в индивидуальном порядке в зависимости от интенсивности и сложности вносимых правок. Хотя и рекомендуется руководствоваться правилом «чаще – лучше». Правым кликом на файле/папке репозитория можно запустить Repository Explorer (TortoiseHg — Repository Explorer), в котором представлены все наши commit’ы с комментариями к ним. В этом окне отображается древовидная структура нашего репозитория, отсюда же можно производить commit’ы, push’и, откаты к предыдущим версиям (backout’ы) и другие операции. По адресу bitbucket.org/имя_вашей_учетки/remote_project находится аналогичный набор changeset’ов, при этом можно скачать любую версию проекта одним архивом, что иногда также очень удобно. В общем, первоначальное знакомство с Mercurial на этом считаю оконченным. За более подробной информацией можно обратиться по адресу: translated.by/you/mercurial-the-definitive-guide/into-ru/trans/ Для кого эта статья Закончу, пожалуй, тем, с чего следовало бы начать – для кого эта статья? Ответ прост – для тех, кто хочет научиться использовать СКВ. Мне удалось «подсадить» на СКВ нескольких дизайнеров, инженеров и даже писателя. Попробуйте и вы – этим вы, возможно, сильно облегчите себе работу. P. S. Перенес в блог «Системы управления версиями»."], "hab": ["Bitbucket", "Системы управления версиями"]}{"url": "https://habrahabr.ru/post/240105/", "title": ["Работа со спрайтами (Unity3d)", "tutorial"], "text": ["Введение Всем привет. Вначале небольшой экскурс. Эта статья является своеобразным переводом обучающей программы от Jesse Freeman. Ему спасибо за мою теперешнюю возможность получить инвайт. Остальным спасибо за понимание, что это моя первая статья. В утопическом мире населенном только хабралюдьми, я постараюсь сделать так, чтобы за этой статьей последовало еще 9, но мир не идеален, так что пока не известно как всё пойдет. Вроде всё, тогда поехали. Импортирование спрайтов Чтобы импортировать спрайты в ваш проект в Unity достаточно просто перетащить необходимые файлы в любую выбранную вами папку во вкладке Project. Внизу иллюстрация. GIF Как видите всё до безобразия просто. Теперь давайте на примере какого-нибудь спрайта рассмотрим настройки. Перейдем к Assets\\Artwork\\Sprites\\Player и кликнем по текстуре игрока. В инспекторе откроется меню. Если по каким-либо причинам в открывшемся меню напротив вкладки Texture Type указано Texture, то у вас сейчас используется 3D-проект, это легко исправить просто создав новый проект, не забыв переключить на 2D. Внизу иллюстрация. Как исправить Если всё правильно Отлично, теперь перетащим спрайт игрока на сцену, и что мы видим спрайт стал gameobject'ом. Следующее свойство спрайтов о котором я расскажу называется Pixels To Units. Как вы видите спрайт игрока 80x80 пикселей, а в Unity 100x100 пикселей соответствуют одному квадратному метру, поэтому перейдя к настройкам импорта спрайта в строке Pixels To Units вы увидите значение 100 и это означает, что спрайт отображается корректно.Давайте изменим значение на 80, в таком случае вы увидите, что спрайт игрока увеличился, теперь он занимает равно один юнит квадратный. GIF Примечание от переводчика для более корректного отображения спрайтов рекомендуется использовать в этом поле значение 100. Следующее свойство спрайтов о котором я расскажу называется Pivot. Если кратко — это точка привязки. По умолчанию установлена на центр. Вы можете изменить ее в случае необходимости, но в большинстве случае привязка в центре подходит. Здесь обойдемся без иллюстраций. Едем дальше. Поле Filter Mode по умолчанию имеет значение Bilinear, но так как у нас Pixel Art нам нужно изменить его значение на Point, после применения изменений вы видите, что спрайт стал выглядеть лучше, но при приближении видны аномалии, для того чтобы избавиться от них изменим значение поля Format на TrueColor. Теперь спрайт не имеет никаких аномалий. Запомните такие настройки, поскольку их нужно применять каждый раз при работе с Pixel Art'ом. GIF Внимание: этот надстройку необходимо применить ко всем спрайтам, а их много и они разложены по папкам по этому внизу будет прикреплена иллюстрация того, как Вы можете сэкономить себе кучу времени. GIF Возможно следующие уроки будут объемнее, а пока всё. That is all folks."], "hab": ["Unity3D", "Разработка под Android", "Разработка игр"]}{"url": "https://habrahabr.ru/post/156831/", "title": ["Линус Торвальдс предлагает 2560х1600 как новый стандарт для ноутбуков"], "text": ["Линус Торвальдс высказал в своём блоге просьбу к производителям ноутбуков сделать 2560х1600 стандартным разрешением, начиная с 11-дюймовых устройств. Линус говорит, что такое разрешение есть даже на планшетах за $399. У производителей ноутбуков он просит не использовать маркетинговые словечки вроде «ретины», а честно назвать 2560х1600 «приемлемым разрешением». «Тот факт, что ноутбуки застряли в развитии десять лет назад (и даже виден регресс, во многих случаях) примерно на половине этих пикселов по ширине и высоте, очень грустен, — пишет Линус Торвальдс. — Мне не нужны большие переносимые лаптопы, но эти 1366x768 так архаичны. Господи, скоро даже мобильные телефоны начнут издеваться над столь до смешного плохими экранами ноутбуков»."], "hab": ["Usability"]}